#!/usr/bin/env python3
"""æ–‡ä»¶ç®¡ç†API"""

from fastapi import APIRouter, Depends, File, UploadFile, Form, HTTPException, BackgroundTasks
from fastapi.responses import FileResponse
from sqlalchemy.orm import Session, joinedload
from sqlalchemy import and_, or_, desc, asc, func
from typing import List, Optional, Dict, Any
from datetime import datetime, timedelta
import os
import hashlib
import shutil
import json
import mimetypes
import pytz

from database import get_db
from models import ManagedFile, FileVersion, FileUsage, FileCategory, LawyerCertificate, LawyerCertificateFile, SystemSettings, AITask
from schemas import *
import logging
from ai_service import create_ai_task, update_ai_task

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/files", tags=["file_management"])

# æ–‡ä»¶å­˜å‚¨è·¯å¾„é…ç½®
TEMP_FILES_PATH = "/app/uploads/temp"
PERMANENT_FILES_PATH = "/app/uploads/permanent"
PROCESSED_FILES_PATH = "/app/uploads/processed"

# ç¡®ä¿ç›®å½•å­˜åœ¨
for path in [TEMP_FILES_PATH, PERMANENT_FILES_PATH, PROCESSED_FILES_PATH]:
    os.makedirs(path, exist_ok=True)

# ============ è¾…åŠ©å‡½æ•° ============

def calculate_file_hash(file_path: str) -> str:
    """è®¡ç®—æ–‡ä»¶MD5å“ˆå¸Œå€¼"""
    hash_md5 = hashlib.md5()
    with open(file_path, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()

def get_file_type_from_mime(mime_type: str) -> str:
    """æ ¹æ®MIMEç±»å‹ç¡®å®šæ–‡ä»¶ç±»å‹"""
    if mime_type.startswith('image/'):
        return 'image'
    elif mime_type == 'application/pdf':
        return 'pdf'
    elif mime_type.startswith('text/'):
        return 'text'
    elif 'word' in mime_type or 'document' in mime_type:
        return 'document'
    elif 'spreadsheet' in mime_type or 'excel' in mime_type:
        return 'spreadsheet'
    elif 'presentation' in mime_type or 'powerpoint' in mime_type:
        return 'presentation'
    else:
        return 'other'

def cleanup_expired_files(db: Session):
    """æ¸…ç†è¿‡æœŸçš„ä¸´æ—¶æ–‡ä»¶"""
    try:
        # æ¸…ç†è¿‡æœŸçš„ä¸Šä¼ ä¸´æ—¶æ–‡ä»¶ï¼ˆ30å¤©ï¼‰
        expired_upload_files = db.query(ManagedFile).filter(
            and_(
                ManagedFile.file_category == "temporary_upload",
                ManagedFile.expires_at < datetime.now(),
                ManagedFile.is_archived == False
            )
        ).all()
        
        # æ¸…ç†è¿‡æœŸçš„ç”Ÿæˆæ–‡ä»¶ï¼ˆ180å¤©ï¼‰
        expired_generated_files = db.query(ManagedFile).filter(
            and_(
                ManagedFile.file_category == "temporary_generated",
                ManagedFile.expires_at < datetime.now(),
                ManagedFile.is_archived == False
            )
        ).all()
        
        expired_files = expired_upload_files + expired_generated_files
        
        for file in expired_files:
            try:
                # åˆ é™¤ç‰©ç†æ–‡ä»¶
                if os.path.exists(file.storage_path):
                    os.remove(file.storage_path)
                if file.processed_path and os.path.exists(file.processed_path):
                    os.remove(file.processed_path)
                
                # æ ‡è®°ä¸ºå·²å½’æ¡£
                file.is_archived = True
                file.archived_at = datetime.now()
                
            except Exception as e:
                logger.error(f"æ¸…ç†æ–‡ä»¶å¤±è´¥ {file.id}: {e}")
        
        db.commit()
        logger.info(f"æ¸…ç†äº† {len(expired_files)} ä¸ªè¿‡æœŸä¸´æ—¶æ–‡ä»¶")
        
    except Exception as e:
        logger.error(f"æ¸…ç†è¿‡æœŸæ–‡ä»¶å¤±è´¥: {e}")
        db.rollback()

# ============ APIç«¯ç‚¹ ============

@router.post("/upload/temporary")
async def upload_temporary_file(
    file: UploadFile = File(...),
    category: Optional[str] = Form(None),
    description: Optional[str] = Form(None),
    tags: Optional[str] = Form(None),  # JSONå­—ç¬¦ä¸²
    expires_hours: int = Form(24),  # é»˜è®¤24å°æ—¶è¿‡æœŸ
    db: Session = Depends(get_db)
):
    """ä¸Šä¼ ä¸´æ—¶æ–‡ä»¶"""
    try:
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        content = await file.read()
        file_size = len(content)
        
        if file_size > 100 * 1024 * 1024:  # 100MBé™åˆ¶
            raise HTTPException(status_code=413, detail="æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶(100MB)")
        
        # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
        await file.seek(0)
        
        # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
        file_hash = hashlib.md5(content).hexdigest()
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒæ–‡ä»¶
        existing_file = db.query(ManagedFile).filter(
            and_(
                ManagedFile.file_hash == file_hash,
                ManagedFile.is_archived == False
            )
        ).first()
        
        if existing_file:
            # æ›´æ–°è®¿é—®æ—¶é—´
            existing_file.access_count += 1
            existing_file.last_accessed = datetime.now()
            db.commit()
            
            return {
                "success": True,
                "message": "æ–‡ä»¶å·²å­˜åœ¨ï¼Œè¿”å›ç°æœ‰æ–‡ä»¶ä¿¡æ¯",
                "file_id": existing_file.id,
                "is_duplicate": True
            }
        
        # ç”Ÿæˆå­˜å‚¨è·¯å¾„
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename_base = os.path.splitext(file.filename)[0]
        file_ext = os.path.splitext(file.filename)[1]
        storage_filename = f"temp_{timestamp}_{file_hash[:8]}{file_ext}"
        storage_path = os.path.join(TEMP_FILES_PATH, storage_filename)
        
        # ä¿å­˜æ–‡ä»¶
        with open(storage_path, "wb") as f:
            await file.seek(0)
            shutil.copyfileobj(file.file, f)
        
        # è·å–MIMEç±»å‹
        mime_type, _ = mimetypes.guess_type(file.filename)
        if not mime_type:
            mime_type = "application/octet-stream"
        
        # è§£ææ ‡ç­¾
        tags_list = []
        if tags:
            try:
                tags_list = json.loads(tags)
            except:
                tags_list = [tag.strip() for tag in tags.split(",") if tag.strip()]
        
        # åˆ›å»ºæ•°æ®åº“è®°å½•
        db_file = ManagedFile(
            original_filename=file.filename,
            display_name=file.filename,
            storage_path=storage_path,
            file_type=get_file_type_from_mime(mime_type),
            mime_type=mime_type,
            file_size=file_size,
            file_hash=file_hash,
            file_category="temporary_upload",  # ä¸Šä¼ çš„ä¸´æ—¶æ–‡ä»¶
            category=category,
            tags=tags_list,
            description=description,
            expires_at=datetime.now() + timedelta(days=30),  # 30å¤©è¿‡æœŸ
            access_count=1,
            last_accessed=datetime.now()
        )
        
        db.add(db_file)
        db.commit()
        db.refresh(db_file)
        
        logger.info(f"ä¸´æ—¶æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {file.filename} -> {storage_path}")
        
        return {
            "success": True,
            "message": "ä¸´æ—¶æ–‡ä»¶ä¸Šä¼ æˆåŠŸ",
            "file_id": db_file.id,
            "filename": db_file.original_filename,
            "file_size": db_file.file_size,
            "expires_at": db_file.expires_at.isoformat(),
            "is_duplicate": False
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ä¸Šä¼ ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ä¸Šä¼ å¤±è´¥: {str(e)}")

@router.post("/upload/permanent")
async def upload_permanent_file(
    file: UploadFile = File(...),
    display_name: str = Form(...),
    category: str = Form(None),  # å¯é€‰ï¼Œå¦‚æœä¸æä¾›å°†ä½¿ç”¨AIåˆ†ç±»
    description: Optional[str] = Form(None),
    tags: Optional[str] = Form(None),  # JSONå­—ç¬¦ä¸²
    keywords: Optional[str] = Form(None),
    is_public: bool = Form(True),
    enable_ai_classification: bool = Form(True),  # æ˜¯å¦å¯ç”¨AIåˆ†ç±»
    enable_vision_analysis: bool = Form(True),  # æ˜¯å¦å¯ç”¨è§†è§‰åˆ†æ
    db: Session = Depends(get_db)
):
    """ä¸Šä¼ å¸¸é©»æ–‡ä»¶"""
    try:
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        content = await file.read()
        file_size = len(content)
        
        if file_size > 200 * 1024 * 1024:  # 200MBé™åˆ¶
            raise HTTPException(status_code=413, detail="æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶(200MB)")
        
        # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
        file_hash = hashlib.md5(content).hexdigest()
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒæ–‡ä»¶
        existing_file = db.query(ManagedFile).filter(
            and_(
                ManagedFile.file_hash == file_hash,
                ManagedFile.file_category == "permanent",
                ManagedFile.is_archived == False
            )
        ).first()
        
        if existing_file:
            # æ›´æ–°è®¿é—®æ—¶é—´å’Œè®¡æ•°ï¼Œè¿”å›ç°æœ‰æ–‡ä»¶ä¿¡æ¯è€Œä¸æ˜¯æŠ›å‡ºé”™è¯¯
            existing_file.access_count += 1
            existing_file.last_accessed = datetime.now()
            db.commit()
            
            return {
                "success": True,
                "message": "ç›¸åŒæ–‡ä»¶å·²å­˜åœ¨ï¼Œè¿”å›ç°æœ‰æ–‡ä»¶ä¿¡æ¯",
                "file_id": existing_file.id,
                "display_name": existing_file.display_name,
                "file_size": existing_file.file_size,
                "category": existing_file.category,
                "tags": existing_file.tags or [],
                "is_duplicate": True
            }
        
        # ç”Ÿæˆå­˜å‚¨è·¯å¾„
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        file_ext = os.path.splitext(file.filename)[1]
        storage_filename = f"perm_{timestamp}_{file_hash[:8]}{file_ext}"
        storage_path = os.path.join(PERMANENT_FILES_PATH, storage_filename)
        
        # ä¿å­˜æ–‡ä»¶
        with open(storage_path, "wb") as f:
            f.write(content)
        
        # è·å–MIMEç±»å‹
        mime_type, _ = mimetypes.guess_type(file.filename)
        if not mime_type:
            mime_type = "application/octet-stream"
        
        # è§£ææ ‡ç­¾
        tags_list = []
        if tags:
            try:
                tags_list = json.loads(tags)
            except:
                tags_list = [tag.strip() for tag in tags.split(",") if tag.strip()]
        
        # AIæ™ºèƒ½åˆ†æå’Œåˆ†ç±»
        ai_classification = None
        final_category = category
        final_tags = tags_list.copy()
        
        if enable_ai_classification:
            try:
                from ai_service import ai_service
                
                # è¿›è¡Œæ™ºèƒ½æ–‡æ¡£åˆ†æ
                if enable_vision_analysis:
                    analysis_result = await ai_service.smart_document_analysis(
                        storage_path, 
                        enable_vision=True
                    )
                else:
                    analysis_result = await ai_service.smart_document_analysis(storage_path)
                
                if analysis_result.get("success"):
                    if enable_vision_analysis:
                        ai_classification = analysis_result["results"]["final_classification"]
                    else:
                        ai_classification = analysis_result.get("classification")
                    
                    # å¦‚æœç”¨æˆ·æ²¡æœ‰æŒ‡å®šåˆ†ç±»ï¼Œä½¿ç”¨AIåˆ†ç±»ç»“æœ
                    if not final_category and ai_classification:
                        final_category = ai_classification.get("category", "other")
                    
                    # æå–AIå»ºè®®çš„æ ‡ç­¾
                    if ai_classification:
                        tag_result = await ai_service.extract_document_tags(
                            storage_path, 
                            existing_tags=final_tags
                        )
                        if tag_result.get("success"):
                            final_tags = tag_result.get("all_tags", final_tags)
                    
                logger.info(f"AIåˆ†ç±»å®Œæˆ: {final_category}, æ ‡ç­¾: {final_tags}")
                
            except Exception as ai_err:
                logger.warning(f"AIåˆ†ç±»å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åˆ†ç±»: {ai_err}")
                if not final_category:
                    final_category = "other"
        else:
            if not final_category:
                final_category = "other"
        
        # åˆ›å»ºæ•°æ®åº“è®°å½•
        db_file = ManagedFile(
            original_filename=file.filename,
            display_name=display_name,
            storage_path=storage_path,
            file_type=get_file_type_from_mime(mime_type),
            mime_type=mime_type,
            file_size=file_size,
            file_hash=file_hash,
            file_category="permanent",
            category=final_category,
            tags=final_tags,
            description=description,
            keywords=keywords,
            is_public=is_public,
            access_count=0,
            last_accessed=datetime.now()
        )
        
        db.add(db_file)
        db.commit()
        db.refresh(db_file)
        
        # åˆ›å»ºAIåˆ†æä»»åŠ¡
        task_id = None
        if enable_ai_classification:
            task_id = create_ai_task(db, db_file.id, "permanent_file")
            logger.info(f"ğŸ“‹ å¸¸é©»æ–‡ä»¶AIä»»åŠ¡å·²åˆ›å»º: ä»»åŠ¡ID={task_id}, æ–‡ä»¶ID={db_file.id}")
            
            # æ›´æ–°AIä»»åŠ¡çŠ¶æ€ä¸ºæˆåŠŸï¼ˆå¦‚æœAIåˆ†ææˆåŠŸï¼‰
            if ai_classification:
                update_ai_task(db, task_id, "success", result={
                    "ai_classification": ai_classification,
                    "final_category": final_category,
                    "final_tags": final_tags,
                    "analysis_enabled": True
                })
            else:
                # AIåˆ†æå¤±è´¥æˆ–æ— ç»“æœ
                update_ai_task(db, task_id, "completed", result={
                    "final_category": final_category,
                    "final_tags": final_tags,
                    "analysis_enabled": False
                })
        
        logger.info(f"å¸¸é©»æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {display_name} -> {storage_path}")
        
        return {
            "success": True,
            "message": "å¸¸é©»æ–‡ä»¶ä¸Šä¼ æˆåŠŸ",
            "file_id": db_file.id,
            "display_name": db_file.display_name,
            "file_size": db_file.file_size,
            "category": final_category,
            "tags": final_tags,
            "ai_classification": ai_classification if enable_ai_classification else None,
            "task_id": task_id  # è¿”å›ä»»åŠ¡ID
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ä¸Šä¼ å¸¸é©»æ–‡ä»¶å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ä¸Šä¼ å¤±è´¥: {str(e)}")

@router.get("/list")
async def list_files(
    file_category: Optional[str] = None,  # temporary, permanent
    category: Optional[str] = None,
    search: Optional[str] = None,
    tags: Optional[str] = None,
    page: int = 1,
    page_size: int = 20,
    sort_by: str = "created_at",
    sort_order: str = "desc",
    include_archived: bool = False,
    db: Session = Depends(get_db)
):
    """è·å–æ–‡ä»¶åˆ—è¡¨"""
    try:
        # æ¸…ç†è¿‡æœŸæ–‡ä»¶
        if file_category in ["temporary_upload", "temporary_generated", "temporary"] or file_category is None:
            cleanup_expired_files(db)
        
        # æ„å»ºæŸ¥è¯¢
        query = db.query(ManagedFile)
        
        # åŸºç¡€è¿‡æ»¤
        if not include_archived:
            query = query.filter(ManagedFile.is_archived == False)
        
        if file_category:
            if file_category == "temporary":
                # æŸ¥è¯¢æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶ç±»å‹
                query = query.filter(
                    or_(
                        ManagedFile.file_category == "temporary",
                        ManagedFile.file_category == "temporary_upload",
                        ManagedFile.file_category == "temporary_generated"
                    )
                )
            else:
                query = query.filter(ManagedFile.file_category == file_category)
        
        if category:
            query = query.filter(ManagedFile.category == category)
        
        # æœç´¢
        if search:
            search_filter = or_(
                ManagedFile.display_name.ilike(f"%{search}%"),
                ManagedFile.original_filename.ilike(f"%{search}%"),
                ManagedFile.description.ilike(f"%{search}%"),
                ManagedFile.keywords.ilike(f"%{search}%")
            )
            query = query.filter(search_filter)
        
        # æ ‡ç­¾è¿‡æ»¤
        if tags:
            tag_list = [tag.strip() for tag in tags.split(",")]
            for tag in tag_list:
                query = query.filter(ManagedFile.tags.contains([tag]))
        
        # æ’åº
        sort_column = getattr(ManagedFile, sort_by, ManagedFile.created_at)
        if sort_order.lower() == "desc":
            query = query.order_by(desc(sort_column))
        else:
            query = query.order_by(asc(sort_column))
        
        # åˆ†é¡µ
        total = query.count()
        offset = (page - 1) * page_size
        files = query.offset(offset).limit(page_size).all()
        
        # æ ¼å¼åŒ–ç»“æœ
        result_files = []
        for file in files:
            china_tz = pytz.timezone('Asia/Shanghai')
            
            file_info = {
                "id": file.id,
                "display_name": file.display_name,
                "original_filename": file.original_filename,
                "file_type": file.file_type,
                "file_size": file.file_size,
                "file_category": file.file_category,
                "category": file.category,
                "tags": file.tags or [],
                "description": file.description,
                "keywords": file.keywords,
                "is_public": file.is_public,
                "access_count": file.access_count,
                "created_at": file.created_at.replace(tzinfo=pytz.UTC).astimezone(china_tz).strftime('%Y-%m-%d %H:%M:%S'),
                "last_accessed": file.last_accessed.replace(tzinfo=pytz.UTC).astimezone(china_tz).strftime('%Y-%m-%d %H:%M:%S') if file.last_accessed else None,
                "is_archived": file.is_archived
            }
            
            # ä¸´æ—¶æ–‡ä»¶æ˜¾ç¤ºè¿‡æœŸæ—¶é—´
            if file.file_category in ["temporary_upload", "temporary_generated", "temporary"] and file.expires_at:
                file_info["expires_at"] = file.expires_at.replace(tzinfo=pytz.UTC).astimezone(china_tz).strftime('%Y-%m-%d %H:%M:%S')
                file_info["is_expired"] = file.expires_at < datetime.now()
            
            result_files.append(file_info)
        
        return {
            "success": True,
            "files": result_files,
            "pagination": {
                "total": total,
                "page": page,
                "page_size": page_size,
                "total_pages": (total + page_size - 1) // page_size
            }
        }
        
    except Exception as e:
        logger.error(f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}")

# å…·ä½“è·¯å¾„è·¯ç”±å¿…é¡»åœ¨å‚æ•°è·¯ç”±ä¹‹å‰å®šä¹‰

@router.get("/stats")
async def get_file_stats(db: Session = Depends(get_db)):
    """è·å–æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯"""
    try:
        # æ¸…ç†è¿‡æœŸæ–‡ä»¶
        cleanup_expired_files(db)
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_files = db.query(ManagedFile).filter(ManagedFile.is_archived == False).count()
        temp_files = db.query(ManagedFile).filter(
            and_(
                or_(
                    ManagedFile.file_category == "temporary",
                    ManagedFile.file_category == "temporary_upload",
                    ManagedFile.file_category == "temporary_generated"
                ),
                ManagedFile.is_archived == False
            )
        ).count()
        permanent_files = db.query(ManagedFile).filter(
            and_(ManagedFile.file_category == "permanent", ManagedFile.is_archived == False)
        ).count()
        
        # å­˜å‚¨å¤§å°ç»Ÿè®¡
        total_size = db.query(func.sum(ManagedFile.file_size)).filter(ManagedFile.is_archived == False).scalar() or 0
        temp_size = db.query(func.sum(ManagedFile.file_size)).filter(
            and_(
                or_(
                    ManagedFile.file_category == "temporary",
                    ManagedFile.file_category == "temporary_upload",
                    ManagedFile.file_category == "temporary_generated"
                ),
                ManagedFile.is_archived == False
            )
        ).scalar() or 0
        permanent_size = db.query(func.sum(ManagedFile.file_size)).filter(
            and_(ManagedFile.file_category == "permanent", ManagedFile.is_archived == False)
        ).scalar() or 0
        
        # æ–‡ä»¶ç±»å‹ç»Ÿè®¡
        type_stats = db.query(
            ManagedFile.file_type,
            func.count(ManagedFile.id).label('count'),
            func.sum(ManagedFile.file_size).label('size')
        ).filter(ManagedFile.is_archived == False).group_by(ManagedFile.file_type).all()
        
        return {
            "success": True,
            "stats": {
                "total_files": total_files,
                "temporary_files": temp_files,
                "permanent_files": permanent_files,
                "total_size": total_size,
                "temporary_size": temp_size,
                "permanent_size": permanent_size,
                "file_types": [
                    {
                        "type": stat.file_type,
                        "count": stat.count,
                        "size": stat.size or 0
                    }
                    for stat in type_stats
                ]
            }
        }
        
    except Exception as e:
        logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")

@router.get("/categories/list")
async def list_categories(db: Session = Depends(get_db)):
    """è·å–åˆ†ç±»åˆ—è¡¨"""
    try:
        categories = db.query(FileCategory).filter(FileCategory.is_active == True).order_by(FileCategory.sort_order).all()
        
        return {
            "success": True,
            "categories": [
                {
                    "id": cat.id,
                    "name": cat.name,
                    "display_name": cat.display_name,
                    "description": cat.description,
                    "parent_id": cat.parent_id,
                    "icon": cat.icon,
                    "color": cat.color,
                    "sort_order": cat.sort_order
                }
                for cat in categories
            ]
        }
        
    except Exception as e:
        logger.error(f"è·å–åˆ†ç±»åˆ—è¡¨å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–åˆ†ç±»åˆ—è¡¨å¤±è´¥: {str(e)}")

@router.get("/categories/suggestions")
async def get_category_suggestions():
    """è·å–æ–‡æ¡£åˆ†ç±»å»ºè®®"""
    try:
        from ai_service import ai_service
        
        return {
            "success": True,
            "categories": [
                {
                    "code": "performance_contract",
                    "name": "ä¸šç»©åˆåŒ",
                    "description": "æ³•å¾‹æœåŠ¡åˆåŒã€å§”æ‰˜åè®®ç­‰"
                },
                {
                    "code": "award_certificate", 
                    "name": "è£èª‰å¥–é¡¹",
                    "description": "å„ç§æ³•å¾‹è¡Œä¸šå¥–é¡¹è¯ä¹¦ã€æ’åè®¤è¯ç­‰"
                },
                {
                    "code": "qualification_certificate",
                    "name": "èµ„è´¨è¯ç…§", 
                    "description": "å¾‹å¸ˆäº‹åŠ¡æ‰€æ‰§ä¸šè®¸å¯è¯ã€è¥ä¸šæ‰§ç…§ç­‰æœºæ„èµ„è´¨è¯æ˜"
                },
                {
                    "code": "lawyer_certificate",
                    "name": "å¾‹å¸ˆè¯",
                    "description": "ä¸ªäººå¾‹å¸ˆæ‰§ä¸šè¯ä¹¦ï¼ŒåŒ…å«å¾‹å¸ˆå§“åã€æ‰§ä¸šè¯å·ç­‰ä¿¡æ¯"
                },
                {
                    "code": "other",
                    "name": "å…¶ä»–æ‚é¡¹",
                    "description": "ä¸å±äºä»¥ä¸Šç±»åˆ«çš„å…¶ä»–æ–‡æ¡£"
                }
            ],
            "business_fields": ai_service.business_fields,
            "lawyer_certificate_tags": {
                "position": ["åˆä¼™äºº", "å¾‹å¸ˆ"],
                "business_fields": ai_service.business_fields
            }
        }
        
    except Exception as e:
        logger.error(f"è·å–åˆ†ç±»å»ºè®®å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–å»ºè®®å¤±è´¥: {str(e)}")

# å‚æ•°è·¯ç”±å¿…é¡»åœ¨å…·ä½“è·¯å¾„è·¯ç”±ä¹‹å

@router.get("/{file_id}")
async def get_file_info(file_id: int, db: Session = Depends(get_db)):
    """è·å–æ–‡ä»¶è¯¦ç»†ä¿¡æ¯"""
    try:
        file = db.query(ManagedFile).filter(ManagedFile.id == file_id).first()
        if not file:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        # æ›´æ–°è®¿é—®ç»Ÿè®¡
        file.access_count += 1
        file.last_accessed = datetime.now()
        db.commit()
        
        china_tz = pytz.timezone('Asia/Shanghai')
        
        return {
            "success": True,
            "file": {
                "id": file.id,
                "display_name": file.display_name,
                "original_filename": file.original_filename,
                "storage_path": file.storage_path,
                "file_type": file.file_type,
                "mime_type": file.mime_type,
                "file_size": file.file_size,
                "file_hash": file.file_hash,
                "file_category": file.file_category,
                "category": file.category,
                "tags": file.tags or [],
                "description": file.description,
                "keywords": file.keywords,
                "is_public": file.is_public,
                "access_count": file.access_count,
                "created_at": file.created_at.replace(tzinfo=pytz.UTC).astimezone(china_tz).strftime('%Y-%m-%d %H:%M:%S'),
                "last_accessed": file.last_accessed.replace(tzinfo=pytz.UTC).astimezone(china_tz).strftime('%Y-%m-%d %H:%M:%S'),
                "expires_at": file.expires_at.replace(tzinfo=pytz.UTC).astimezone(china_tz).strftime('%Y-%m-%d %H:%M:%S') if file.expires_at else None,
                "is_archived": file.is_archived
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {str(e)}")

@router.get("/{file_id}/download")
async def download_file(file_id: int, db: Session = Depends(get_db)):
    """ä¸‹è½½æ–‡ä»¶"""
    try:
        file = db.query(ManagedFile).filter(ManagedFile.id == file_id).first()
        if not file:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        if not os.path.exists(file.storage_path):
            raise HTTPException(status_code=404, detail="ç‰©ç†æ–‡ä»¶ä¸å­˜åœ¨")
        
        # æ›´æ–°è®¿é—®ç»Ÿè®¡
        file.access_count += 1
        file.last_accessed = datetime.now()
        db.commit()
        
        return FileResponse(
            path=file.storage_path,
            filename=file.original_filename,
            media_type=file.mime_type
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ä¸‹è½½æ–‡ä»¶å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ä¸‹è½½æ–‡ä»¶å¤±è´¥: {str(e)}")

@router.put("/{file_id}")
async def update_file_info(
    file_id: int,
    display_name: Optional[str] = Form(None),
    category: Optional[str] = Form(None),
    description: Optional[str] = Form(None),
    tags: Optional[str] = Form(None),
    keywords: Optional[str] = Form(None),
    is_public: Optional[bool] = Form(None),
    enable_ai_reanalysis: bool = Form(False),  # æ˜¯å¦é‡æ–°è¿›è¡ŒAIåˆ†æ
    db: Session = Depends(get_db)
):
    """æ›´æ–°æ–‡ä»¶ä¿¡æ¯"""
    try:
        file = db.query(ManagedFile).filter(ManagedFile.id == file_id).first()
        if not file:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        # è®°å½•æ›´æ–°å‰çš„ä¿¡æ¯
        old_info = {
            "display_name": file.display_name,
            "category": file.category,
            "description": file.description,
            "tags": file.tags,
            "keywords": file.keywords
        }
        
        # AIå­¦ä¹ æœºåˆ¶ï¼šå½“ç”¨æˆ·ä¿®æ”¹åˆ†ç±»æ—¶ï¼Œè®°å½•å­¦ä¹ æ•°æ®
        if category and category != old_info["category"]:
            try:
                from ai_service import ai_service
                
                # è®°å½•ç”¨æˆ·çš„ä¿®æ­£è¡Œä¸ºä¾›AIå­¦ä¹ 
                learning_data = {
                    "file_path": file.storage_path,
                    "original_classification": old_info["category"],
                    "user_correction": category,
                    "file_type": file.file_type,
                    "file_size": file.file_size,
                    "correction_reason": "user_manual_edit",
                    "timestamp": datetime.now().isoformat()
                }
                
                # å¦‚æœæ˜¯ä»"å¾‹å¸ˆè¯"ä¿®æ­£ä¸º"èµ„è´¨è¯ç…§"ï¼Œè®°å½•è¯¦ç»†åŸå› 
                if old_info["category"] == "lawyer_certificate" and category == "qualification_certificate":
                    learning_data["specific_correction"] = "law_firm_license_vs_personal_certificate"
                    learning_data["learning_note"] = "å¾‹å¸ˆäº‹åŠ¡æ‰€æ‰§ä¸šè®¸å¯è¯åº”å½’ç±»ä¸ºèµ„è´¨è¯ç…§ï¼Œè€Œéä¸ªäººå¾‹å¸ˆè¯"
                
                # ä¿å­˜å­¦ä¹ æ•°æ®åˆ°é…ç½®ç®¡ç†å™¨
                ai_service.record_user_correction(learning_data)
                
                logger.info(f"AIå­¦ä¹ è®°å½•: ç”¨æˆ·å°† {old_info['category']} ä¿®æ­£ä¸º {category}")
                
            except Exception as learn_err:
                logger.warning(f"AIå­¦ä¹ è®°å½•å¤±è´¥ï¼Œä½†ä¸å½±å“æ›´æ–°: {learn_err}")
        
        # æ›´æ–°å­—æ®µ
        if display_name is not None:
            file.display_name = display_name
        if category is not None:
            file.category = category
        if description is not None:
            file.description = description
        if keywords is not None:
            file.keywords = keywords
        if is_public is not None:
            file.is_public = is_public
        
        # æ›´æ–°æ ‡ç­¾
        if tags is not None:
            try:
                file.tags = json.loads(tags)
            except:
                file.tags = [tag.strip() for tag in tags.split(",") if tag.strip()]
        
        file.updated_at = datetime.now()
        
        # å¦‚æœå¯ç”¨AIé‡æ–°åˆ†æ
        ai_result = None
        if enable_ai_reanalysis and file.file_category == "permanent":
            try:
                from ai_service import ai_service
                analysis_result = await ai_service.smart_document_analysis(
                    file.storage_path, 
                    enable_vision=True
                )
                if analysis_result.get("success"):
                    ai_result = analysis_result["results"]["final_classification"]
                    
                    # å¦‚æœç”¨æˆ·æ²¡æœ‰æ‰‹åŠ¨è®¾ç½®åˆ†ç±»ï¼Œä½¿ç”¨AIåˆ†æç»“æœ
                    if category is None and ai_result.get("category"):
                        file.category = ai_result["category"]
                    
                    # æ›´æ–°AIåˆ†æç»“æœåˆ°å¤„ç†ç»“æœä¸­
                    if not file.processing_result:
                        file.processing_result = {}
                    file.processing_result["ai_analysis"] = ai_result
                    file.processing_result["last_analysis"] = datetime.now().isoformat()
                    
                    logger.info(f"æ–‡ä»¶ {file_id} AIé‡æ–°åˆ†æå®Œæˆ: {ai_result.get('category')}")
            except Exception as ai_err:
                logger.warning(f"AIé‡æ–°åˆ†æå¤±è´¥: {ai_err}")
        
        db.commit()
        
        return {
            "success": True, 
            "message": "æ–‡ä»¶ä¿¡æ¯æ›´æ–°æˆåŠŸ",
            "ai_analysis": ai_result if enable_ai_reanalysis else None,
            "updated_fields": {
                "display_name": file.display_name,
                "category": file.category,
                "description": file.description,
                "tags": file.tags,
                "keywords": file.keywords
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ›´æ–°æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {str(e)}")
        db.rollback()
        raise HTTPException(status_code=500, detail=f"æ›´æ–°æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {str(e)}")

@router.delete("/{file_id}")
async def delete_file(file_id: int, force: bool = False, db: Session = Depends(get_db)):
    """åˆ é™¤æ–‡ä»¶"""
    try:
        file = db.query(ManagedFile).filter(ManagedFile.id == file_id).first()
        if not file:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        if force:
            # ç‰©ç†åˆ é™¤
            try:
                if os.path.exists(file.storage_path):
                    os.remove(file.storage_path)
                if file.processed_path and os.path.exists(file.processed_path):
                    os.remove(file.processed_path)
            except Exception as e:
                logger.warning(f"åˆ é™¤ç‰©ç†æ–‡ä»¶å¤±è´¥: {e}")
            
            # åˆ é™¤æ•°æ®åº“è®°å½•
            db.delete(file)
        else:
            # è½¯åˆ é™¤ï¼ˆå½’æ¡£ï¼‰
            file.is_archived = True
            file.archived_at = datetime.now()
        
        db.commit()
        
        return {
            "success": True,
            "message": "æ–‡ä»¶åˆ é™¤æˆåŠŸ" if force else "æ–‡ä»¶å·²å½’æ¡£"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {str(e)}")

@router.post("/categories")
async def create_category(
    name: str = Form(...),
    display_name: str = Form(...),
    description: Optional[str] = Form(None),
    parent_id: Optional[int] = Form(None),
    icon: Optional[str] = Form(None),
    color: Optional[str] = Form(None),
    db: Session = Depends(get_db)
):
    """åˆ›å»ºæ–‡ä»¶åˆ†ç±»"""
    try:
        # æ£€æŸ¥åˆ†ç±»åæ˜¯å¦å·²å­˜åœ¨
        existing = db.query(FileCategory).filter(FileCategory.name == name).first()
        if existing:
            raise HTTPException(status_code=409, detail="åˆ†ç±»åç§°å·²å­˜åœ¨")
        
        category = FileCategory(
            name=name,
            display_name=display_name,
            description=description,
            parent_id=parent_id,
            icon=icon,
            color=color
        )
        
        db.add(category)
        db.commit()
        db.refresh(category)
        
        return {
            "success": True,
            "message": "åˆ†ç±»åˆ›å»ºæˆåŠŸ",
            "category_id": category.id
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ›å»ºåˆ†ç±»å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºåˆ†ç±»å¤±è´¥: {str(e)}")

# å¯¼å‡ºè·¯ç”±å™¨ä¾›main.pyä½¿ç”¨
@router.post("/analyze-document")
async def analyze_document_ai(
    file_id: int,
    enable_vision: bool = True,
    force_reanalyze: bool = False,
    db: Session = Depends(get_db)
):
    """å¯¹å·²ä¸Šä¼ çš„æ–‡æ¡£è¿›è¡ŒAIåˆ†æ"""
    try:
        # è·å–æ–‡ä»¶ä¿¡æ¯
        file_record = db.query(ManagedFile).filter(ManagedFile.id == file_id).first()
        if not file_record:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        if not os.path.exists(file_record.storage_path):
            raise HTTPException(status_code=404, detail="ç‰©ç†æ–‡ä»¶ä¸å­˜åœ¨")
        
        # è¿›è¡ŒAIåˆ†æ
        from ai_service import ai_service
        
        if enable_vision:
            analysis_result = await ai_service.smart_document_analysis(
                file_record.storage_path,
                enable_vision=True
            )
        else:
            analysis_result = await ai_service.smart_document_analysis(file_record.storage_path)
        
        if not analysis_result.get("success"):
            raise HTTPException(status_code=500, detail=f"AIåˆ†æå¤±è´¥: {analysis_result.get('error')}")
        
        # æå–åˆ†ç±»ä¿¡æ¯
        if enable_vision:
            classification = analysis_result["results"]["final_classification"]
        else:
            classification = analysis_result.get("classification")
        
        # æå–æ ‡ç­¾å»ºè®®
        tag_result = await ai_service.extract_document_tags(
            file_record.storage_path,
            existing_tags=file_record.tags or []
        )
        
        suggested_tags = tag_result.get("suggested_tags", []) if tag_result.get("success") else []
        
        # å¤„ç†å¾‹å¸ˆè¯çš„ç‰¹æ®Šé€»è¾‘
        lawyer_cert_created = None
        if classification and classification.get("category") == "lawyer_certificate":
            try:
                lawyer_cert_created = await handle_lawyer_certificate_creation(
                    file_record, classification, analysis_result, db
                )
            except Exception as lawyer_err:
                logger.warning(f"åˆ›å»ºå¾‹å¸ˆè¯è®°å½•å¤±è´¥: {lawyer_err}")
        
        # å¦‚æœéœ€è¦ï¼Œæ›´æ–°æ–‡ä»¶è®°å½•
        if force_reanalyze and classification:
            # ä¿®å¤åˆ†ç±»å­—æ®µè®¿é—® - ä½¿ç”¨typeè€Œä¸æ˜¯category
            new_category = classification.get("type", file_record.category)
            if new_category:
                file_record.category = new_category
                logger.info(f"æ›´æ–°æ–‡ä»¶åˆ†ç±»: {file_record.id} -> {new_category}")
            
            if tag_result.get("success"):
                file_record.tags = tag_result.get("all_tags", file_record.tags)
            
            # æ›´æ–°æè¿°ä¿¡æ¯
            if classification.get("summary") and not file_record.description:
                file_record.description = classification["summary"]
            
            # æ›´æ–°å¤„ç†ç»“æœ
            if not file_record.processing_result:
                file_record.processing_result = {}
            file_record.processing_result["ai_analysis"] = analysis_result
            file_record.processing_result["classification"] = classification
            
            db.commit()
            logger.info(f"æ–‡ä»¶è®°å½•å·²æ›´æ–°: åˆ†ç±»={new_category}, æ ‡ç­¾={file_record.tags}")
        
        return {
            "success": True,
            "file_id": file_id,
            "analysis_result": analysis_result,
            "classification": classification,
            "suggested_tags": suggested_tags,
            "updated": force_reanalyze,
            "lawyer_certificate_created": lawyer_cert_created
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ–‡æ¡£AIåˆ†æå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ†æå¤±è´¥: {str(e)}")

@router.post("/batch-analyze")
async def batch_analyze_documents(
    file_ids: List[int],
    enable_vision: bool = True,
    update_records: bool = False,
    db: Session = Depends(get_db)
):
    """æ‰¹é‡åˆ†ææ–‡æ¡£"""
    try:
        results = []
        
        for file_id in file_ids:
            try:
                # åˆ†æå•ä¸ªæ–‡æ¡£
                analysis_result = await analyze_document_ai(
                    file_id=file_id,
                    enable_vision=enable_vision,
                    force_reanalyze=update_records,
                    db=db
                )
                results.append({
                    "file_id": file_id,
                    "success": True,
                    "result": analysis_result
                })
            except Exception as e:
                results.append({
                    "file_id": file_id,
                    "success": False,
                    "error": str(e)
                })
        
        success_count = sum(1 for r in results if r["success"])
        
        return {
            "success": True,
            "total": len(file_ids),
            "success_count": success_count,
            "failed_count": len(file_ids) - success_count,
            "results": results
        }
        
    except Exception as e:
        logger.error(f"æ‰¹é‡åˆ†æå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡åˆ†æå¤±è´¥: {str(e)}")

async def handle_lawyer_certificate_creation(file_record, classification, analysis_result, db):
    """å¤„ç†å¾‹å¸ˆè¯çš„åˆ›å»ºé€»è¾‘"""
    try:
        from models import LawyerCertificate, LawyerCertificateFile
        
        # æå–å¾‹å¸ˆè¯ä¿¡æ¯
        lawyer_info = classification.get("lawyer_info", {})
        key_entities = classification.get("key_entities", {})
        
        # å¿…è¦å­—æ®µæ£€æŸ¥
        lawyer_name = lawyer_info.get("name") or key_entities.get("holder_name")
        certificate_number = lawyer_info.get("certificate_number") or key_entities.get("certificate_number")
        law_firm = lawyer_info.get("law_firm") or classification.get("description", "")
        
        if not lawyer_name or not certificate_number:
            logger.warning("å¾‹å¸ˆè¯ä¿¡æ¯ä¸å®Œæ•´ï¼Œè·³è¿‡åˆ›å»º")
            return None
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        existing = db.query(LawyerCertificate).filter(
            LawyerCertificate.certificate_number == certificate_number
        ).first()
        
        if existing:
            logger.info(f"å¾‹å¸ˆè¯å·²å­˜åœ¨: {certificate_number}")
            return {"id": existing.id, "action": "exists"}
        
        # åˆ›å»ºå¾‹å¸ˆè¯è®°å½•
        lawyer_cert = LawyerCertificate(
            lawyer_name=lawyer_name,
            certificate_number=certificate_number,
            law_firm=law_firm,
            issuing_authority=lawyer_info.get("issuing_authority"),
            age=lawyer_info.get("age"),
            id_number=lawyer_info.get("id_number"),
            source_document=file_record.storage_path,
            ai_analysis=analysis_result,
            confidence_score=classification.get("confidence", 0.0),
            extracted_text=analysis_result.get("results", {}).get("text_extraction_result", {}).get("extracted_content", {}).get("text", ""),
            is_verified=False,
            is_manual_input=False
        )
        
        # å¤„ç†èŒä½æ ‡ç­¾
        position = "å¾‹å¸ˆ"  # é»˜è®¤
        if "åˆä¼™äºº" in str(classification.get("description", "")):
            position = "åˆä¼™äºº"
        lawyer_cert.position = position
        lawyer_cert.position_tags = [position]
        
        # å¤„ç†ä¸šåŠ¡é¢†åŸŸæ ‡ç­¾
        business_field = classification.get("business_field")
        if business_field:
            lawyer_cert.business_field_tags = [business_field]
        
        db.add(lawyer_cert)
        db.flush()  # è·å–ID
        
        # åˆ›å»ºæ–‡ä»¶å…³è”
        cert_file = LawyerCertificateFile(
            certificate_id=lawyer_cert.id,
            file_path=file_record.storage_path,
            file_type="original",
            file_name=file_record.original_filename,
            file_size=file_record.file_size
        )
        
        db.add(cert_file)
        db.commit()
        
        logger.info(f"å¾‹å¸ˆè¯åˆ›å»ºæˆåŠŸ: {lawyer_name} ({certificate_number})")
        
        return {
            "id": lawyer_cert.id,
            "action": "created",
            "lawyer_name": lawyer_name,
            "certificate_number": certificate_number,
            "law_firm": law_firm,
            "position": position
        }
        
    except Exception as e:
        db.rollback()
        logger.error(f"åˆ›å»ºå¾‹å¸ˆè¯è®°å½•å¤±è´¥: {str(e)}")
        raise e

@router.post("/{file_id}/create-lawyer-certificate")
async def create_lawyer_certificate_from_file(
    file_id: int,
    lawyer_name: str = Form(...),
    certificate_number: str = Form(...),
    law_firm: str = Form(...),
    issuing_authority: Optional[str] = Form(None),
    age: Optional[int] = Form(None),
    id_number: Optional[str] = Form(None),
    issue_date: Optional[str] = Form(None),
    position: Optional[str] = Form("å¾‹å¸ˆ"),
    position_tags: Optional[str] = Form(None),
    business_field_tags: Optional[str] = Form(None),
    custom_tags: Optional[str] = Form(None),
    verification_notes: Optional[str] = Form(None),
    db: Session = Depends(get_db)
):
    """ä»æ–‡ä»¶ç®¡ç†åˆ›å»ºå¾‹å¸ˆè¯è®°å½•ï¼ˆåŒå‘ç®¡ç†ï¼‰"""
    try:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        file_record = db.query(ManagedFile).filter(ManagedFile.id == file_id).first()
        if not file_record:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        # æ£€æŸ¥è¯ä¹¦å·æ˜¯å¦å·²å­˜åœ¨
        from models import LawyerCertificate, LawyerCertificateFile
        existing = db.query(LawyerCertificate).filter(
            LawyerCertificate.certificate_number == certificate_number
        ).first()
        if existing:
            raise HTTPException(status_code=409, detail="æ‰§ä¸šè¯å·å·²å­˜åœ¨")
        
        # è§£ææ ‡ç­¾
        position_tags_list = []
        business_field_tags_list = []
        custom_tags_list = []
        
        if position_tags:
            try:
                position_tags_list = json.loads(position_tags)
            except:
                position_tags_list = [tag.strip() for tag in position_tags.split(",") if tag.strip()]
        
        if business_field_tags:
            try:
                business_field_tags_list = json.loads(business_field_tags)
            except:
                business_field_tags_list = [tag.strip() for tag in business_field_tags.split(",") if tag.strip()]
        
        if custom_tags:
            try:
                custom_tags_list = json.loads(custom_tags)
            except:
                custom_tags_list = [tag.strip() for tag in custom_tags.split(",") if tag.strip()]
        
        # è§£ææ—¥æœŸ
        issue_date_obj = None
        if issue_date:
            try:
                issue_date_obj = datetime.fromisoformat(issue_date.replace('Z', '+00:00'))
            except:
                pass
        
        # åˆ›å»ºå¾‹å¸ˆè¯è®°å½•
        lawyer_cert = LawyerCertificate(
            lawyer_name=lawyer_name,
            certificate_number=certificate_number,
            law_firm=law_firm,
            issuing_authority=issuing_authority,
            age=age,
            id_number=id_number,
            issue_date=issue_date_obj,
            position=position,
            position_tags=position_tags_list,
            business_field_tags=business_field_tags_list,
            custom_tags=custom_tags_list,
            verification_notes=verification_notes,
            source_document=file_record.storage_path,
            is_verified=True,  # æ‰‹åŠ¨åˆ›å»ºé»˜è®¤å·²éªŒè¯
            is_manual_input=True  # ä»æ–‡ä»¶ç®¡ç†æ‰‹åŠ¨åˆ›å»º
        )
        
        db.add(lawyer_cert)
        db.flush()  # è·å–ID
        
        # åˆ›å»ºæ–‡ä»¶å…³è”
        cert_file = LawyerCertificateFile(
            certificate_id=lawyer_cert.id,
            file_path=file_record.storage_path,
            file_type="from_file_management",
            file_name=file_record.original_filename,
            file_size=file_record.file_size
        )
        
        db.add(cert_file)
        
        # æ›´æ–°æ–‡ä»¶è®°å½•çš„åˆ†ç±»
        file_record.category = "lawyer_certificate"
        if not file_record.description:
            file_record.description = f"å¾‹å¸ˆè¯æ–‡æ¡£ï¼š{lawyer_name}ï¼ˆ{certificate_number}ï¼‰"
        
        db.commit()
        
        logger.info(f"ä»æ–‡ä»¶ç®¡ç†åˆ›å»ºå¾‹å¸ˆè¯æˆåŠŸ: {lawyer_name} ({certificate_number})")
        
        return {
            "success": True,
            "message": "å¾‹å¸ˆè¯åˆ›å»ºæˆåŠŸ",
            "certificate_id": lawyer_cert.id,
            "file_updated": True
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ä»æ–‡ä»¶ç®¡ç†åˆ›å»ºå¾‹å¸ˆè¯å¤±è´¥: {str(e)}")
        db.rollback()
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºå¤±è´¥: {str(e)}")

@router.get("/{file_id}/lawyer-certificate")
async def get_related_lawyer_certificate(file_id: int, db: Session = Depends(get_db)):
    """æŸ¥çœ‹æ–‡ä»¶å…³è”çš„å¾‹å¸ˆè¯ä¿¡æ¯"""
    try:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        file_record = db.query(ManagedFile).filter(ManagedFile.id == file_id).first()
        if not file_record:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        # æŸ¥æ‰¾å…³è”çš„å¾‹å¸ˆè¯
        from models import LawyerCertificate, LawyerCertificateFile
        
        # é€šè¿‡æ–‡ä»¶è·¯å¾„æŸ¥æ‰¾å¾‹å¸ˆè¯
        cert_file = db.query(LawyerCertificateFile).filter(
            LawyerCertificateFile.file_path == file_record.storage_path
        ).first()
        
        if not cert_file:
            # é€šè¿‡æºæ–‡æ¡£æŸ¥æ‰¾
            cert = db.query(LawyerCertificate).filter(
                LawyerCertificate.source_document == file_record.storage_path
            ).first()
            
            if not cert:
                return {
                    "success": True,
                    "has_lawyer_certificate": False,
                    "message": "è¯¥æ–‡ä»¶æœªå…³è”å¾‹å¸ˆè¯"
                }
        else:
            cert = cert_file.certificate
        
        return {
            "success": True,
            "has_lawyer_certificate": True,
            "lawyer_certificate": {
                "id": cert.id,
                "lawyer_name": cert.lawyer_name,
                "certificate_number": cert.certificate_number,
                "law_firm": cert.law_firm,
                "position": cert.position,
                "is_verified": cert.is_verified,
                "created_at": cert.created_at.isoformat()
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æŸ¥çœ‹æ–‡ä»¶å…³è”å¾‹å¸ˆè¯å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æŸ¥çœ‹å¤±è´¥: {str(e)}")

def setup_router(app):
    app.include_router(router) 