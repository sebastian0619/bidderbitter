#!/usr/bin/env python3
"""æ–‡ä»¶ç®¡ç†API"""

from fastapi import APIRouter, Depends, File, UploadFile, Form, HTTPException, BackgroundTasks
from fastapi.responses import FileResponse
from sqlalchemy.orm import Session, joinedload
from sqlalchemy import and_, or_, desc, asc, func
from typing import List, Optional, Dict, Any
from datetime import datetime, timedelta
import os
import hashlib
import shutil
import json
import mimetypes
import pytz

from database import get_db
from models import ManagedFile, FileVersion, FileUsage, FileCategory, LawyerCertificate, LawyerCertificateFile, SystemSettings, AITask
from schemas import *
import logging
from ai_service import create_ai_task, update_ai_task

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/files", tags=["file_management"])

# æ–‡ä»¶å­˜å‚¨è·¯å¾„é…ç½®
TEMP_FILES_PATH = "/app/uploads/temp"
PERMANENT_FILES_PATH = "/app/uploads/permanent"
PROCESSED_FILES_PATH = "/app/uploads/processed"

# ç¡®ä¿ç›®å½•å­˜åœ¨
for path in [TEMP_FILES_PATH, PERMANENT_FILES_PATH, PROCESSED_FILES_PATH]:
    os.makedirs(path, exist_ok=True)

# ============ è¾…åŠ©å‡½æ•° ============

def calculate_file_hash(file_path: str) -> str:
    """è®¡ç®—æ–‡ä»¶MD5å“ˆå¸Œå€¼"""
    hash_md5 = hashlib.md5()
    with open(file_path, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()

def get_file_type_from_mime(mime_type: str) -> str:
    """æ ¹æ®MIMEç±»å‹ç¡®å®šæ–‡ä»¶ç±»å‹"""
    if mime_type.startswith('image/'):
        return 'image'
    elif mime_type == 'application/pdf':
        return 'pdf'
    elif mime_type.startswith('text/'):
        return 'text'
    elif 'word' in mime_type or 'document' in mime_type:
        return 'document'
    elif 'spreadsheet' in mime_type or 'excel' in mime_type:
        return 'spreadsheet'
    elif 'presentation' in mime_type or 'powerpoint' in mime_type:
        return 'presentation'
    else:
        return 'other'

def cleanup_expired_files(db: Session):
    """æ¸…ç†è¿‡æœŸçš„ä¸´æ—¶æ–‡ä»¶"""
    try:
        # æ¸…ç†è¿‡æœŸçš„ä¸Šä¼ ä¸´æ—¶æ–‡ä»¶ï¼ˆ30å¤©ï¼‰
        expired_upload_files = db.query(ManagedFile).filter(
            and_(
                ManagedFile.file_category == "temporary_upload",
                ManagedFile.expires_at < datetime.now(),
                ManagedFile.is_archived == False
            )
        ).all()
        
        # æ¸…ç†è¿‡æœŸçš„ç”Ÿæˆæ–‡ä»¶ï¼ˆ180å¤©ï¼‰
        expired_generated_files = db.query(ManagedFile).filter(
            and_(
                ManagedFile.file_category == "temporary_generated",
                ManagedFile.expires_at < datetime.now(),
                ManagedFile.is_archived == False
            )
        ).all()
        
        expired_files = expired_upload_files + expired_generated_files
        
        for file in expired_files:
            try:
                # åˆ é™¤ç‰©ç†æ–‡ä»¶
                if os.path.exists(file.storage_path):
                    os.remove(file.storage_path)
                if file.processed_path and os.path.exists(file.processed_path):
                    os.remove(file.processed_path)
                
                # æ ‡è®°ä¸ºå·²å½’æ¡£
                file.is_archived = True
                file.archived_at = datetime.now()
                
            except Exception as e:
                logger.error(f"æ¸…ç†æ–‡ä»¶å¤±è´¥ {file.id}: {e}")
        
        db.commit()
        logger.info(f"æ¸…ç†äº† {len(expired_files)} ä¸ªè¿‡æœŸä¸´æ—¶æ–‡ä»¶")
        
    except Exception as e:
        logger.error(f"æ¸…ç†è¿‡æœŸæ–‡ä»¶å¤±è´¥: {e}")
        db.rollback()

# ============ APIç«¯ç‚¹ ============

@router.post("/upload/temporary")
async def upload_temporary_file(
    file: UploadFile = File(...),
    category: Optional[str] = Form(None),
    description: Optional[str] = Form(None),
    tags: Optional[str] = Form(None),  # JSONå­—ç¬¦ä¸²
    expires_hours: int = Form(24),  # é»˜è®¤24å°æ—¶è¿‡æœŸ
    db: Session = Depends(get_db)
):
    """ä¸Šä¼ ä¸´æ—¶æ–‡ä»¶ï¼ˆå•æ–‡ä»¶ï¼‰"""
    try:
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        content = await file.read()
        file_size = len(content)
        
        if file_size > 100 * 1024 * 1024:  # 100MBé™åˆ¶
            raise HTTPException(status_code=413, detail="æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶(100MB)")
        
        # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
        await file.seek(0)
        
        # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
        file_hash = hashlib.md5(content).hexdigest()
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒæ–‡ä»¶
        existing_file = db.query(ManagedFile).filter(
            and_(
                ManagedFile.file_hash == file_hash,
                ManagedFile.is_archived == False
            )
        ).first()
        
        if existing_file:
            # æ›´æ–°è®¿é—®æ—¶é—´
            existing_file.access_count += 1
            existing_file.last_accessed = datetime.now()
            db.commit()
            
            return {
                "success": True,
                "message": "æ–‡ä»¶å·²å­˜åœ¨ï¼Œè¿”å›ç°æœ‰æ–‡ä»¶ä¿¡æ¯",
                "file_id": existing_file.id,
                "is_duplicate": True
            }
        
        # ç”Ÿæˆå­˜å‚¨è·¯å¾„
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename_base = os.path.splitext(file.filename)[0]
        file_ext = os.path.splitext(file.filename)[1]
        storage_filename = f"temp_{timestamp}_{file_hash[:8]}{file_ext}"
        storage_path = os.path.join(TEMP_FILES_PATH, storage_filename)
        
        # ä¿å­˜æ–‡ä»¶
        with open(storage_path, "wb") as f:
            await file.seek(0)
            shutil.copyfileobj(file.file, f)
        
        # è·å–MIMEç±»å‹
        mime_type, _ = mimetypes.guess_type(file.filename)
        if not mime_type:
            mime_type = "application/octet-stream"
        
        # è§£ææ ‡ç­¾
        tags_list = []
        if tags:
            try:
                tags_list = json.loads(tags)
            except:
                tags_list = [tag.strip() for tag in tags.split(",") if tag.strip()]
        
        # åˆ›å»ºæ•°æ®åº“è®°å½•
        db_file = ManagedFile(
            original_filename=file.filename,
            display_name=file.filename,
            storage_path=storage_path,
            file_type=get_file_type_from_mime(mime_type),
            mime_type=mime_type,
            file_size=file_size,
            file_hash=file_hash,
            file_category="temporary_upload",  # ä¸Šä¼ çš„ä¸´æ—¶æ–‡ä»¶
            category=category,
            tags=tags_list,
            description=description,
            expires_at=datetime.now() + timedelta(days=30),  # 30å¤©è¿‡æœŸ
            access_count=1,
            last_accessed=datetime.now()
        )
        
        db.add(db_file)
        db.commit()
        db.refresh(db_file)
        
        logger.info(f"ä¸´æ—¶æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {file.filename} -> {storage_path}")
        
        return {
            "success": True,
            "message": "ä¸´æ—¶æ–‡ä»¶ä¸Šä¼ æˆåŠŸ",
            "file_id": db_file.id,
            "filename": db_file.original_filename,
            "file_size": db_file.file_size,
            "expires_at": db_file.expires_at.isoformat(),
            "is_duplicate": False
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ä¸Šä¼ ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ä¸Šä¼ å¤±è´¥: {str(e)}")

@router.post("/upload/permanent")
async def upload_permanent_file(
    file: UploadFile = File(...),
    display_name: str = Form(...),
    category: str = Form(None),  # å¯é€‰ï¼Œå¦‚æœä¸æä¾›å°†ä½¿ç”¨AIåˆ†ç±»
    description: Optional[str] = Form(None),
    tags: Optional[str] = Form(None),  # JSONå­—ç¬¦ä¸²
    keywords: Optional[str] = Form(None),
    is_public: bool = Form(True),
    enable_ai_classification: bool = Form(True),  # æ˜¯å¦å¯ç”¨AIåˆ†ç±»
    enable_vision_analysis: bool = Form(True),  # æ˜¯å¦å¯ç”¨è§†è§‰åˆ†æ
    db: Session = Depends(get_db)
):
    """ä¸Šä¼ å¸¸é©»æ–‡ä»¶"""
    try:
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        content = await file.read()
        file_size = len(content)
        
        if file_size > 200 * 1024 * 1024:  # 200MBé™åˆ¶
            raise HTTPException(status_code=413, detail="æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶(200MB)")
        
        # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
        file_hash = hashlib.md5(content).hexdigest()
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒæ–‡ä»¶
        existing_file = db.query(ManagedFile).filter(
            and_(
                ManagedFile.file_hash == file_hash,
                ManagedFile.file_category == "permanent",
                ManagedFile.is_archived == False
            )
        ).first()
        
        if existing_file:
            # æ›´æ–°è®¿é—®æ—¶é—´å’Œè®¡æ•°ï¼Œè¿”å›ç°æœ‰æ–‡ä»¶ä¿¡æ¯è€Œä¸æ˜¯æŠ›å‡ºé”™è¯¯
            existing_file.access_count += 1
            existing_file.last_accessed = datetime.now()
            db.commit()
            
            return {
                "success": True,
                "message": "ç›¸åŒæ–‡ä»¶å·²å­˜åœ¨ï¼Œè¿”å›ç°æœ‰æ–‡ä»¶ä¿¡æ¯",
                "file_id": existing_file.id,
                "display_name": existing_file.display_name,
                "file_size": existing_file.file_size,
                "category": existing_file.category,
                "tags": existing_file.tags or [],
                "is_duplicate": True
            }
        
        # ç”Ÿæˆå­˜å‚¨è·¯å¾„
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        file_ext = os.path.splitext(file.filename)[1]
        storage_filename = f"perm_{timestamp}_{file_hash[:8]}{file_ext}"
        storage_path = os.path.join(PERMANENT_FILES_PATH, storage_filename)
        
        # ä¿å­˜æ–‡ä»¶
        with open(storage_path, "wb") as f:
            f.write(content)
        
        # è·å–MIMEç±»å‹
        mime_type, _ = mimetypes.guess_type(file.filename)
        if not mime_type:
            mime_type = "application/octet-stream"
        
        # è§£ææ ‡ç­¾
        tags_list = []
        if tags:
            try:
                tags_list = json.loads(tags)
            except:
                tags_list = [tag.strip() for tag in tags.split(",") if tag.strip()]
        
        # ç¡®å®šåˆå§‹åˆ†ç±»ï¼ˆå¦‚æœç”¨æˆ·æœªæŒ‡å®šï¼Œä½¿ç”¨é»˜è®¤å€¼ï¼‰
        final_category = category if category else "other"
        final_tags = tags_list.copy()
        
        # åˆ›å»ºæ•°æ®åº“è®°å½•
        db_file = ManagedFile(
            original_filename=file.filename,
            display_name=display_name,
            storage_path=storage_path,
            file_type=get_file_type_from_mime(mime_type),
            mime_type=mime_type,
            file_size=file_size,
            file_hash=file_hash,
            file_category="permanent",
            category=final_category,
            tags=final_tags,
            description=description,
            keywords=keywords,
            is_public=is_public,
            access_count=0,
            last_accessed=datetime.now()
        )
        
        db.add(db_file)
        db.commit()
        db.refresh(db_file)
        
        # åˆ›å»ºAIåˆ†æä»»åŠ¡
        task_id = None
        if enable_ai_classification:
            task_id = create_ai_task(db, db_file.id, "permanent_file")
            logger.info(f"ğŸ“‹ å¸¸é©»æ–‡ä»¶AIä»»åŠ¡å·²åˆ›å»º: ä»»åŠ¡ID={task_id}, æ–‡ä»¶ID={db_file.id}")
            
            # å¯åŠ¨å¼‚æ­¥åå°AIåˆ†æ
            import asyncio
            asyncio.create_task(
                analyze_permanent_file_in_background(
                    db_file.id,
                    enable_vision_analysis,
                    task_id,
                    user_category=category  # ä¼ é€’ç”¨æˆ·æŒ‡å®šçš„åˆ†ç±»
                )
            )
            logger.info(f"ğŸ¤– å·²å¯åŠ¨åå°AIåˆ†æä»»åŠ¡ï¼Œæ–‡ä»¶ID={db_file.id}")
        
        logger.info(f"å¸¸é©»æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {display_name} -> {storage_path}")
        
        return {
            "success": True,
            "message": "å¸¸é©»æ–‡ä»¶ä¸Šä¼ æˆåŠŸ",
            "file_id": db_file.id,
            "display_name": db_file.display_name,
            "file_size": db_file.file_size,
            "category": final_category,
            "tags": final_tags,
            "ai_analysis": {
                "enabled": enable_ai_classification,
                "status": "background_processing" if enable_ai_classification else "disabled",
                "task_id": task_id
            },
            "task_id": task_id  # è¿”å›ä»»åŠ¡ID
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ä¸Šä¼ å¸¸é©»æ–‡ä»¶å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ä¸Šä¼ å¤±è´¥: {str(e)}")

@router.post("/upload/temporary/batch")
async def upload_temporary_files_batch(
    files: List[UploadFile] = File(...),
    category: Optional[str] = Form(None),
    description: Optional[str] = Form(None),
    tags: Optional[str] = Form(None),  # JSONå­—ç¬¦ä¸²
    expires_hours: int = Form(24),  # é»˜è®¤24å°æ—¶è¿‡æœŸ
    db: Session = Depends(get_db)
):
    """æ‰¹é‡ä¸Šä¼ ä¸´æ—¶æ–‡ä»¶"""
    try:
        uploaded_files = []
        failed_files = []
        
        # è§£ææ ‡ç­¾
        tags_list = []
        if tags:
            try:
                tags_list = json.loads(tags)
            except:
                tags_list = [tag.strip() for tag in tags.split(",") if tag.strip()]
        
        for i, file in enumerate(files):
            try:
                # æ£€æŸ¥æ–‡ä»¶å¤§å°
                content = await file.read()
                file_size = len(content)
                
                if file_size > 100 * 1024 * 1024:  # 100MBé™åˆ¶
                    failed_files.append({
                        "filename": file.filename,
                        "error": "æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶(100MB)"
                    })
                    continue
                
                # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
                file_hash = hashlib.md5(content).hexdigest()
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒæ–‡ä»¶
                existing_file = db.query(ManagedFile).filter(
                    and_(
                        ManagedFile.file_hash == file_hash,
                        ManagedFile.is_archived == False
                    )
                ).first()
                
                if existing_file:
                    # æ›´æ–°è®¿é—®æ—¶é—´
                    existing_file.access_count += 1
                    existing_file.last_accessed = datetime.now()
                    uploaded_files.append({
                        "file_id": existing_file.id,
                        "filename": file.filename,
                        "file_size": file_size,
                        "is_duplicate": True,
                        "message": "æ–‡ä»¶å·²å­˜åœ¨"
                    })
                    continue
                
                # ç”Ÿæˆå­˜å‚¨è·¯å¾„
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                file_ext = os.path.splitext(file.filename)[1]
                storage_filename = f"temp_{timestamp}_{i}_{file_hash[:8]}{file_ext}"
                storage_path = os.path.join(TEMP_FILES_PATH, storage_filename)
                
                # ä¿å­˜æ–‡ä»¶
                with open(storage_path, "wb") as f:
                    f.write(content)
                
                # è·å–MIMEç±»å‹
                mime_type, _ = mimetypes.guess_type(file.filename)
                if not mime_type:
                    mime_type = "application/octet-stream"
                
                # åˆ›å»ºæ•°æ®åº“è®°å½•
                db_file = ManagedFile(
                    original_filename=file.filename,
                    display_name=file.filename,
                    storage_path=storage_path,
                    file_type=get_file_type_from_mime(mime_type),
                    mime_type=mime_type,
                    file_size=file_size,
                    file_hash=file_hash,
                    file_category="temporary_upload",
                    category=category,
                    tags=tags_list,
                    description=description,
                    expires_at=datetime.now() + timedelta(hours=expires_hours),
                    access_count=1,
                    last_accessed=datetime.now()
                )
                
                db.add(db_file)
                db.flush()  # è·å–IDä½†ä¸æäº¤
                
                uploaded_files.append({
                    "file_id": db_file.id,
                    "filename": file.filename,
                    "file_size": file_size,
                    "is_duplicate": False,
                    "expires_at": db_file.expires_at.isoformat()
                })
                
                logger.info(f"æ‰¹é‡ä¸´æ—¶æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {file.filename}")
                
            except Exception as file_err:
                failed_files.append({
                    "filename": file.filename,
                    "error": str(file_err)
                })
                logger.error(f"ä¸Šä¼ æ–‡ä»¶å¤±è´¥ {file.filename}: {file_err}")
        
        db.commit()
        
        return {
            "success": True,
            "message": f"æ‰¹é‡ä¸Šä¼ å®Œæˆ: {len(uploaded_files)} æˆåŠŸ, {len(failed_files)} å¤±è´¥",
            "uploaded_files": uploaded_files,
            "failed_files": failed_files,
            "summary": {
                "total": len(files),
                "success": len(uploaded_files),
                "failed": len(failed_files)
            }
        }
        
    except Exception as e:
        db.rollback()
        logger.error(f"æ‰¹é‡ä¸Šä¼ ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡ä¸Šä¼ å¤±è´¥: {str(e)}")

@router.post("/upload/permanent/batch")
async def upload_permanent_files_batch(
    files: List[UploadFile] = File(...),
    display_names: Optional[str] = Form(None),  # JSONæ•°ç»„ï¼Œæ¯ä¸ªæ–‡ä»¶çš„æ˜¾ç¤ºåç§°
    category: Optional[str] = Form(None),
    description: Optional[str] = Form(None),
    tags: Optional[str] = Form(None),  # JSONå­—ç¬¦ä¸²
    keywords: Optional[str] = Form(None),
    is_public: bool = Form(True),
    enable_ai_classification: bool = Form(True),
    enable_vision_analysis: bool = Form(True),
    db: Session = Depends(get_db)
):
    """æ‰¹é‡ä¸Šä¼ å¸¸é©»æ–‡ä»¶"""
    try:
        uploaded_files = []
        failed_files = []
        ai_tasks = []
        
        # è§£ææ˜¾ç¤ºåç§°
        display_names_list = []
        if display_names:
            try:
                display_names_list = json.loads(display_names)
            except:
                display_names_list = []
        
        # è§£ææ ‡ç­¾
        tags_list = []
        if tags:
            try:
                tags_list = json.loads(tags)
            except:
                tags_list = [tag.strip() for tag in tags.split(",") if tag.strip()]
        
        for i, file in enumerate(files):
            try:
                # æ£€æŸ¥æ–‡ä»¶å¤§å°
                content = await file.read()
                file_size = len(content)
                
                if file_size > 200 * 1024 * 1024:  # 200MBé™åˆ¶
                    failed_files.append({
                        "filename": file.filename,
                        "error": "æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶(200MB)"
                    })
                    continue
                
                # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
                file_hash = hashlib.md5(content).hexdigest()
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒæ–‡ä»¶
                existing_file = db.query(ManagedFile).filter(
                    and_(
                        ManagedFile.file_hash == file_hash,
                        ManagedFile.file_category == "permanent",
                        ManagedFile.is_archived == False
                    )
                ).first()
                
                if existing_file:
                    existing_file.access_count += 1
                    existing_file.last_accessed = datetime.now()
                    uploaded_files.append({
                        "file_id": existing_file.id,
                        "filename": file.filename,
                        "display_name": existing_file.display_name,
                        "file_size": file_size,
                        "is_duplicate": True,
                        "message": "æ–‡ä»¶å·²å­˜åœ¨"
                    })
                    continue
                
                # ç¡®å®šæ˜¾ç¤ºåç§°
                display_name = display_names_list[i] if i < len(display_names_list) else file.filename
                
                # ç”Ÿæˆå­˜å‚¨è·¯å¾„
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                file_ext = os.path.splitext(file.filename)[1]
                storage_filename = f"perm_{timestamp}_{i}_{file_hash[:8]}{file_ext}"
                storage_path = os.path.join(PERMANENT_FILES_PATH, storage_filename)
                
                # ä¿å­˜æ–‡ä»¶
                with open(storage_path, "wb") as f:
                    f.write(content)
                
                # è·å–MIMEç±»å‹
                mime_type, _ = mimetypes.guess_type(file.filename)
                if not mime_type:
                    mime_type = "application/octet-stream"
                
                # ç¡®å®šåˆå§‹åˆ†ç±»
                final_category = category if category else "other"
                
                # åˆ›å»ºæ•°æ®åº“è®°å½•
                db_file = ManagedFile(
                    original_filename=file.filename,
                    display_name=display_name,
                    storage_path=storage_path,
                    file_type=get_file_type_from_mime(mime_type),
                    mime_type=mime_type,
                    file_size=file_size,
                    file_hash=file_hash,
                    file_category="permanent",
                    category=final_category,
                    tags=tags_list.copy(),
                    description=description,
                    keywords=keywords,
                    is_public=is_public,
                    access_count=0,
                    last_accessed=datetime.now()
                )
                
                db.add(db_file)
                db.flush()  # è·å–IDä½†ä¸æäº¤
                
                # åˆ›å»ºAIåˆ†æä»»åŠ¡
                task_id = None
                if enable_ai_classification:
                    task_id = create_ai_task(db, db_file.id, "permanent_file")
                    ai_tasks.append({
                        "file_id": db_file.id,
                        "task_id": task_id,
                        "filename": file.filename
                    })
                
                uploaded_files.append({
                    "file_id": db_file.id,
                    "filename": file.filename,
                    "display_name": display_name,
                    "file_size": file_size,
                    "category": final_category,
                    "is_duplicate": False,
                    "task_id": task_id
                })
                
                logger.info(f"æ‰¹é‡å¸¸é©»æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {display_name}")
                
            except Exception as file_err:
                failed_files.append({
                    "filename": file.filename,
                    "error": str(file_err)
                })
                logger.error(f"ä¸Šä¼ æ–‡ä»¶å¤±è´¥ {file.filename}: {file_err}")
        
        db.commit()
        
        # å¯åŠ¨å¼‚æ­¥AIåˆ†æä»»åŠ¡
        if enable_ai_classification and ai_tasks:
            import asyncio
            for task in ai_tasks:
                asyncio.create_task(
                    analyze_permanent_file_in_background(
                        task["file_id"],
                        enable_vision_analysis,
                        task["task_id"],
                        user_category=category
                    )
                )
            logger.info(f"ğŸ¤– å·²å¯åŠ¨ {len(ai_tasks)} ä¸ªåå°AIåˆ†æä»»åŠ¡")
        
        return {
            "success": True,
            "message": f"æ‰¹é‡ä¸Šä¼ å®Œæˆ: {len(uploaded_files)} æˆåŠŸ, {len(failed_files)} å¤±è´¥",
            "uploaded_files": uploaded_files,
            "failed_files": failed_files,
            "ai_analysis": {
                "enabled": enable_ai_classification,
                "tasks_created": len(ai_tasks),
                "status": "background_processing" if ai_tasks else "disabled"
            },
            "summary": {
                "total": len(files),
                "success": len(uploaded_files),
                "failed": len(failed_files)
            }
        }
        
    except Exception as e:
        db.rollback()
        logger.error(f"æ‰¹é‡ä¸Šä¼ å¸¸é©»æ–‡ä»¶å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡ä¸Šä¼ å¤±è´¥: {str(e)}")

@router.get("/list")
async def list_files(
    file_category: Optional[str] = None,  # temporary, permanent
    category: Optional[str] = None,
    search: Optional[str] = None,
    tags: Optional[str] = None,
    page: int = 1,
    page_size: int = 20,
    sort_by: str = "created_at",
    sort_order: str = "desc",
    include_archived: bool = False,
    db: Session = Depends(get_db)
):
    """è·å–æ–‡ä»¶åˆ—è¡¨"""
    try:
        # æ¸…ç†è¿‡æœŸæ–‡ä»¶
        if file_category in ["temporary_upload", "temporary_generated", "temporary"] or file_category is None:
            cleanup_expired_files(db)
        
        # æ„å»ºæŸ¥è¯¢
        query = db.query(ManagedFile)
        
        # åŸºç¡€è¿‡æ»¤
        if not include_archived:
            query = query.filter(ManagedFile.is_archived == False)
        
        if file_category:
            if file_category == "temporary":
                # æŸ¥è¯¢æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶ç±»å‹
                query = query.filter(
                    or_(
                        ManagedFile.file_category == "temporary",
                        ManagedFile.file_category == "temporary_upload",
                        ManagedFile.file_category == "temporary_generated"
                    )
                )
            else:
                query = query.filter(ManagedFile.file_category == file_category)
        
        if category:
            query = query.filter(ManagedFile.category == category)
        
        # æœç´¢
        if search:
            search_filter = or_(
                ManagedFile.display_name.ilike(f"%{search}%"),
                ManagedFile.original_filename.ilike(f"%{search}%"),
                ManagedFile.description.ilike(f"%{search}%"),
                ManagedFile.keywords.ilike(f"%{search}%")
            )
            query = query.filter(search_filter)
        
        # æ ‡ç­¾è¿‡æ»¤
        if tags:
            tag_list = [tag.strip() for tag in tags.split(",")]
            for tag in tag_list:
                query = query.filter(ManagedFile.tags.contains([tag]))
        
        # æ’åº
        sort_column = getattr(ManagedFile, sort_by, ManagedFile.created_at)
        if sort_order.lower() == "desc":
            query = query.order_by(desc(sort_column))
        else:
            query = query.order_by(asc(sort_column))
        
        # åˆ†é¡µ
        total = query.count()
        offset = (page - 1) * page_size
        files = query.offset(offset).limit(page_size).all()
        
        # æ ¼å¼åŒ–ç»“æœ
        result_files = []
        for file in files:
            china_tz = pytz.timezone('Asia/Shanghai')
            
            file_info = {
                "id": file.id,
                "display_name": file.display_name,
                "original_filename": file.original_filename,
                "file_type": file.file_type,
                "file_size": file.file_size,
                "file_category": file.file_category,
                "category": file.category,
                "tags": file.tags or [],
                "description": file.description,
                "keywords": file.keywords,
                "is_public": file.is_public,
                "access_count": file.access_count,
                "created_at": file.created_at.replace(tzinfo=pytz.UTC).astimezone(china_tz).strftime('%Y-%m-%d %H:%M:%S'),
                "last_accessed": file.last_accessed.replace(tzinfo=pytz.UTC).astimezone(china_tz).strftime('%Y-%m-%d %H:%M:%S') if file.last_accessed else None,
                "is_archived": file.is_archived
            }
            
            # ä¸´æ—¶æ–‡ä»¶æ˜¾ç¤ºè¿‡æœŸæ—¶é—´
            if file.file_category in ["temporary_upload", "temporary_generated", "temporary"] and file.expires_at:
                file_info["expires_at"] = file.expires_at.replace(tzinfo=pytz.UTC).astimezone(china_tz).strftime('%Y-%m-%d %H:%M:%S')
                file_info["is_expired"] = file.expires_at < datetime.now()
            
            result_files.append(file_info)
        
        return {
            "success": True,
            "files": result_files,
            "pagination": {
                "total": total,
                "page": page,
                "page_size": page_size,
                "total_pages": (total + page_size - 1) // page_size
            }
        }
        
    except Exception as e:
        logger.error(f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}")

# å…·ä½“è·¯å¾„è·¯ç”±å¿…é¡»åœ¨å‚æ•°è·¯ç”±ä¹‹å‰å®šä¹‰

@router.get("/stats")
async def get_file_stats(db: Session = Depends(get_db)):
    """è·å–æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯"""
    try:
        # æ¸…ç†è¿‡æœŸæ–‡ä»¶
        cleanup_expired_files(db)
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_files = db.query(ManagedFile).filter(ManagedFile.is_archived == False).count()
        temp_files = db.query(ManagedFile).filter(
            and_(
                or_(
                    ManagedFile.file_category == "temporary",
                    ManagedFile.file_category == "temporary_upload",
                    ManagedFile.file_category == "temporary_generated"
                ),
                ManagedFile.is_archived == False
            )
        ).count()
        permanent_files = db.query(ManagedFile).filter(
            and_(ManagedFile.file_category == "permanent", ManagedFile.is_archived == False)
        ).count()
        
        # å­˜å‚¨å¤§å°ç»Ÿè®¡
        total_size = db.query(func.sum(ManagedFile.file_size)).filter(ManagedFile.is_archived == False).scalar() or 0
        temp_size = db.query(func.sum(ManagedFile.file_size)).filter(
            and_(
                or_(
                    ManagedFile.file_category == "temporary",
                    ManagedFile.file_category == "temporary_upload",
                    ManagedFile.file_category == "temporary_generated"
                ),
                ManagedFile.is_archived == False
            )
        ).scalar() or 0
        permanent_size = db.query(func.sum(ManagedFile.file_size)).filter(
            and_(ManagedFile.file_category == "permanent", ManagedFile.is_archived == False)
        ).scalar() or 0
        
        # æ–‡ä»¶ç±»å‹ç»Ÿè®¡
        type_stats = db.query(
            ManagedFile.file_type,
            func.count(ManagedFile.id).label('count'),
            func.sum(ManagedFile.file_size).label('size')
        ).filter(ManagedFile.is_archived == False).group_by(ManagedFile.file_type).all()
        
        return {
            "success": True,
            "stats": {
                "total_files": total_files,
                "temporary_files": temp_files,
                "permanent_files": permanent_files,
                "total_size": total_size,
                "temporary_size": temp_size,
                "permanent_size": permanent_size,
                "file_types": [
                    {
                        "type": stat.file_type,
                        "count": stat.count,
                        "size": stat.size or 0
                    }
                    for stat in type_stats
                ]
            }
        }
        
    except Exception as e:
        logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")

@router.get("/categories/list")
async def list_categories(db: Session = Depends(get_db)):
    """è·å–åˆ†ç±»åˆ—è¡¨"""
    try:
        categories = db.query(FileCategory).filter(FileCategory.is_active == True).order_by(FileCategory.sort_order).all()
        
        return {
            "success": True,
            "categories": [
                {
                    "id": cat.id,
                    "name": cat.name,
                    "display_name": cat.display_name,
                    "description": cat.description,
                    "parent_id": cat.parent_id,
                    "icon": cat.icon,
                    "color": cat.color,
                    "sort_order": cat.sort_order
                }
                for cat in categories
            ]
        }
        
    except Exception as e:
        logger.error(f"è·å–åˆ†ç±»åˆ—è¡¨å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–åˆ†ç±»åˆ—è¡¨å¤±è´¥: {str(e)}")

@router.get("/categories/suggestions")
async def get_category_suggestions():
    """è·å–æ–‡æ¡£åˆ†ç±»å»ºè®®"""
    try:
        from ai_service import ai_service
        
        return {
            "success": True,
            "categories": [
                {
                    "code": "performance_contract",
                    "name": "ä¸šç»©åˆåŒ",
                    "description": "æ³•å¾‹æœåŠ¡åˆåŒã€å§”æ‰˜åè®®ç­‰"
                },
                {
                    "code": "award_certificate", 
                    "name": "è£èª‰å¥–é¡¹",
                    "description": "å„ç§æ³•å¾‹è¡Œä¸šå¥–é¡¹è¯ä¹¦ã€æ’åè®¤è¯ç­‰"
                },
                {
                    "code": "qualification_certificate",
                    "name": "èµ„è´¨è¯ç…§", 
                    "description": "å¾‹å¸ˆäº‹åŠ¡æ‰€æ‰§ä¸šè®¸å¯è¯ã€è¥ä¸šæ‰§ç…§ç­‰æœºæ„èµ„è´¨è¯æ˜"
                },
                {
                    "code": "lawyer_certificate",
                    "name": "å¾‹å¸ˆè¯",
                    "description": "ä¸ªäººå¾‹å¸ˆæ‰§ä¸šè¯ä¹¦ï¼ŒåŒ…å«å¾‹å¸ˆå§“åã€æ‰§ä¸šè¯å·ç­‰ä¿¡æ¯"
                },
                {
                    "code": "financial_data",
                    "name": "è´¢åŠ¡æ•°æ®",
                    "description": "å®¡è®¡æŠ¥å‘Šã€è´¢åŠ¡æŠ¥è¡¨ã€è´¢åŠ¡åˆ†æç­‰è´¢åŠ¡ç›¸å…³æ–‡æ¡£"
                },
                {
                    "code": "other",
                    "name": "å…¶ä»–æ‚é¡¹",
                    "description": "ä¸å±äºä»¥ä¸Šç±»åˆ«çš„å…¶ä»–æ–‡æ¡£"
                }
            ],
            "business_fields": ai_service.business_fields,
            "lawyer_certificate_tags": {
                "position": ["åˆä¼™äºº", "å¾‹å¸ˆ"],
                "business_fields": ai_service.business_fields
            }
        }
        
    except Exception as e:
        logger.error(f"è·å–åˆ†ç±»å»ºè®®å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–å»ºè®®å¤±è´¥: {str(e)}")

# å‚æ•°è·¯ç”±å¿…é¡»åœ¨å…·ä½“è·¯å¾„è·¯ç”±ä¹‹å

@router.get("/{file_id}")
async def get_file_info(file_id: int, db: Session = Depends(get_db)):
    """è·å–æ–‡ä»¶è¯¦ç»†ä¿¡æ¯"""
    try:
        file = db.query(ManagedFile).filter(ManagedFile.id == file_id).first()
        if not file:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        # æ›´æ–°è®¿é—®ç»Ÿè®¡
        file.access_count += 1
        file.last_accessed = datetime.now()
        db.commit()
        
        china_tz = pytz.timezone('Asia/Shanghai')
        
        return {
            "success": True,
            "file": {
                "id": file.id,
                "display_name": file.display_name,
                "original_filename": file.original_filename,
                "storage_path": file.storage_path,
                "file_type": file.file_type,
                "mime_type": file.mime_type,
                "file_size": file.file_size,
                "file_hash": file.file_hash,
                "file_category": file.file_category,
                "category": file.category,
                "tags": file.tags or [],
                "description": file.description,
                "keywords": file.keywords,
                "is_public": file.is_public,
                "access_count": file.access_count,
                "created_at": file.created_at.replace(tzinfo=pytz.UTC).astimezone(china_tz).strftime('%Y-%m-%d %H:%M:%S'),
                "last_accessed": file.last_accessed.replace(tzinfo=pytz.UTC).astimezone(china_tz).strftime('%Y-%m-%d %H:%M:%S'),
                "expires_at": file.expires_at.replace(tzinfo=pytz.UTC).astimezone(china_tz).strftime('%Y-%m-%d %H:%M:%S') if file.expires_at else None,
                "is_archived": file.is_archived
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {str(e)}")

@router.get("/{file_id}/download")
async def download_file(file_id: int, db: Session = Depends(get_db)):
    """ä¸‹è½½æ–‡ä»¶"""
    try:
        file = db.query(ManagedFile).filter(ManagedFile.id == file_id).first()
        if not file:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        if not os.path.exists(file.storage_path):
            raise HTTPException(status_code=404, detail="ç‰©ç†æ–‡ä»¶ä¸å­˜åœ¨")
        
        # æ›´æ–°è®¿é—®ç»Ÿè®¡
        file.access_count += 1
        file.last_accessed = datetime.now()
        db.commit()
        
        return FileResponse(
            path=file.storage_path,
            filename=file.original_filename,
            media_type=file.mime_type
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ä¸‹è½½æ–‡ä»¶å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ä¸‹è½½æ–‡ä»¶å¤±è´¥: {str(e)}")

@router.put("/{file_id}")
async def update_file_info(
    file_id: int,
    display_name: Optional[str] = Form(None),
    category: Optional[str] = Form(None),
    description: Optional[str] = Form(None),
    tags: Optional[str] = Form(None),
    keywords: Optional[str] = Form(None),
    is_public: Optional[bool] = Form(None),
    enable_ai_reanalysis: bool = Form(False),  # æ˜¯å¦é‡æ–°è¿›è¡ŒAIåˆ†æ
    db: Session = Depends(get_db)
):
    """æ›´æ–°æ–‡ä»¶ä¿¡æ¯"""
    try:
        file = db.query(ManagedFile).filter(ManagedFile.id == file_id).first()
        if not file:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        # è®°å½•æ›´æ–°å‰çš„ä¿¡æ¯
        old_info = {
            "display_name": file.display_name,
            "category": file.category,
            "description": file.description,
            "tags": file.tags,
            "keywords": file.keywords
        }
        
        # AIå­¦ä¹ æœºåˆ¶ï¼šå½“ç”¨æˆ·ä¿®æ”¹åˆ†ç±»æ—¶ï¼Œè®°å½•å­¦ä¹ æ•°æ®
        if category and category != old_info["category"]:
            try:
                from ai_service import ai_service
                
                # è®°å½•ç”¨æˆ·çš„ä¿®æ­£è¡Œä¸ºä¾›AIå­¦ä¹ 
                learning_data = {
                    "file_path": file.storage_path,
                    "original_classification": old_info["category"],
                    "user_correction": category,
                    "file_type": file.file_type,
                    "file_size": file.file_size,
                    "correction_reason": "user_manual_edit",
                    "timestamp": datetime.now().isoformat()
                }
                
                # å¦‚æœæ˜¯ä»"å¾‹å¸ˆè¯"ä¿®æ­£ä¸º"èµ„è´¨è¯ç…§"ï¼Œè®°å½•è¯¦ç»†åŸå› 
                if old_info["category"] == "lawyer_certificate" and category == "qualification_certificate":
                    learning_data["specific_correction"] = "law_firm_license_vs_personal_certificate"
                    learning_data["learning_note"] = "å¾‹å¸ˆäº‹åŠ¡æ‰€æ‰§ä¸šè®¸å¯è¯åº”å½’ç±»ä¸ºèµ„è´¨è¯ç…§ï¼Œè€Œéä¸ªäººå¾‹å¸ˆè¯"
                
                # ä¿å­˜å­¦ä¹ æ•°æ®åˆ°é…ç½®ç®¡ç†å™¨
                ai_service.record_user_correction(learning_data)
                
                logger.info(f"AIå­¦ä¹ è®°å½•: ç”¨æˆ·å°† {old_info['category']} ä¿®æ­£ä¸º {category}")
                
            except Exception as learn_err:
                logger.warning(f"AIå­¦ä¹ è®°å½•å¤±è´¥ï¼Œä½†ä¸å½±å“æ›´æ–°: {learn_err}")
        
        # æ›´æ–°å­—æ®µ
        if display_name is not None:
            file.display_name = display_name
        if category is not None:
            file.category = category
        if description is not None:
            file.description = description
        if keywords is not None:
            file.keywords = keywords
        if is_public is not None:
            file.is_public = is_public
        
        # æ›´æ–°æ ‡ç­¾
        if tags is not None:
            try:
                file.tags = json.loads(tags)
            except:
                file.tags = [tag.strip() for tag in tags.split(",") if tag.strip()]
        
        file.updated_at = datetime.now()
        
        # å¦‚æœå¯ç”¨AIé‡æ–°åˆ†æ
        ai_result = None
        if enable_ai_reanalysis and file.file_category == "permanent":
            try:
                from ai_service import ai_service
                analysis_result = await ai_service.smart_document_analysis(
                    file.storage_path, 
                    enable_vision=True
                )
                if analysis_result.get("success"):
                    ai_result = analysis_result["results"]["final_classification"]
                    
                    # å¦‚æœç”¨æˆ·æ²¡æœ‰æ‰‹åŠ¨è®¾ç½®åˆ†ç±»ï¼Œä½¿ç”¨AIåˆ†æç»“æœ
                    if category is None and ai_result.get("category"):
                        file.category = ai_result["category"]
                    
                    # æ›´æ–°AIåˆ†æç»“æœåˆ°å¤„ç†ç»“æœä¸­
                    if not file.processing_result:
                        file.processing_result = {}
                    file.processing_result["ai_analysis"] = ai_result
                    file.processing_result["last_analysis"] = datetime.now().isoformat()
                    
                    logger.info(f"æ–‡ä»¶ {file_id} AIé‡æ–°åˆ†æå®Œæˆ: {ai_result.get('category')}")
            except Exception as ai_err:
                logger.warning(f"AIé‡æ–°åˆ†æå¤±è´¥: {ai_err}")
        
        db.commit()
        
        return {
            "success": True, 
            "message": "æ–‡ä»¶ä¿¡æ¯æ›´æ–°æˆåŠŸ",
            "ai_analysis": ai_result if enable_ai_reanalysis else None,
            "updated_fields": {
                "display_name": file.display_name,
                "category": file.category,
                "description": file.description,
                "tags": file.tags,
                "keywords": file.keywords
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ›´æ–°æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {str(e)}")
        db.rollback()
        raise HTTPException(status_code=500, detail=f"æ›´æ–°æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {str(e)}")

@router.delete("/{file_id}")
async def delete_file(file_id: int, force: bool = False, db: Session = Depends(get_db)):
    """åˆ é™¤æ–‡ä»¶"""
    try:
        file = db.query(ManagedFile).filter(ManagedFile.id == file_id).first()
        if not file:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        if force:
            # ç‰©ç†åˆ é™¤
            try:
                if os.path.exists(file.storage_path):
                    os.remove(file.storage_path)
                if file.processed_path and os.path.exists(file.processed_path):
                    os.remove(file.processed_path)
            except Exception as e:
                logger.warning(f"åˆ é™¤ç‰©ç†æ–‡ä»¶å¤±è´¥: {e}")
            
            # åˆ é™¤æ•°æ®åº“è®°å½•
            db.delete(file)
        else:
            # è½¯åˆ é™¤ï¼ˆå½’æ¡£ï¼‰
            file.is_archived = True
            file.archived_at = datetime.now()
        
        db.commit()
        
        return {
            "success": True,
            "message": "æ–‡ä»¶åˆ é™¤æˆåŠŸ" if force else "æ–‡ä»¶å·²å½’æ¡£"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {str(e)}")

@router.post("/categories")
async def create_category(
    name: str = Form(...),
    display_name: str = Form(...),
    description: Optional[str] = Form(None),
    parent_id: Optional[int] = Form(None),
    icon: Optional[str] = Form(None),
    color: Optional[str] = Form(None),
    db: Session = Depends(get_db)
):
    """åˆ›å»ºæ–‡ä»¶åˆ†ç±»"""
    try:
        # æ£€æŸ¥åˆ†ç±»åæ˜¯å¦å·²å­˜åœ¨
        existing = db.query(FileCategory).filter(FileCategory.name == name).first()
        if existing:
            raise HTTPException(status_code=409, detail="åˆ†ç±»åç§°å·²å­˜åœ¨")
        
        category = FileCategory(
            name=name,
            display_name=display_name,
            description=description,
            parent_id=parent_id,
            icon=icon,
            color=color
        )
        
        db.add(category)
        db.commit()
        db.refresh(category)
        
        return {
            "success": True,
            "message": "åˆ†ç±»åˆ›å»ºæˆåŠŸ",
            "category_id": category.id
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ›å»ºåˆ†ç±»å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºåˆ†ç±»å¤±è´¥: {str(e)}")

# å¯¼å‡ºè·¯ç”±å™¨ä¾›main.pyä½¿ç”¨
@router.post("/analyze-document")
async def analyze_document_ai(
    file_id: int,
    enable_vision: bool = True,
    force_reanalyze: bool = False,
    db: Session = Depends(get_db)
):
    """å¯¹å·²ä¸Šä¼ çš„æ–‡æ¡£è¿›è¡ŒAIåˆ†æ"""
    try:
        # è·å–æ–‡ä»¶ä¿¡æ¯
        file_record = db.query(ManagedFile).filter(ManagedFile.id == file_id).first()
        if not file_record:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        if not os.path.exists(file_record.storage_path):
            raise HTTPException(status_code=404, detail="ç‰©ç†æ–‡ä»¶ä¸å­˜åœ¨")
        
        # è¿›è¡ŒAIåˆ†æ
        from ai_service import ai_service
        
        if enable_vision:
            analysis_result = await ai_service.smart_document_analysis(
                file_record.storage_path,
                enable_vision=True
            )
        else:
            analysis_result = await ai_service.smart_document_analysis(file_record.storage_path)
        
        if not analysis_result.get("success"):
            raise HTTPException(status_code=500, detail=f"AIåˆ†æå¤±è´¥: {analysis_result.get('error')}")
        
        # æå–åˆ†ç±»ä¿¡æ¯
        if enable_vision:
            classification = analysis_result["results"]["final_classification"]
        else:
            classification = analysis_result.get("classification")
        
        # æå–æ ‡ç­¾å»ºè®®
        tag_result = await ai_service.extract_document_tags(
            file_record.storage_path,
            existing_tags=file_record.tags or []
        )
        
        suggested_tags = tag_result.get("suggested_tags", []) if tag_result.get("success") else []
        
        # å¤„ç†å¾‹å¸ˆè¯çš„ç‰¹æ®Šé€»è¾‘
        lawyer_cert_created = None
        if classification and classification.get("category") == "lawyer_certificate":
            try:
                lawyer_cert_created = await handle_lawyer_certificate_creation(
                    file_record, classification, analysis_result, db
                )
            except Exception as lawyer_err:
                logger.warning(f"åˆ›å»ºå¾‹å¸ˆè¯è®°å½•å¤±è´¥: {lawyer_err}")
        
        # å¦‚æœéœ€è¦ï¼Œæ›´æ–°æ–‡ä»¶è®°å½•
        if force_reanalyze and classification:
            # ä¿®å¤åˆ†ç±»å­—æ®µè®¿é—® - ä½¿ç”¨typeè€Œä¸æ˜¯category
            new_category = classification.get("type", file_record.category)
            if new_category:
                file_record.category = new_category
                logger.info(f"æ›´æ–°æ–‡ä»¶åˆ†ç±»: {file_record.id} -> {new_category}")
            
            if tag_result.get("success"):
                file_record.tags = tag_result.get("all_tags", file_record.tags)
            
            # æ›´æ–°æè¿°ä¿¡æ¯
            if classification.get("summary") and not file_record.description:
                file_record.description = classification["summary"]
            
            # æ›´æ–°å¤„ç†ç»“æœ
            if not file_record.processing_result:
                file_record.processing_result = {}
            file_record.processing_result["ai_analysis"] = analysis_result
            file_record.processing_result["classification"] = classification
            
            db.commit()
            logger.info(f"æ–‡ä»¶è®°å½•å·²æ›´æ–°: åˆ†ç±»={new_category}, æ ‡ç­¾={file_record.tags}")
        
        return {
            "success": True,
            "file_id": file_id,
            "analysis_result": analysis_result,
            "classification": classification,
            "suggested_tags": suggested_tags,
            "updated": force_reanalyze,
            "lawyer_certificate_created": lawyer_cert_created
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ–‡æ¡£AIåˆ†æå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ†æå¤±è´¥: {str(e)}")

@router.post("/batch-analyze")
async def batch_analyze_documents(
    file_ids: List[int],
    enable_vision: bool = True,
    update_records: bool = False,
    db: Session = Depends(get_db)
):
    """æ‰¹é‡åˆ†ææ–‡æ¡£"""
    try:
        results = []
        
        for file_id in file_ids:
            try:
                # åˆ†æå•ä¸ªæ–‡æ¡£
                analysis_result = await analyze_document_ai(
                    file_id=file_id,
                    enable_vision=enable_vision,
                    force_reanalyze=update_records,
                    db=db
                )
                results.append({
                    "file_id": file_id,
                    "success": True,
                    "result": analysis_result
                })
            except Exception as e:
                results.append({
                    "file_id": file_id,
                    "success": False,
                    "error": str(e)
                })
        
        success_count = sum(1 for r in results if r["success"])
        
        return {
            "success": True,
            "total": len(file_ids),
            "success_count": success_count,
            "failed_count": len(file_ids) - success_count,
            "results": results
        }
        
    except Exception as e:
        logger.error(f"æ‰¹é‡åˆ†æå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡åˆ†æå¤±è´¥: {str(e)}")

async def handle_lawyer_certificate_creation(file_record, classification, analysis_result, db):
    """å¤„ç†å¾‹å¸ˆè¯çš„åˆ›å»ºé€»è¾‘"""
    try:
        from models import LawyerCertificate, LawyerCertificateFile
        
        # æå–å¾‹å¸ˆè¯ä¿¡æ¯
        lawyer_info = classification.get("lawyer_info", {})
        key_entities = classification.get("key_entities", {})
        
        # å¿…è¦å­—æ®µæ£€æŸ¥
        lawyer_name = lawyer_info.get("name") or key_entities.get("holder_name")
        certificate_number = lawyer_info.get("certificate_number") or key_entities.get("certificate_number")
        law_firm = lawyer_info.get("law_firm") or classification.get("description", "")
        
        if not lawyer_name or not certificate_number:
            logger.warning("å¾‹å¸ˆè¯ä¿¡æ¯ä¸å®Œæ•´ï¼Œè·³è¿‡åˆ›å»º")
            return None
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        existing = db.query(LawyerCertificate).filter(
            LawyerCertificate.certificate_number == certificate_number
        ).first()
        
        if existing:
            logger.info(f"å¾‹å¸ˆè¯å·²å­˜åœ¨: {certificate_number}")
            return {"id": existing.id, "action": "exists"}
        
        # åˆ›å»ºå¾‹å¸ˆè¯è®°å½•
        lawyer_cert = LawyerCertificate(
            lawyer_name=lawyer_name,
            certificate_number=certificate_number,
            law_firm=law_firm,
            issuing_authority=lawyer_info.get("issuing_authority"),
            age=lawyer_info.get("age"),
            id_number=lawyer_info.get("id_number"),
            source_document=file_record.storage_path,
            ai_analysis=analysis_result,
            confidence_score=classification.get("confidence", 0.0),
            extracted_text=analysis_result.get("results", {}).get("text_extraction_result", {}).get("extracted_content", {}).get("text", ""),
            is_verified=False,
            is_manual_input=False
        )
        
        # å¤„ç†èŒä½æ ‡ç­¾
        position = "å¾‹å¸ˆ"  # é»˜è®¤
        if "åˆä¼™äºº" in str(classification.get("description", "")):
            position = "åˆä¼™äºº"
        lawyer_cert.position = position
        lawyer_cert.position_tags = [position]
        
        # å¤„ç†ä¸šåŠ¡é¢†åŸŸæ ‡ç­¾
        business_field = classification.get("business_field")
        if business_field:
            lawyer_cert.business_field_tags = [business_field]
        
        db.add(lawyer_cert)
        db.flush()  # è·å–ID
        
        # åˆ›å»ºæ–‡ä»¶å…³è”
        cert_file = LawyerCertificateFile(
            certificate_id=lawyer_cert.id,
            file_path=file_record.storage_path,
            file_type="original",
            file_name=file_record.original_filename,
            file_size=file_record.file_size
        )
        
        db.add(cert_file)
        db.commit()
        
        logger.info(f"å¾‹å¸ˆè¯åˆ›å»ºæˆåŠŸ: {lawyer_name} ({certificate_number})")
        
        return {
            "id": lawyer_cert.id,
            "action": "created",
            "lawyer_name": lawyer_name,
            "certificate_number": certificate_number,
            "law_firm": law_firm,
            "position": position
        }
        
    except Exception as e:
        db.rollback()
        logger.error(f"åˆ›å»ºå¾‹å¸ˆè¯è®°å½•å¤±è´¥: {str(e)}")
        raise e

@router.post("/{file_id}/create-lawyer-certificate")
async def create_lawyer_certificate_from_file(
    file_id: int,
    lawyer_name: str = Form(...),
    certificate_number: str = Form(...),
    law_firm: str = Form(...),
    issuing_authority: Optional[str] = Form(None),
    age: Optional[int] = Form(None),
    id_number: Optional[str] = Form(None),
    issue_date: Optional[str] = Form(None),
    position: Optional[str] = Form("å¾‹å¸ˆ"),
    position_tags: Optional[str] = Form(None),
    business_field_tags: Optional[str] = Form(None),
    custom_tags: Optional[str] = Form(None),
    verification_notes: Optional[str] = Form(None),
    db: Session = Depends(get_db)
):
    """ä»æ–‡ä»¶ç®¡ç†åˆ›å»ºå¾‹å¸ˆè¯è®°å½•ï¼ˆåŒå‘ç®¡ç†ï¼‰"""
    try:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        file_record = db.query(ManagedFile).filter(ManagedFile.id == file_id).first()
        if not file_record:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        # æ£€æŸ¥è¯ä¹¦å·æ˜¯å¦å·²å­˜åœ¨
        from models import LawyerCertificate, LawyerCertificateFile
        existing = db.query(LawyerCertificate).filter(
            LawyerCertificate.certificate_number == certificate_number
        ).first()
        if existing:
            raise HTTPException(status_code=409, detail="æ‰§ä¸šè¯å·å·²å­˜åœ¨")
        
        # è§£ææ ‡ç­¾
        position_tags_list = []
        business_field_tags_list = []
        custom_tags_list = []
        
        if position_tags:
            try:
                position_tags_list = json.loads(position_tags)
            except:
                position_tags_list = [tag.strip() for tag in position_tags.split(",") if tag.strip()]
        
        if business_field_tags:
            try:
                business_field_tags_list = json.loads(business_field_tags)
            except:
                business_field_tags_list = [tag.strip() for tag in business_field_tags.split(",") if tag.strip()]
        
        if custom_tags:
            try:
                custom_tags_list = json.loads(custom_tags)
            except:
                custom_tags_list = [tag.strip() for tag in custom_tags.split(",") if tag.strip()]
        
        # è§£ææ—¥æœŸ
        issue_date_obj = None
        if issue_date:
            try:
                issue_date_obj = datetime.fromisoformat(issue_date.replace('Z', '+00:00'))
            except:
                pass
        
        # åˆ›å»ºå¾‹å¸ˆè¯è®°å½•
        lawyer_cert = LawyerCertificate(
            lawyer_name=lawyer_name,
            certificate_number=certificate_number,
            law_firm=law_firm,
            issuing_authority=issuing_authority,
            age=age,
            id_number=id_number,
            issue_date=issue_date_obj,
            position=position,
            position_tags=position_tags_list,
            business_field_tags=business_field_tags_list,
            custom_tags=custom_tags_list,
            verification_notes=verification_notes,
            source_document=file_record.storage_path,
            is_verified=True,  # æ‰‹åŠ¨åˆ›å»ºé»˜è®¤å·²éªŒè¯
            is_manual_input=True  # ä»æ–‡ä»¶ç®¡ç†æ‰‹åŠ¨åˆ›å»º
        )
        
        db.add(lawyer_cert)
        db.flush()  # è·å–ID
        
        # åˆ›å»ºæ–‡ä»¶å…³è”
        cert_file = LawyerCertificateFile(
            certificate_id=lawyer_cert.id,
            file_path=file_record.storage_path,
            file_type="from_file_management",
            file_name=file_record.original_filename,
            file_size=file_record.file_size
        )
        
        db.add(cert_file)
        
        # æ›´æ–°æ–‡ä»¶è®°å½•çš„åˆ†ç±»
        file_record.category = "lawyer_certificate"
        if not file_record.description:
            file_record.description = f"å¾‹å¸ˆè¯æ–‡æ¡£ï¼š{lawyer_name}ï¼ˆ{certificate_number}ï¼‰"
        
        db.commit()
        
        logger.info(f"ä»æ–‡ä»¶ç®¡ç†åˆ›å»ºå¾‹å¸ˆè¯æˆåŠŸ: {lawyer_name} ({certificate_number})")
        
        return {
            "success": True,
            "message": "å¾‹å¸ˆè¯åˆ›å»ºæˆåŠŸ",
            "certificate_id": lawyer_cert.id,
            "file_updated": True
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ä»æ–‡ä»¶ç®¡ç†åˆ›å»ºå¾‹å¸ˆè¯å¤±è´¥: {str(e)}")
        db.rollback()
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºå¤±è´¥: {str(e)}")

@router.get("/{file_id}/lawyer-certificate")
async def get_related_lawyer_certificate(file_id: int, db: Session = Depends(get_db)):
    """æŸ¥çœ‹æ–‡ä»¶å…³è”çš„å¾‹å¸ˆè¯ä¿¡æ¯"""
    try:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        file_record = db.query(ManagedFile).filter(ManagedFile.id == file_id).first()
        if not file_record:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        # æŸ¥æ‰¾å…³è”çš„å¾‹å¸ˆè¯
        from models import LawyerCertificate, LawyerCertificateFile
        
        # é€šè¿‡æ–‡ä»¶è·¯å¾„æŸ¥æ‰¾å¾‹å¸ˆè¯
        cert_file = db.query(LawyerCertificateFile).filter(
            LawyerCertificateFile.file_path == file_record.storage_path
        ).first()
        
        if not cert_file:
            # é€šè¿‡æºæ–‡æ¡£æŸ¥æ‰¾
            cert = db.query(LawyerCertificate).filter(
                LawyerCertificate.source_document == file_record.storage_path
            ).first()
            
            if not cert:
                return {
                    "success": True,
                    "has_lawyer_certificate": False,
                    "message": "è¯¥æ–‡ä»¶æœªå…³è”å¾‹å¸ˆè¯"
                }
        else:
            cert = cert_file.certificate
        
        return {
            "success": True,
            "has_lawyer_certificate": True,
            "lawyer_certificate": {
                "id": cert.id,
                "lawyer_name": cert.lawyer_name,
                "certificate_number": cert.certificate_number,
                "law_firm": cert.law_firm,
                "position": cert.position,
                "is_verified": cert.is_verified,
                "created_at": cert.created_at.isoformat()
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æŸ¥çœ‹æ–‡ä»¶å…³è”å¾‹å¸ˆè¯å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æŸ¥çœ‹å¤±è´¥: {str(e)}")

@router.get("/task/{task_id}/status")
async def get_ai_task_status(task_id: int, db: Session = Depends(get_db)):
    """æŸ¥è¯¢AIä»»åŠ¡çŠ¶æ€"""
    try:
        task = db.query(AITask).filter(AITask.id == task_id).first()
        if not task:
            raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
        
        # è·å–å…³è”çš„æ–‡ä»¶ä¿¡æ¯
        file_record = None
        if task.file_type == "permanent_file":
            file_record = db.query(ManagedFile).filter(ManagedFile.id == task.file_id).first()
        
        return {
            "success": True,
            "task_id": task.id,
            "status": task.status,
            "file_id": task.file_id,
            "file_type": task.file_type,
            "result": task.result_snapshot,
            "error_message": task.error_message,
            "created_at": task.created_at.isoformat(),
            "updated_at": task.updated_at.isoformat(),
            "file_info": {
                "display_name": file_record.display_name if file_record else None,
                "category": file_record.category if file_record else None,
                "processing_status": file_record.processing_status if file_record else None
            } if file_record else None
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æŸ¥è¯¢AIä»»åŠ¡çŠ¶æ€å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢å¤±è´¥: {str(e)}")

@router.post("/{file_id}/reanalyze")
async def reanalyze_permanent_file(
    file_id: int,
    enable_vision_analysis: bool = Form(True),
    force_reclassify: bool = Form(True),
    db: Session = Depends(get_db)
):
    """é‡æ–°åˆ†æå¸¸é©»æ–‡ä»¶ï¼ˆæ™ºèƒ½åˆ†ç±» + ä¸“ä¸šè®°å½•åˆ›å»ºï¼‰"""
    try:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        file_record = db.query(ManagedFile).filter(ManagedFile.id == file_id).first()
        if not file_record:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        if file_record.file_category != "permanent":
            raise HTTPException(status_code=400, detail="åªèƒ½é‡æ–°åˆ†æå¸¸é©»æ–‡ä»¶")
        
        # åˆ›å»ºæ–°çš„AIä»»åŠ¡
        task_id = create_ai_task(db, file_id, "permanent_file")
        logger.info(f"ğŸ“‹ é‡æ–°åˆ†æä»»åŠ¡å·²åˆ›å»º: ä»»åŠ¡ID={task_id}, æ–‡ä»¶ID={file_id}")
        
        # å¯åŠ¨å¼‚æ­¥åå°AIåˆ†æ
        import asyncio
        asyncio.create_task(
            analyze_permanent_file_in_background(
                file_id,
                enable_vision_analysis,
                task_id,
                user_category=None  # è®©AIé‡æ–°æ™ºèƒ½åˆ†ç±»
            )
        )
        logger.info(f"ğŸ¤– å·²å¯åŠ¨é‡æ–°åˆ†æä»»åŠ¡ï¼Œæ–‡ä»¶ID={file_id}")
        
        return {
            "success": True,
            "message": "é‡æ–°åˆ†æä»»åŠ¡å·²å¯åŠ¨",
            "task_id": task_id,
            "file_id": file_id,
            "analysis_options": {
                "vision_enabled": enable_vision_analysis,
                "force_reclassify": force_reclassify
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"é‡æ–°åˆ†ææ–‡ä»¶å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"é‡æ–°åˆ†æå¤±è´¥: {str(e)}")

async def analyze_permanent_file_in_background(file_id: int, enable_vision_analysis: bool, task_id: int = None, user_category: str = None):
    """åå°å¼‚æ­¥åˆ†æå¸¸é©»æ–‡ä»¶"""
    try:
        from database import get_db
        from ai_service import ai_service
        
        db = next(get_db())
        
        if not ai_service.enable_ai:
            logger.warning("AIæœåŠ¡æœªå¯ç”¨ï¼Œè·³è¿‡åå°åˆ†æ")
            if task_id:
                update_ai_task(db, task_id, "completed", result={
                    "ai_analysis": False,
                    "reason": "AIæœåŠ¡æœªå¯ç”¨"
                })
            db.close()
            return
        
        logger.info(f"ğŸ¤– å¼€å§‹åå°AIåˆ†æï¼Œæ–‡ä»¶ID={file_id}")
        
        try:
            # è·å–æ–‡ä»¶è®°å½•
            file_record = db.query(ManagedFile).filter(ManagedFile.id == file_id).first()
            
            if not file_record or not file_record.storage_path:
                logger.warning(f"âš ï¸ æ–‡ä»¶è®°å½•æˆ–æ–‡ä»¶è·¯å¾„ä¸å­˜åœ¨: {file_id}")
                if task_id:
                    update_ai_task(db, task_id, "failed", result={
                        "error": "æ–‡ä»¶è®°å½•æˆ–æ–‡ä»¶è·¯å¾„ä¸å­˜åœ¨"
                    })
                db.close()
                return
            
            if not os.path.exists(file_record.storage_path):
                logger.warning(f"âš ï¸ ç‰©ç†æ–‡ä»¶ä¸å­˜åœ¨: {file_record.storage_path}")
                if task_id:
                    update_ai_task(db, task_id, "failed", result={
                        "error": "ç‰©ç†æ–‡ä»¶ä¸å­˜åœ¨"
                    })
                db.close()
                return
            
            # æ›´æ–°AIä»»åŠ¡çŠ¶æ€ä¸ºæ­£åœ¨å¤„ç†
            if task_id:
                update_ai_task(db, task_id, "processing")
            
            # æ›´æ–°æ–‡ä»¶å¤„ç†çŠ¶æ€
            file_record.processing_status = "processing"
            db.commit()
            
            logger.info(f"ğŸ” å¼€å§‹åˆ†ææ–‡ä»¶: {file_record.display_name}")
            
            # è¿›è¡ŒAIæ™ºèƒ½åˆ†æ
            if enable_vision_analysis:
                analysis_result = await ai_service.smart_document_analysis(
                    file_record.storage_path, 
                    enable_vision=True,
                    enable_ocr=True
                )
            else:
                analysis_result = await ai_service.smart_document_analysis(
                    file_record.storage_path,
                    enable_ocr=True
                )
            
            if analysis_result.get("success"):
                # æå–åˆ†ç±»ä¿¡æ¯
                if enable_vision_analysis:
                    ai_classification = analysis_result["results"]["final_classification"]
                else:
                    ai_classification = analysis_result.get("classification")
                
                # æ›´æ–°æ–‡ä»¶åˆ†ç±»ï¼ˆä»…å½“ç”¨æˆ·æœªæŒ‡å®šåˆ†ç±»æ—¶ï¼‰
                if not user_category and ai_classification:
                    suggested_category = ai_classification.get("category", "other")
                    if suggested_category and suggested_category != "other":
                        file_record.category = suggested_category
                        logger.info(f"ğŸ“‹ AIå»ºè®®åˆ†ç±»: {suggested_category}")
                
                # æå–AIå»ºè®®çš„æ ‡ç­¾
                if ai_classification:
                    try:
                        tag_result = await ai_service.extract_document_tags(
                            file_record.storage_path, 
                            existing_tags=file_record.tags or []
                        )
                        if tag_result.get("success"):
                            ai_tags = tag_result.get("suggested_tags", [])
                            if ai_tags:
                                # åˆå¹¶ç°æœ‰æ ‡ç­¾å’ŒAIå»ºè®®æ ‡ç­¾
                                existing_tags = set(file_record.tags or [])
                                new_tags = existing_tags.union(set(ai_tags))
                                file_record.tags = list(new_tags)
                                logger.info(f"ğŸ·ï¸ AIå»ºè®®æ ‡ç­¾: {ai_tags}")
                    except Exception as tag_err:
                        logger.warning(f"æå–æ ‡ç­¾å¤±è´¥: {tag_err}")
                
                # ä¿å­˜AIåˆ†æç»“æœåˆ°å¤„ç†ç»“æœä¸­
                if not file_record.processing_result:
                    file_record.processing_result = {}
                file_record.processing_result["ai_analysis"] = ai_classification
                file_record.processing_result["analysis_timestamp"] = datetime.now().isoformat()
                file_record.processing_result["vision_enabled"] = enable_vision_analysis
                
                # æ™ºèƒ½åˆ†ç±»å’Œä¸“ä¸šè®°å½•åˆ›å»º
                professional_record_created = None
                if ai_classification:
                    try:
                        professional_record_created = await create_professional_record_from_file(
                            db, file_record, ai_classification, analysis_result
                        )
                        if professional_record_created:
                            logger.info(f"ğŸ¯ è‡ªåŠ¨åˆ›å»ºä¸“ä¸šè®°å½•æˆåŠŸ: {professional_record_created}")
                    except Exception as prof_err:
                        logger.warning(f"è‡ªåŠ¨åˆ›å»ºä¸“ä¸šè®°å½•å¤±è´¥: {prof_err}")
                        professional_record_created = {"error": str(prof_err)}
                
                # æ›´æ–°å¤„ç†çŠ¶æ€
                file_record.processing_status = "completed"
                file_record.is_processed = True
                
                # æ›´æ–°AIä»»åŠ¡çŠ¶æ€ä¸ºæˆåŠŸ
                if task_id:
                    update_ai_task(db, task_id, "success", result={
                        "ai_classification": ai_classification,
                        "category": file_record.category,
                        "tags": file_record.tags,
                        "analysis_enabled": True,
                        "vision_enabled": enable_vision_analysis,
                        "professional_record": professional_record_created
                    })
                
                logger.info(f"âœ… æ–‡ä»¶AIåˆ†æå®Œæˆ: {file_record.display_name}")
                
            else:
                # AIåˆ†æå¤±è´¥
                error_msg = analysis_result.get("error", "AIåˆ†æå¤±è´¥")
                logger.warning(f"âš ï¸ AIåˆ†æå¤±è´¥: {error_msg}")
                
                file_record.processing_status = "failed"
                if not file_record.processing_result:
                    file_record.processing_result = {}
                file_record.processing_result["error"] = error_msg
                file_record.processing_result["analysis_timestamp"] = datetime.now().isoformat()
                
                if task_id:
                    update_ai_task(db, task_id, "failed", result={
                        "error": error_msg,
                        "analysis_enabled": False
                    })
            
            db.commit()
            
        except Exception as process_err:
            logger.error(f"âŒ åå°AIåˆ†æå¼‚å¸¸: {process_err}")
            
            # æ›´æ–°å¤„ç†çŠ¶æ€
            try:
                file_record = db.query(ManagedFile).filter(ManagedFile.id == file_id).first()
                if file_record:
                    file_record.processing_status = "failed"
                    if not file_record.processing_result:
                        file_record.processing_result = {}
                    file_record.processing_result["error"] = str(process_err)
                    file_record.processing_result["analysis_timestamp"] = datetime.now().isoformat()
                
                if task_id:
                    update_ai_task(db, task_id, "failed", result={
                        "error": str(process_err)
                    })
                
                db.commit()
            except Exception as update_err:
                logger.error(f"æ›´æ–°å¤±è´¥çŠ¶æ€å‡ºé”™: {update_err}")
        
        finally:
            db.close()
            
    except Exception as e:
        logger.error(f"âŒ åå°AIåˆ†æå¤±è´¥: {e}")
        import traceback
        logger.error(f"å¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")

async def create_professional_record_from_file(db, file_record, ai_classification, analysis_result):
    """æ ¹æ®AIåˆ†ç±»ç»“æœåˆ›å»ºå¯¹åº”çš„ä¸“ä¸šç®¡ç†è®°å½•"""
    try:
        # æ™ºèƒ½åˆ†ç±»è¯†åˆ« - ä¼˜å…ˆä½¿ç”¨categoryå­—æ®µï¼Œç„¶åä½¿ç”¨typeå­—æ®µï¼Œæœ€åä½¿ç”¨document_typeå­—æ®µ
        ai_category = ai_classification.get("category", "other")
        ai_type = ai_classification.get("type", "")
        document_type = ai_classification.get("document_type", "")
        
        # åŸºäºAIè¯†åˆ«çš„ç±»å‹è¿›è¡Œæ™ºèƒ½åˆ†ç±»æ˜ å°„
        if ai_category == "other":
            # ä¼˜å…ˆä½¿ç”¨document_typeï¼Œç„¶åä½¿ç”¨type
            if document_type:
                ai_category = classify_document_by_ai_type(document_type, ai_classification)
            elif ai_type:
                ai_category = classify_document_by_ai_type(ai_type, ai_classification)
        
        professional_record = None
        
        logger.info(f"ğŸ¯ å¼€å§‹åˆ›å»ºä¸“ä¸šè®°å½•ï¼ŒAIç±»å‹: {ai_type}, æœ€ç»ˆåˆ†ç±»: {ai_category}")
        
        if ai_category == "lawyer_certificate":
            # å¾‹å¸ˆè¯åˆ†æå’Œåˆ›å»º
            professional_record = await create_lawyer_certificate_from_analysis(
                db, file_record, ai_classification, analysis_result
            )
        elif ai_category == "performance_contract":
            # ä¸šç»©åˆåŒåˆ†æå’Œåˆ›å»º
            professional_record = await create_performance_from_analysis(
                db, file_record, ai_classification, analysis_result
            )
        elif ai_category == "award_certificate":
            # å¥–é¡¹è¯ä¹¦åˆ†æå’Œåˆ›å»º
            professional_record = await create_award_from_analysis(
                db, file_record, ai_classification, analysis_result
            )
        elif ai_category == "financial_data":
            # è´¢åŠ¡æ•°æ®åˆ†æï¼ˆæš‚ä¸åˆ›å»ºå…·ä½“è®°å½•ï¼Œä½†è®°å½•åˆ†ç±»ï¼‰
            logger.info(f"ğŸ’° è¯†åˆ«ä¸ºè´¢åŠ¡æ•°æ®: {file_record.display_name}")
            professional_record = {
                "type": "financial_data",
                "category": "è´¢åŠ¡æ•°æ®",
                "description": "å·²è¯†åˆ«ä¸ºè´¢åŠ¡ç›¸å…³æ–‡æ¡£ï¼Œæš‚å­˜ä¸ºå¸¸é©»æ–‡ä»¶",
                "ai_type": ai_type
            }
            # æ›´æ–°æ–‡ä»¶åˆ†ç±»
            file_record.category = "financial_data"
        else:
            logger.info(f"ğŸ“‹ å…¶ä»–ç±»å‹æ–‡æ¡£ï¼ŒAIç±»å‹: {ai_type}, åˆ†ç±»: {ai_category}")
            professional_record = {
                "type": "other",
                "category": ai_category,
                "ai_type": ai_type,
                "description": f"AIè¯†åˆ«ç±»å‹: {ai_type}, åˆ†ç±»ä¸º: {ai_category}"
            }
        
        return professional_record
        
    except Exception as e:
        logger.error(f"åˆ›å»ºä¸“ä¸šè®°å½•å¤±è´¥: {str(e)}")
        return {"error": str(e)}

def classify_document_by_ai_type(ai_type, ai_classification):
    """æ ¹æ®AIè¯†åˆ«çš„ç±»å‹è¿›è¡Œæ™ºèƒ½åˆ†ç±»æ˜ å°„"""
    try:
        ai_type_lower = ai_type.lower()
        summary = ai_classification.get("summary", "").lower()
        entities = ai_classification.get("entities", {})
        
        # ä¼˜å…ˆæ£€æŸ¥AIè¿”å›çš„typeå­—æ®µ
        if ai_type and ai_type != "unknown":
            # åŸºäºAIè¿”å›çš„typeè¿›è¡Œæ™ºèƒ½æ˜ å°„
            if ai_type in ["æŠ¥å‘Š", "å®¡è®¡æŠ¥å‘Š", "è´¢åŠ¡æŠ¥å‘Š", "audit_report", "financial_report"]:
                return "financial_data"
            elif ai_type in ["åˆåŒ", "åè®®", "contract", "agreement"]:
                return "performance_contract"
            elif ai_type in ["è¯ä¹¦", "å¾‹å¸ˆè¯", "æ‰§ä¸šè¯", "certificate", "lawyer_certificate"]:
                return "lawyer_certificate"
            elif ai_type in ["å¥–é¡¹", "å¥–çŠ¶", "award", "prize", "honor"]:
                return "award_certificate"
        
        # åŸºäºå…³é”®è¯çš„å¤‡ç”¨åˆ†ç±»é€»è¾‘
        # è´¢åŠ¡æ•°æ®ç›¸å…³ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        if any(keyword in ai_type_lower for keyword in ["æŠ¥å‘Š", "å®¡è®¡", "è´¢åŠ¡", "ä¼šè®¡", "financial", "audit", "report"]):
            return "financial_data"
        
        # å¾‹å¸ˆè¯ç›¸å…³
        if any(keyword in ai_type_lower for keyword in ["å¾‹å¸ˆè¯", "æ‰§ä¸šè¯", "lawyer", "certificate"]):
            return "lawyer_certificate"
        
        # ä¸šç»©åˆåŒç›¸å…³  
        if any(keyword in ai_type_lower for keyword in ["åˆåŒ", "åè®®", "contract", "agreement"]):
            return "performance_contract"
        
        # å¥–é¡¹è¯ä¹¦ç›¸å…³
        if any(keyword in ai_type_lower for keyword in ["å¥–é¡¹", "è¯ä¹¦", "award", "prize", "honor"]):
            return "award_certificate"
        
        # åŸºäºæ‘˜è¦å†…å®¹è¿›ä¸€æ­¥åˆ¤æ–­
        if "å®¡è®¡" in summary or "è´¢åŠ¡" in summary or "ä¼šè®¡" in summary:
            return "financial_data"
            
        if "åˆåŒ" in summary or "åè®®" in summary:
            return "performance_contract"
            
        if "å¥–" in summary or "è¯ä¹¦" in summary:
            return "award_certificate"
            
        if "å¾‹å¸ˆ" in summary or "æ‰§ä¸š" in summary:
            return "lawyer_certificate"
        
        return "other"
        
    except Exception as e:
        logger.error(f"AIç±»å‹åˆ†ç±»æ˜ å°„å¤±è´¥: {str(e)}")
        return "other"

async def create_lawyer_certificate_from_analysis(db, file_record, ai_classification, analysis_result):
    """ä»AIåˆ†æç»“æœåˆ›å»ºå¾‹å¸ˆè¯è®°å½•"""
    try:
        from models import LawyerCertificate, LawyerCertificateFile
        
        # æå–å¾‹å¸ˆè¯ä¿¡æ¯
        lawyer_info = ai_classification.get("lawyer_info", {})
        key_entities = ai_classification.get("key_entities", {})
        
        extracted_info = {
            "lawyer_name": lawyer_info.get("name") or key_entities.get("holder_name"),
            "certificate_number": lawyer_info.get("certificate_number") or key_entities.get("certificate_number"),
            "law_firm": lawyer_info.get("law_firm") or ai_classification.get("description", ""),
            "issuing_authority": lawyer_info.get("issuing_authority") or key_entities.get("issuer"),
            "age": lawyer_info.get("age"),
            "id_number": lawyer_info.get("id_number"),
            "position": "åˆä¼™äºº" if "åˆä¼™äºº" in str(ai_classification.get("description", "")) else "å¾‹å¸ˆ"
        }
        
        # æ£€æŸ¥å¿…è¦å­—æ®µ
        if not (extracted_info.get("lawyer_name") and extracted_info.get("certificate_number")):
            logger.warning("å¾‹å¸ˆè¯ä¿¡æ¯ä¸å®Œæ•´ï¼Œè·³è¿‡åˆ›å»ºè®°å½•")
            return {"error": "å¾‹å¸ˆè¯ä¿¡æ¯ä¸å®Œæ•´"}
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        existing = db.query(LawyerCertificate).filter(
            LawyerCertificate.certificate_number == extracted_info["certificate_number"]
        ).first()
        
        if existing:
            # å…³è”åˆ°ç°æœ‰è®°å½•
            cert_file = LawyerCertificateFile(
                certificate_id=existing.id,
                file_path=file_record.storage_path,
                file_type="auto_detected_from_permanent_file",
                file_name=file_record.original_filename,
                file_size=file_record.file_size
            )
            db.add(cert_file)
            
            logger.info(f"ğŸ‘¥ å¾‹å¸ˆè¯å·²å­˜åœ¨ï¼Œæ·»åŠ æ–‡ä»¶å…³è”: {existing.lawyer_name}")
            return {
                "type": "lawyer_certificate",
                "action": "linked_to_existing",
                "certificate_id": existing.id,
                "lawyer_name": existing.lawyer_name
            }
        else:
            # åˆ›å»ºæ–°è®°å½•
            cert = LawyerCertificate(
                lawyer_name=extracted_info["lawyer_name"],
                certificate_number=extracted_info["certificate_number"],
                law_firm=extracted_info["law_firm"] or "",
                issuing_authority=extracted_info.get("issuing_authority"),
                age=extracted_info.get("age"),
                id_number=extracted_info.get("id_number"),
                position=extracted_info.get("position", "å¾‹å¸ˆ"),
                position_tags=[extracted_info.get("position", "å¾‹å¸ˆ")],
                source_document=file_record.original_filename,
                ai_analysis=analysis_result,
                confidence_score=ai_classification.get("confidence", 0.0),
                extracted_text=analysis_result.get('text_extraction_result', {}).get('text', ''),
                is_verified=False,
                is_manual_input=False
            )
            
            db.add(cert)
            db.flush()  # è·å–ID
            
            # åˆ›å»ºæ–‡ä»¶å…³è”
            cert_file = LawyerCertificateFile(
                certificate_id=cert.id,
                file_path=file_record.storage_path,
                file_type="auto_detected_from_permanent_file",
                file_name=file_record.original_filename,
                file_size=file_record.file_size
            )
            
            db.add(cert_file)
            
            logger.info(f"âœ… è‡ªåŠ¨åˆ›å»ºå¾‹å¸ˆè¯è®°å½•: {cert.lawyer_name} ({cert.certificate_number})")
            return {
                "type": "lawyer_certificate",
                "action": "created_new",
                "certificate_id": cert.id,
                "lawyer_name": cert.lawyer_name,
                "certificate_number": cert.certificate_number
            }
        
    except Exception as e:
        logger.error(f"åˆ›å»ºå¾‹å¸ˆè¯è®°å½•å¤±è´¥: {str(e)}")
        return {"error": str(e)}

async def create_performance_from_analysis(db, file_record, ai_classification, analysis_result):
    """ä»AIåˆ†æç»“æœåˆ›å»ºä¸šç»©è®°å½•"""
    try:
        from models import Performance, PerformanceFile
        
        # æå–ä¸šç»©ä¿¡æ¯
        performance_info = ai_classification.get("performance_info", {})
        key_entities = ai_classification.get("key_entities", {})
        
        extracted_info = {
            "project_name": performance_info.get("project_name") or key_entities.get("project_name") or file_record.display_name,
            "client_name": performance_info.get("client_name") or key_entities.get("client_name") or "å¾…å®Œå–„",
            "business_field": performance_info.get("business_field") or ai_classification.get("business_field", "å¾…AIåˆ†æ"),
            "description": ai_classification.get("description", f"ä»æ–‡ä»¶ '{file_record.original_filename}' è‡ªåŠ¨è¯†åˆ«")
        }
        
        # åˆ›å»ºä¸šç»©è®°å½•
        performance = Performance(
            project_name=extracted_info["project_name"],
            client_name=extracted_info["client_name"],
            project_type="é‡å¤§ä¸ªæ¡ˆ(éè¯‰)",
            business_field=extracted_info["business_field"],
            year=datetime.now().year,
            is_manual_input=False,
            is_verified=False,
            confidence_score=ai_classification.get("confidence", 0.0),
            description=extracted_info["description"],
            source_document=file_record.storage_path,
            ai_analysis_status="completed",
            extracted_text=analysis_result.get('text_extraction_result', {}).get('text', '')
        )
        
        db.add(performance)
        db.flush()  # è·å–ID
        
        # åˆ›å»ºæ–‡ä»¶å…³è”
        perf_file = PerformanceFile(
            performance_id=performance.id,
            file_path=file_record.storage_path,
            file_type="auto_detected_from_permanent_file",
            file_name=file_record.original_filename,
            file_size=file_record.file_size
        )
        
        db.add(perf_file)
        
        logger.info(f"âœ… è‡ªåŠ¨åˆ›å»ºä¸šç»©è®°å½•: {performance.project_name}")
        return {
            "type": "performance_contract",
            "action": "created_new",
            "performance_id": performance.id,
            "project_name": performance.project_name,
            "client_name": performance.client_name
        }
        
    except Exception as e:
        logger.error(f"åˆ›å»ºä¸šç»©è®°å½•å¤±è´¥: {str(e)}")
        return {"error": str(e)}

async def create_award_from_analysis(db, file_record, ai_classification, analysis_result):
    """ä»AIåˆ†æç»“æœåˆ›å»ºå¥–é¡¹è®°å½•"""
    try:
        from models import Award
        
        # æå–å¥–é¡¹ä¿¡æ¯
        award_info = ai_classification.get("award_info", {})
        key_entities = ai_classification.get("key_entities", {})
        
        extracted_info = {
            "title": award_info.get("title") or key_entities.get("title") or file_record.display_name,
            "brand": award_info.get("brand") or key_entities.get("brand") or "å¾…AIåˆ†æ",
            "year": award_info.get("year") or datetime.now().year,
            "business_type": award_info.get("business_type") or ai_classification.get("business_field", "å¾…AIåˆ†æ"),
            "description": ai_classification.get("description", f"ä»æ–‡ä»¶ '{file_record.original_filename}' è‡ªåŠ¨è¯†åˆ«")
        }
        
        # åˆ›å»ºå¥–é¡¹è®°å½•
        award = Award(
            title=extracted_info["title"],
            brand=extracted_info["brand"],
            year=extracted_info["year"],
            business_type=extracted_info["business_type"],
            description=extracted_info["description"],
            is_verified=False,
            source_document=file_record.storage_path,
            confidence_score=ai_classification.get("confidence", 0.0)
        )
        
        db.add(award)
        db.flush()  # è·å–ID
        
        logger.info(f"âœ… è‡ªåŠ¨åˆ›å»ºå¥–é¡¹è®°å½•: {award.title}")
        return {
            "type": "award_certificate",
            "action": "created_new",
            "award_id": award.id,
            "title": award.title,
            "brand": award.brand
        }
        
    except Exception as e:
        logger.error(f"åˆ›å»ºå¥–é¡¹è®°å½•å¤±è´¥: {str(e)}")
        return {"error": str(e)}

def setup_router(app):
    app.include_router(router) 