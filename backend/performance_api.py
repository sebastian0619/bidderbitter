"""
ä¸šç»©ç®¡ç†API
"""
from fastapi import APIRouter, Depends, HTTPException, File, UploadFile, Form
from sqlalchemy.orm import Session
from sqlalchemy import func, desc
from typing import Optional, List, Dict, Any
import logging
import os
from datetime import datetime

from database import get_db
from models import Performance, PerformanceFile, SystemSettings, AITask
from schemas import PerformanceCreate, PerformanceUpdate, PerformanceResponse
from config_manager import config_manager
from ai_service import create_ai_task, update_ai_task

router = APIRouter()
logger = logging.getLogger(__name__)

@router.get("/stats")
async def get_performance_stats(db: Session = Depends(get_db)):
    """è·å–ä¸šç»©ç»Ÿè®¡æ•°æ®"""
    try:
        total_performances = db.query(Performance).count()
        verified_performances = db.query(Performance).filter(Performance.is_verified == True).count()
        manual_input_performances = db.query(Performance).filter(Performance.is_manual_input == True).count()
        
        # æŒ‰å¹´ä»½ç»Ÿè®¡
        yearly_stats = db.query(
            Performance.year,
            func.count(Performance.id).label('count')
        ).group_by(Performance.year).order_by(desc(Performance.year)).limit(5).all()
        
        # æŒ‰ä¸šåŠ¡é¢†åŸŸç»Ÿè®¡
        field_stats = db.query(
            Performance.business_field,
            func.count(Performance.id).label('count')
        ).group_by(Performance.business_field).order_by(desc(func.count(Performance.id))).limit(5).all()
        
        return {
            "success": True,
            "stats": {
                "total_performances": total_performances,
                "verified_performances": verified_performances,
                "manual_input_performances": manual_input_performances,
                "yearly_distribution": [
                    {"year": stat.year, "count": stat.count} 
                    for stat in yearly_stats
                ],
                "field_distribution": [
                    {"field": stat.business_field, "count": stat.count} 
                    for stat in field_stats if stat.business_field
                ]
            }
        }
        
    except Exception as e:
        logger.error(f"è·å–ä¸šç»©ç»Ÿè®¡å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–ç»Ÿè®¡æ•°æ®å¤±è´¥: {str(e)}")

@router.get("/list")
async def list_performances(
    page: int = 1,
    page_size: int = 20,
    search: Optional[str] = None,
    business_field: Optional[str] = None,
    year: Optional[int] = None,
    is_verified: Optional[bool] = None,
    db: Session = Depends(get_db)
):
    """è·å–ä¸šç»©åˆ—è¡¨"""
    try:
        query = db.query(Performance)
        
        # æœç´¢è¿‡æ»¤
        if search:
            query = query.filter(
                Performance.project_name.ilike(f'%{search}%') |
                Performance.client_name.ilike(f'%{search}%') |
                Performance.description.ilike(f'%{search}%')
            )
        
        # ä¸šåŠ¡é¢†åŸŸè¿‡æ»¤
        if business_field:
            query = query.filter(Performance.business_field == business_field)
        
        # å¹´ä»½è¿‡æ»¤
        if year:
            query = query.filter(Performance.year == year)
        
        # éªŒè¯çŠ¶æ€è¿‡æ»¤
        if is_verified is not None:
            query = query.filter(Performance.is_verified == is_verified)
        
        # æ€»æ•°
        total = query.count()
        
        # åˆ†é¡µ
        offset = (page - 1) * page_size
        performances = query.order_by(desc(Performance.created_at)).offset(offset).limit(page_size).all()
        
        return {
            "success": True,
            "performances": [
                {
                    "id": perf.id,
                    "project_name": perf.project_name,
                    "client_name": perf.client_name,
                    "project_type": perf.project_type,
                    "business_field": perf.business_field,
                    "year": perf.year,
                    "contract_amount": perf.contract_amount,
                    "currency": perf.currency,
                    "start_date": perf.start_date.isoformat() if perf.start_date else None,
                    "end_date": perf.end_date.isoformat() if perf.end_date else None,
                    "description": perf.description,
                    "confidence_score": perf.confidence_score,
                    "ai_analysis_status": getattr(perf, 'ai_analysis_status', 'pending'),  # å…¼å®¹æ—§æ•°æ®
                    "is_verified": perf.is_verified,
                    "is_manual_input": perf.is_manual_input,
                    "source_document": perf.source_document,
                    "created_at": perf.created_at.isoformat()
                }
                for perf in performances
            ],
            "pagination": {
                "page": page,
                "page_size": page_size,
                "total": total,
                "pages": (total + page_size - 1) // page_size
            }
        }
        
    except Exception as e:
        logger.error(f"è·å–ä¸šç»©åˆ—è¡¨å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–ä¸šç»©åˆ—è¡¨å¤±è´¥: {str(e)}")

@router.post("/create")
async def create_performance(
    performance_data: dict,
    db: Session = Depends(get_db)
):
    """æ‰‹åŠ¨åˆ›å»ºä¸šç»©è®°å½•"""
    try:
        # åˆ›å»ºä¸šç»©è®°å½•
        performance = Performance(
            project_name=performance_data.get("project_name"),
            client_name=performance_data.get("client_name"),
            project_type=performance_data.get("project_type"),
            business_field=performance_data.get("business_field"),
            year=int(performance_data.get("year")),
            contract_amount=performance_data.get("contract_amount"),
            currency=performance_data.get("currency", "CNY"),
            start_date=datetime.fromisoformat(performance_data["start_date"]) if performance_data.get("start_date") else None,
            end_date=datetime.fromisoformat(performance_data["end_date"]) if performance_data.get("end_date") else None,
            description=performance_data.get("description"),
            is_manual_input=True,
            is_verified=True  # æ‰‹åŠ¨åˆ›å»ºçš„é»˜è®¤ä¸ºå·²éªŒè¯
        )
        
        db.add(performance)
        db.commit()
        db.refresh(performance)
        
        logger.info(f"æ‰‹åŠ¨åˆ›å»ºä¸šç»©è®°å½•æˆåŠŸ: {performance.project_name}")
        
        return {
            "success": True,
            "message": "ä¸šç»©è®°å½•åˆ›å»ºæˆåŠŸ",
            "performance_id": performance.id
        }
        
    except Exception as e:
        db.rollback()
        logger.error(f"åˆ›å»ºä¸šç»©è®°å½•å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºä¸šç»©è®°å½•å¤±è´¥: {str(e)}")

@router.post("/upload")
async def upload_performance_files(
    files: List[UploadFile] = File(...),
    project_name: Optional[str] = Form(None),
    client_name: Optional[str] = Form(None),
    business_field: Optional[str] = Form(None),
    enable_ai_analysis: bool = Form(True),
    enable_vision_analysis: bool = Form(True),
    db: Session = Depends(get_db)
):
    """ä¸Šä¼ ä¸šç»©æ–‡ä»¶ï¼ˆæ”¯æŒå¤šæ–‡ä»¶ï¼‰å¹¶ä¸ºæ¯ä¸ªæ–‡ä»¶åˆ›å»ºç‹¬ç«‹çš„ä¸šç»©è®°å½•"""
    uploaded_performances = []
    uploaded_files = []
    
    try:
        # ä¿å­˜æ–‡ä»¶ç›®å½•
        upload_dir = os.path.join(os.path.dirname(__file__), "uploads", "performances")
        os.makedirs(upload_dir, exist_ok=True)
        
        # ä¸ºæ¯ä¸ªæ–‡ä»¶åˆ›å»ºç‹¬ç«‹çš„ä¸šç»©è®°å½•
        for i, file in enumerate(files):
            # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            file_path = os.path.join(upload_dir, f"{timestamp}_{i}_{file.filename}")
            
            # ä¿å­˜æ–‡ä»¶
            with open(file_path, "wb") as buffer:
                content = await file.read()
                buffer.write(content)
            
            logger.info(f"âœ… ä¸šç»©æ–‡ä»¶ä¿å­˜æˆåŠŸ: {file_path}")
            
            # ä¸ºæ¯ä¸ªæ–‡ä»¶åˆ›å»ºç‹¬ç«‹çš„ä¸šç»©è®°å½•
            # ä½¿ç”¨å»é™¤æ‰©å±•åçš„æ–‡ä»¶åä½œä¸ºä¸´æ—¶é¡¹ç›®åç§°
            temp_project_name = os.path.splitext(file.filename)[0] if not project_name else f"{project_name}_{i+1}"
            
            performance = Performance(
                project_name=temp_project_name,
                client_name=client_name or "å¾…AIåˆ†æ",
                project_type="é‡å¤§ä¸ªæ¡ˆ(éè¯‰)",  # ä½¿ç”¨æ–°çš„é¡¹ç›®ç±»å‹åç§°
                business_field=business_field or "å¾…AIåˆ†æ",
                year=datetime.now().year,
                is_manual_input=False,
                is_verified=False,
                confidence_score=0.0,
                description=f"æ–‡ä»¶ '{file.filename}' å·²ä¸Šä¼ ï¼Œç­‰å¾…AIåˆ†æ...",
                source_document=file_path,
                ai_analysis_status="pending"  # æ·»åŠ åˆ†æçŠ¶æ€æ ‡è®°
            )
            
            db.add(performance)
            db.commit()  # ç«‹å³æäº¤æ¯ä¸ªè®°å½•
            db.refresh(performance)
            
            # åˆ›å»ºæ–‡ä»¶è®°å½•
            performance_file = PerformanceFile(
                performance_id=performance.id,
                file_path=file_path,
                file_type="contract" if "åˆåŒ" in file.filename or "contract" in file.filename.lower() else "supporting_doc",
                file_name=file.filename,
                file_size=len(content)
            )
            
            db.add(performance_file)
            db.commit()
            
            # åˆ›å»ºAIåˆ†æä»»åŠ¡
            task_id = None
            if enable_ai_analysis:
                task_id = create_ai_task(db, performance.id, "performance")
                logger.info(f"ğŸ“‹ AIä»»åŠ¡å·²åˆ›å»º: ä»»åŠ¡ID={task_id}, ä¸šç»©ID={performance.id}")
            
            uploaded_performances.append({
                "id": performance.id,
                "project_name": performance.project_name,
                "file_name": file.filename,
                "ai_analysis_status": "pending",
                "task_id": task_id  # è¿”å›ä»»åŠ¡ID
            })
            uploaded_files.append(file.filename)
            
            logger.info(f"âœ… ä¸šç»©è®°å½•å·²åˆ›å»º: ID={performance.id}, é¡¹ç›®åç§°={performance.project_name}")
        
    except Exception as e:
        logger.error(f"âŒ æ–‡ä»¶ä¿å­˜å¤±è´¥: {str(e)}")
        db.rollback()
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}")
    
    # å¼‚æ­¥è¿›è¡ŒAIåˆ†æï¼ˆåœ¨åå°ä¸ºæ¯ä¸ªè®°å½•åˆ†åˆ«å¤„ç†ï¼‰
    if enable_ai_analysis:
        # å¯åŠ¨åå°ä»»åŠ¡è¿›è¡ŒAIåˆ†æ
        import asyncio
        asyncio.create_task(
            analyze_performances_in_background(
                [perf["id"] for perf in uploaded_performances],
                enable_vision_analysis,
                db
            )
        )
        
        logger.info(f"ğŸ¤– å·²å¯åŠ¨åå°AIåˆ†æä»»åŠ¡ï¼Œå…±{len(uploaded_performances)}ä¸ªæ–‡ä»¶")
    
    # ç«‹å³è¿”å›ç»“æœï¼Œä¸ç­‰å¾…AIåˆ†æå®Œæˆ
    return {
        "success": True,
        "message": f"æˆåŠŸä¸Šä¼  {len(files)} ä¸ªæ–‡ä»¶ï¼Œå·²åˆ›å»º {len(uploaded_performances)} æ¡ä¸šç»©è®°å½•",
        "performances": uploaded_performances,
        "uploaded_files": uploaded_files,
        "ai_analysis": {
            "enabled": enable_ai_analysis,
            "status": "background_processing" if enable_ai_analysis else "disabled",
            "total_files": len(files)
        }
    }

async def analyze_performances_in_background(performance_ids: List[int], enable_vision_analysis: bool, db: Session):
    """åå°å¼‚æ­¥åˆ†æä¸šç»©è®°å½•"""
    try:
        from ai_service import ai_service
        
        if not ai_service.enable_ai:
            logger.warning("AIæœåŠ¡æœªå¯ç”¨ï¼Œè·³è¿‡åå°åˆ†æ")
            return
        
        logger.info(f"ğŸ¤– å¼€å§‹åå°AIåˆ†æï¼Œå…±{len(performance_ids)}æ¡è®°å½•")
        
        for performance_id in performance_ids:
            task_id = None
            try:
                # é‡æ–°è·å–performanceè®°å½•
                performance = db.query(Performance).filter(Performance.id == performance_id).first()
                
                if not performance or not performance.source_document:
                    logger.warning(f"âš ï¸ ä¸šç»©è®°å½•æˆ–æ–‡ä»¶ä¸å­˜åœ¨: {performance_id}")
                    continue
                
                # æŸ¥æ‰¾å¯¹åº”çš„AIä»»åŠ¡
                task = db.query(AITask).filter(
                    AITask.file_id == performance_id,
                    AITask.file_type == "performance"
                ).first()
                
                if task:
                    task_id = task.id
                    # æ›´æ–°AIä»»åŠ¡çŠ¶æ€ä¸ºprocessing
                    update_ai_task(db, task_id, "processing")
                
                # æ›´æ–°çŠ¶æ€ä¸ºæ­£åœ¨åˆ†æ
                performance.ai_analysis_status = "analyzing"
                performance.description = f"æ­£åœ¨è¿›è¡ŒAIåˆ†æ..."
                db.commit()
                
                logger.info(f"ğŸ” å¼€å§‹åˆ†æä¸šç»©è®°å½• {performance_id}: {performance.project_name}")
                
                # è¿›è¡ŒAIåˆ†æ
                ai_result = await ai_service.smart_document_analysis(
                    performance.source_document,
                    enable_vision=enable_vision_analysis,
                    enable_ocr=True
                )
                
                if ai_result and ai_result.get("success"):
                    # æå–ä¸šç»©ä¿¡æ¯
                    extracted_info = await _extract_performance_info(ai_result)
                    
                    # æ›´æ–°ä¸šç»©è®°å½•ï¼ˆåªåœ¨AIæå–åˆ°æœ‰æ•ˆä¿¡æ¯æ—¶æ›´æ–°ï¼‰
                    updated_fields = []
                    
                    # å¦‚æœAIæå–åˆ°äº†æœ‰æ„ä¹‰çš„é¡¹ç›®åç§°ï¼Œåˆ™æ›´æ–°
                    if extracted_info.get('project_name') and len(extracted_info['project_name']) > 3:
                        performance.project_name = extracted_info['project_name']
                        updated_fields.append("project_name")
                    
                    if extracted_info.get('client_name') and performance.client_name == "å¾…AIåˆ†æ":
                        performance.client_name = extracted_info['client_name']
                        updated_fields.append("client_name")
                    
                    if extracted_info.get('business_field') and performance.business_field == "å¾…AIåˆ†æ":
                        performance.business_field = extracted_info['business_field']
                        updated_fields.append("business_field")
                    
                    if extracted_info.get('contract_amount'):
                        performance.contract_amount = extracted_info['contract_amount']
                        updated_fields.append("contract_amount")
                    
                    if extracted_info.get('year'):
                        performance.year = extracted_info['year']
                        updated_fields.append("year")
                    
                    # æ›´æ–°åˆ†æç»“æœ
                    performance.description = extracted_info.get('description', "AIåˆ†æå®Œæˆ")
                    performance.confidence_score = ai_result.get("results", {}).get("final_classification", {}).get("confidence", 0.0)
                    performance.ai_analysis_status = "completed"
                    
                    # ä¿å­˜AIåˆ†æç»“æœ
                    performance.ai_analysis = {
                        "analysis_result": ai_result,
                        "extracted_info": extracted_info,
                        "updated_fields": updated_fields,
                        "analysis_timestamp": datetime.now().isoformat()
                    }
                    
                    db.commit()
                    
                    # æ›´æ–°AIä»»åŠ¡çŠ¶æ€ä¸ºæˆåŠŸ
                    if task_id:
                        update_ai_task(db, task_id, "success", result={
                            "analysis_result": ai_result,
                            "extracted_info": extracted_info,
                            "updated_fields": updated_fields,
                            "confidence_score": performance.confidence_score
                        })
                    
                    logger.info(f"âœ… AIåˆ†æå®Œæˆ: è®°å½•{performance_id}, æ›´æ–°å­—æ®µ: {updated_fields}")
                
                else:
                    # AIåˆ†æå¤±è´¥
                    performance.ai_analysis_status = "failed"
                    performance.description = "AIåˆ†æå¤±è´¥ï¼Œè¯·æ‰‹åŠ¨ç¼–è¾‘"
                    db.commit()
                    
                    # æ›´æ–°AIä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
                    if task_id:
                        update_ai_task(db, task_id, "failed", error="AIåˆ†ææœªè¿”å›æˆåŠŸç»“æœ")
                    
                    logger.warning(f"âš ï¸ AIåˆ†æå¤±è´¥: è®°å½•{performance_id}")
                
            except Exception as single_error:
                logger.error(f"âŒ å•ä¸ªè®°å½•åˆ†æå¤±è´¥ {performance_id}: {str(single_error)}")
                
                # æ ‡è®°åˆ†æå¤±è´¥
                try:
                    performance = db.query(Performance).filter(Performance.id == performance_id).first()
                    if performance:
                        performance.ai_analysis_status = "failed"
                        performance.description = f"AIåˆ†æå‡ºé”™: {str(single_error)}"
                        db.commit()
                    
                    # æ›´æ–°AIä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
                    if task_id:
                        update_ai_task(db, task_id, "failed", error=str(single_error))
                        
                except:
                    pass
        
        logger.info(f"ğŸ‰ åå°AIåˆ†æä»»åŠ¡å®Œæˆï¼Œå…±å¤„ç†{len(performance_ids)}æ¡è®°å½•")
        
    except Exception as e:
        logger.error(f"âŒ åå°AIåˆ†æä»»åŠ¡å¤±è´¥: {str(e)}")

async def _extract_performance_info(ai_result):
    """ä»AIåˆ†æç»“æœä¸­æå–ä¸šç»©ä¿¡æ¯å¹¶ä½¿ç”¨AIæ™ºèƒ½æå–æ‰€æœ‰ä¿¡æ¯"""
    extracted_info = {}
    
    if not ai_result:
        return extracted_info
    
    # ä»æ–°çš„AIåˆ†æç»“æœç»“æ„ä¸­æå–æ–‡æœ¬å†…å®¹
    text_content = ""
    
    # æ–°ç»“æ„ï¼šai_result.results.text_extraction_result.text
    if ai_result.get("results", {}).get("text_extraction_result", {}).get("text"):
        text_content = ai_result["results"]["text_extraction_result"]["text"]
    # å…¼å®¹æ—§ç»“æ„ï¼šai_result.text_extraction_result.text
    elif ai_result.get('text_extraction_result', {}).get('text'):
        text_content = ai_result['text_extraction_result']['text']
    
    # æ·»åŠ è°ƒè¯•æ—¥å¿—
    logger.info(f"æå–çš„æ–‡æœ¬å†…å®¹é•¿åº¦: {len(text_content)} å­—ç¬¦")
    if text_content:
        logger.info(f"æ–‡æœ¬å†…å®¹é¢„è§ˆ: {text_content[:200]}...")
    else:
        logger.warning("æ²¡æœ‰æ‰¾åˆ°æ–‡æœ¬å†…å®¹")
    
    # ä»OCRç»“æœä¸­å¯»æ‰¾å…³é”®ä¿¡æ¯ï¼ˆå…¼å®¹å¤šç§ç»“æ„ï¼‰
    ocr_text = ""
    if ai_result.get("results", {}).get("ocr_result", {}).get("text"):
        ocr_text = ai_result["results"]["ocr_result"]["text"]
    elif ai_result.get('ocr_result', {}).get('text'):
        ocr_text = ai_result['ocr_result']['text']
    
    # ä»æœ€ç»ˆåˆ†ç±»ç»“æœä¸­æå–ä¿¡æ¯
    final_classification = ai_result.get("results", {}).get("final_classification", {})
    if final_classification:
        if final_classification.get("business_field"):
            extracted_info['business_field'] = final_classification["business_field"]
        if final_classification.get("description"):
            extracted_info['description'] = final_classification["description"]
    
    # åˆå¹¶æ‰€æœ‰æ–‡æœ¬å†…å®¹
    full_text = f"{text_content}\n{ocr_text}".strip()
    
    if not full_text:
        logger.warning("åˆå¹¶åçš„æ–‡æœ¬å†…å®¹ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
        return extracted_info
    
    logger.info(f"åˆå¹¶åçš„æ–‡æœ¬é•¿åº¦: {len(full_text)} å­—ç¬¦")
    
    # ä½¿ç”¨AIæ™ºèƒ½æå–å®Œæ•´çš„ä¸šç»©ä¿¡æ¯
    try:
        from ai_service import ai_service
        from config_manager import ConfigManager
        
        config_manager = ConfigManager()
        
        # æ„å»ºä¸šç»©åˆ†æçš„prompt
        performance_analysis_prompt = config_manager.build_prompt("performance_analysis", {
            "text_content": full_text[:2000]  # é™åˆ¶æ–‡æœ¬é•¿åº¦é¿å…tokenè¿‡å¤š
        })
        
        if ai_service.enable_ai and performance_analysis_prompt:
            logger.info("ğŸ¤– ä½¿ç”¨AIæ™ºèƒ½æå–ä¸šç»©ä¿¡æ¯...")
            
            # è°ƒç”¨AIè¿›è¡Œå®Œæ•´çš„ä¸šç»©ä¿¡æ¯æå–
            ai_extraction_result = await ai_service.analyze_text(performance_analysis_prompt)
            
            if ai_extraction_result.get("success"):
                import json
                try:
                    # å°è¯•è§£æAIè¿”å›çš„JSONç»“æœ
                    extraction_data = ai_extraction_result.get("result", {})
                    if isinstance(extraction_data, dict):
                        pass  # å·²ç»æ˜¯å­—å…¸æ ¼å¼
                    else:
                        # å°è¯•ä»å­—ç¬¦ä¸²ä¸­è§£æJSON
                        extraction_data = json.loads(str(extraction_data))
                    
                    # æå–AIè¯†åˆ«çš„æ‰€æœ‰ä¿¡æ¯
                    if extraction_data.get("client_name"):
                        extracted_info['client_name'] = extraction_data["client_name"]
                        logger.info(f"âœ… AIæå–å®¢æˆ·åç§°: {extraction_data['client_name']}")
                    
                    if extraction_data.get("project_type"):
                        extracted_info['project_type'] = extraction_data["project_type"]
                        logger.info(f"âœ… AIåˆ¤æ–­é¡¹ç›®ç±»å‹: {extraction_data['project_type']}")
                    
                    if extraction_data.get("business_field"):
                        extracted_info['business_field'] = extraction_data["business_field"]
                        logger.info(f"âœ… AIåˆ¤æ–­ä¸šåŠ¡é¢†åŸŸ: {extraction_data['business_field']}")
                    
                    if extraction_data.get("project_description"):
                        extracted_info['project_description'] = extraction_data["project_description"]
                        
                    if extraction_data.get("case_cause"):
                        extracted_info['case_cause'] = extraction_data["case_cause"]
                    
                    if extraction_data.get("contract_amount"):
                        try:
                            extracted_info['contract_amount'] = float(extraction_data["contract_amount"])
                            logger.info(f"âœ… AIæå–åˆåŒé‡‘é¢: {extraction_data['contract_amount']}")
                        except:
                            pass
                    
                    if extraction_data.get("year"):
                        try:
                            extracted_info['year'] = int(extraction_data["year"])
                            logger.info(f"âœ… AIæå–å¹´ä»½: {extraction_data['year']}")
                        except:
                            pass
                    
                    if extraction_data.get("confidence"):
                        extracted_info['ai_confidence'] = extraction_data["confidence"]
                    
                    if extraction_data.get("reasoning"):
                        logger.info(f"ğŸ’¡ AIæå–ç†ç”±: {extraction_data['reasoning']}")
                        
                    # AIæå–æˆåŠŸï¼Œè·³è¿‡å…³é”®è¯åŒ¹é…
                    logger.info("âœ… AIæ™ºèƒ½æå–å®Œæˆï¼Œè·³è¿‡å…³é”®è¯åŒ¹é…")
                    
                except json.JSONDecodeError as e:
                    logger.warning(f"AIè¿”å›ç»“æœä¸æ˜¯æœ‰æ•ˆJSONï¼Œä½¿ç”¨å…³é”®è¯å›é€€: {e}")
                    raise Exception("JSONè§£æå¤±è´¥")
                except Exception as e:
                    logger.warning(f"AIæå–å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨å…³é”®è¯å›é€€: {e}")
                    raise Exception("AIæå–å¤„ç†å¤±è´¥")
            else:
                logger.warning("AIæå–æœªè¿”å›ç»“æœï¼Œä½¿ç”¨å…³é”®è¯å›é€€")
                raise Exception("AIæ— è¿”å›ç»“æœ")
        else:
            logger.warning("AIæœåŠ¡æœªå¯ç”¨æˆ–promptæ„å»ºå¤±è´¥ï¼Œä½¿ç”¨å…³é”®è¯å›é€€")
            raise Exception("AIæœåŠ¡ä¸å¯ç”¨")
            
    except Exception as e:
        logger.warning(f"AIæ™ºèƒ½æå–å¤±è´¥: {e}ï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…ä½œä¸ºå›é€€æ–¹æ¡ˆ")
        
        # å…³é”®è¯åŒ¹é…ä½œä¸ºå›é€€æ–¹æ¡ˆï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼‰
        import re
        full_text_lower = full_text.lower()
        
        # ä½¿ç”¨å…³é”®è¯åŒ¹é…æå–åŸºç¡€ä¿¡æ¯ï¼ˆå¦‚æœAIæ²¡æœ‰æå–åˆ°ï¼‰
        if not extracted_info.get('client_name'):
            client_patterns = [
                r'ç”²æ–¹[ï¼š:]\s*([^\n\rï¼Œã€‚ï¼›]+)',
                r'å§”æ‰˜æ–¹[ï¼š:]\s*([^\n\rï¼Œã€‚ï¼›]+)', 
                r'å®¢æˆ·[ï¼š:]\s*([^\n\rï¼Œã€‚ï¼›]+)',
                r'å§”æ‰˜äºº[ï¼š:]\s*([^\n\rï¼Œã€‚ï¼›]+)',
                r'å½“äº‹äºº[ï¼š:]\s*([^\n\rï¼Œã€‚ï¼›]+)',
                r'ç”³è¯·äºº[ï¼š:]\s*([^\n\rï¼Œã€‚ï¼›]+)'
            ]
            for pattern in client_patterns:
                match = re.search(pattern, full_text)
                if match:
                    client_name = match.group(1).strip()
                    client_name = re.sub(r'ï¼ˆ.*?ï¼‰|\(.*?\)', '', client_name)
                    client_name = re.sub(r'æ³•å®šä»£è¡¨äºº.*|ç»Ÿä¸€ç¤¾ä¼šä¿¡ç”¨ä»£ç .*|åœ°å€.*', '', client_name)
                    client_name = client_name.strip()
                    if len(client_name) >= 2 and len(client_name) < 50:
                        extracted_info['client_name'] = client_name
                        logger.info(f"ğŸ”„ å…³é”®è¯æå–å®¢æˆ·åç§°: {client_name}")
                        break
        
        # åˆ¤æ–­é¡¹ç›®ç±»å‹ï¼ˆå¦‚æœAIæ²¡æœ‰åˆ¤æ–­ï¼‰
        if not extracted_info.get('project_type'):
            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯ç ´äº§é‡æ•´ç›¸å…³ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
            if any(keyword in full_text_lower for keyword in ['ç ´äº§', 'é‡æ•´', 'å€ºæƒç”³æŠ¥', 'å€ºåŠ¡é‡ç»„', 'ä¼ä¸šé‡æ•´', 'ç ´äº§æ¸…ç®—', 'é‡æ•´è®¡åˆ’']):
                extracted_info['project_type'] = "é‡å¤§ä¸ªæ¡ˆ(éè¯‰)"
                logger.info("ğŸ”„ å…³é”®è¯åŒ¹é…åˆ¤æ–­ä¸º: é‡å¤§ä¸ªæ¡ˆ(éè¯‰) (ç ´äº§é‡æ•´ç›¸å…³)")
                
                if 'å€ºæƒç”³æŠ¥' in full_text_lower:
                    extracted_info['project_description'] = "ç ´äº§é‡æ•´å€ºæƒç”³æŠ¥"
                elif 'é‡æ•´' in full_text_lower:
                    extracted_info['project_description'] = "ç ´äº§é‡æ•´"
                elif 'å€ºåŠ¡é‡ç»„' in full_text_lower:
                    extracted_info['project_description'] = "å€ºåŠ¡é‡ç»„"
                else:
                    extracted_info['project_description'] = "ç ´äº§é‡æ•´"
                    
            # ç„¶åæ£€æŸ¥æ˜¯å¦æ˜¯è¯‰è®¼ä»²è£ï¼ˆæ’é™¤ç ´äº§é‡æ•´ç›¸å…³ï¼‰
            elif any(keyword in full_text_lower for keyword in ['è¯‰è®¼', 'ä»²è£', 'æ³•é™¢', 'æ¡ˆä»¶', 'èµ·è¯‰', 'åº”è¯‰']) or \
                 (any(keyword in full_text_lower for keyword in ['çº çº·', 'äº‰è®®', 'ä»£ç†']) and \
                  not any(keyword in full_text_lower for keyword in ['ç ´äº§', 'é‡æ•´', 'å€ºæƒç”³æŠ¥', 'å€ºåŠ¡é‡ç»„'])):
                extracted_info['project_type'] = "è¯‰è®¼ä»²è£"
                logger.info("ğŸ”„ å…³é”®è¯åŒ¹é…åˆ¤æ–­ä¸º: è¯‰è®¼ä»²è£")
                
            elif any(keyword in full_text_lower for keyword in ['å¸¸å¹´', 'é¡¾é—®', 'å¹´åº¦æ³•å¾‹æœåŠ¡', 'é•¿æœŸæœåŠ¡']):
                extracted_info['project_type'] = "å¸¸å¹´æ³•å¾‹é¡¾é—®"
                logger.info("ğŸ”„ å…³é”®è¯åŒ¹é…åˆ¤æ–­ä¸º: å¸¸å¹´æ³•å¾‹é¡¾é—®")
                
            else:
                extracted_info['project_type'] = "é‡å¤§ä¸ªæ¡ˆ(éè¯‰)"
                logger.info("ğŸ”„ å…³é”®è¯åŒ¹é…åˆ¤æ–­ä¸º: é‡å¤§ä¸ªæ¡ˆ(éè¯‰)")
        
        # å…¶ä»–ä¿¡æ¯æå–ï¼ˆé‡‘é¢ã€å¹´ä»½ç­‰ï¼‰
        if not extracted_info.get('contract_amount'):
            amount_patterns = [
                r'åˆåŒé‡‘é¢[ï¼š:]?\s*([0-9,ï¼Œ.]+)\s*ä¸‡?å…ƒ',
                r'æ€»é‡‘é¢[ï¼š:]?\s*([0-9,ï¼Œ.]+)\s*ä¸‡?å…ƒ',
                r'è´¹ç”¨[ï¼š:]?\s*([0-9,ï¼Œ.]+)\s*ä¸‡?å…ƒ',
                r'å¾‹å¸ˆè´¹[ï¼š:]?\s*([0-9,ï¼Œ.]+)\s*ä¸‡?å…ƒ',
                r'æœåŠ¡è´¹[ï¼š:]?\s*([0-9,ï¼Œ.]+)\s*ä¸‡?å…ƒ'
            ]
            for pattern in amount_patterns:
                match = re.search(pattern, full_text)
                if match:
                    try:
                        amount_str = match.group(1).replace(',', '').replace('ï¼Œ', '')
                        extracted_info['contract_amount'] = float(amount_str)
                        logger.info(f"ğŸ”„ å…³é”®è¯æå–åˆåŒé‡‘é¢: {amount_str}")
                        break
                    except:
                        pass
        
        if not extracted_info.get('year'):
            year_patterns = [
                r'(\d{4})\s*å¹´',
                r'ç­¾è®¢æ—¶é—´[ï¼š:]\s*(\d{4})',
                r'åˆåŒæ—¥æœŸ[ï¼š:]\s*(\d{4})',
                r'(\d{4})[å¹´/-]\d{1,2}[æœˆ/-]\d{1,2}'
            ]
            for pattern in year_patterns:
                match = re.search(pattern, full_text)
                if match:
                    try:
                        year = int(match.group(1))
                        if 2000 <= year <= 2030:
                            extracted_info['year'] = year
                            logger.info(f"ğŸ”„ å…³é”®è¯æå–å¹´ä»½: {year}")
                            break
                    except:
                        pass
    
    # ç”Ÿæˆè§„èŒƒåŒ–é¡¹ç›®åç§°
    client_name = extracted_info.get('client_name', '').strip()
    project_type = extracted_info.get('project_type', '')
    current_year = extracted_info.get('year')
    
    if client_name and project_type:
        if project_type == "è¯‰è®¼ä»²è£":
            case_cause = extracted_info.get('case_cause', 'å•†äº‹çº çº·')
            standardized_name = f"ä»£è¡¨{client_name}çš„{case_cause}"
            if not case_cause.endswith('çº çº·'):
                standardized_name += "çº çº·" if not case_cause.endswith('äº‰è®®') else ""
        elif project_type == "å¸¸å¹´æ³•å¾‹é¡¾é—®":
            year_suffix = f"({current_year}å¹´åº¦)" if current_year else ""
            standardized_name = f"{client_name}å¸¸å¹´æ³•å¾‹é¡¾é—®{year_suffix}"
        else:  # é‡å¤§ä¸ªæ¡ˆ(éè¯‰)
            project_desc = extracted_info.get('project_description', 'æ³•å¾‹æœåŠ¡')
            standardized_name = f"ä»£è¡¨{client_name}çš„{project_desc}é¡¹ç›®"
        
        extracted_info['project_name'] = standardized_name
        logger.info(f"ç”Ÿæˆè§„èŒƒåŒ–é¡¹ç›®åç§°: {standardized_name}")
    
    # ç”Ÿæˆé¡¹ç›®æè¿°
    if not extracted_info.get('description'):
        business_field = extracted_info.get('business_field', 'æ³•å¾‹æœåŠ¡')
        
        if project_type == "è¯‰è®¼ä»²è£":
            case_cause = extracted_info.get('case_cause', 'å•†äº‹çº çº·')
            description = f"ä»£è¡¨{client_name}å¤„ç†{case_cause}æ¡ˆä»¶ï¼Œæä¾›ä¸“ä¸šçš„è¯‰è®¼ä»£ç†æœåŠ¡"
        elif project_type == "å¸¸å¹´æ³•å¾‹é¡¾é—®":
            description = f"ä¸º{client_name}æä¾›{current_year or ''}å¹´åº¦å¸¸å¹´æ³•å¾‹é¡¾é—®æœåŠ¡ï¼Œæ¶µç›–æ—¥å¸¸æ³•å¾‹äº‹åŠ¡å’¨è¯¢å’Œé£é™©é˜²æ§"
        else:
            project_desc = extracted_info.get('project_description', 'æ³•å¾‹æœåŠ¡')
            description = f"ä»£è¡¨{client_name}çš„{project_desc}é¡¹ç›®ï¼Œåœ¨{business_field}é¢†åŸŸæä¾›ä¸“ä¸šæ³•å¾‹æœåŠ¡"
        
        extracted_info['description'] = description
        logger.info(f"ç”Ÿæˆé¡¹ç›®æè¿°: {description}")
    
    logger.info(f"æœ€ç»ˆæå–çš„ä¸šç»©ä¿¡æ¯: {extracted_info}")
    return extracted_info

@router.patch("/{performance_id}")
async def update_performance(
    performance_id: int,
    performance_data: dict,
    db: Session = Depends(get_db)
):
    """æ›´æ–°ä¸šç»©è®°å½•"""
    try:
        performance = db.query(Performance).filter(Performance.id == performance_id).first()
        if not performance:
            raise HTTPException(status_code=404, detail="ä¸šç»©è®°å½•ä¸å­˜åœ¨")
        
        # æ›´æ–°å­—æ®µ
        updateable_fields = [
            'project_name', 'client_name', 'project_type', 'business_field',
            'year', 'contract_amount', 'currency', 'start_date', 'end_date',
            'description', 'is_manual_input'
        ]
        
        for field in updateable_fields:
            if field in performance_data:
                if field in ['start_date', 'end_date'] and performance_data[field]:
                    # å¤„ç†æ—¥æœŸå­—æ®µ
                    setattr(performance, field, datetime.fromisoformat(performance_data[field]))
                elif field == 'year' and performance_data[field]:
                    # å¤„ç†å¹´ä»½å­—æ®µ
                    setattr(performance, field, int(performance_data[field]))
                else:
                    setattr(performance, field, performance_data[field])
        
        performance.updated_at = datetime.now()
        db.commit()
        
        logger.info(f"ä¸šç»©è®°å½•æ›´æ–°æˆåŠŸ: ID={performance_id}")
        
        return {
            "success": True,
            "message": "ä¸šç»©è®°å½•æ›´æ–°æˆåŠŸ"
        }
        
    except Exception as e:
        db.rollback()
        logger.error(f"æ›´æ–°ä¸šç»©è®°å½•å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ›´æ–°ä¸šç»©è®°å½•å¤±è´¥: {str(e)}")

@router.patch("/{performance_id}/verify")
async def verify_performance(
    performance_id: int,
    verification_notes: Optional[str] = None,
    db: Session = Depends(get_db)
):
    """ç”¨æˆ·ä¸»åŠ¨éªŒè¯ä¸šç»©è®°å½•"""
    try:
        performance = db.query(Performance).filter(Performance.id == performance_id).first()
        if not performance:
            raise HTTPException(status_code=404, detail="ä¸šç»©è®°å½•ä¸å­˜åœ¨")
        
        # è®°å½•éªŒè¯å†å²
        if not performance.verification_history:
            performance.verification_history = []
        
        verification_record = {
            "timestamp": datetime.now().isoformat(),
            "verified_by": "user",  # å¯ä»¥æ‰©å±•ä¸ºå…·ä½“ç”¨æˆ·ID
            "verification_notes": verification_notes,
            "previous_status": performance.is_verified,
            "ai_confidence_score": performance.confidence_score,
            "ai_analysis_status": performance.ai_analysis_status
        }
        
        performance.verification_history.append(verification_record)
        performance.is_verified = True
        performance.verified_at = datetime.now()
        performance.verification_notes = verification_notes
        
        db.commit()
        
        logger.info(f"ä¸šç»©è®°å½•éªŒè¯æˆåŠŸ: ID={performance_id}, éªŒè¯å¤‡æ³¨={verification_notes}")
        
        return {
            "success": True,
            "message": "ä¸šç»©è®°å½•éªŒè¯æˆåŠŸ",
            "verification_record": verification_record
        }
        
    except Exception as e:
        db.rollback()
        logger.error(f"éªŒè¯ä¸šç»©è®°å½•å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"éªŒè¯å¤±è´¥: {str(e)}")

@router.patch("/{performance_id}/unverify")
async def unverify_performance(
    performance_id: int,
    unverification_reason: Optional[str] = None,
    db: Session = Depends(get_db)
):
    """å–æ¶ˆéªŒè¯ä¸šç»©è®°å½•ï¼ˆé‡æ–°éªŒè¯ï¼‰"""
    try:
        performance = db.query(Performance).filter(Performance.id == performance_id).first()
        if not performance:
            raise HTTPException(status_code=404, detail="ä¸šç»©è®°å½•ä¸å­˜åœ¨")
        
        # è®°å½•å–æ¶ˆéªŒè¯å†å²
        if not performance.verification_history:
            performance.verification_history = []
        
        unverification_record = {
            "timestamp": datetime.now().isoformat(),
            "action": "unverify",
            "unverified_by": "user",
            "unverification_reason": unverification_reason,
            "previous_status": performance.is_verified,
            "ai_confidence_score": performance.confidence_score,
            "ai_analysis_status": performance.ai_analysis_status
        }
        
        performance.verification_history.append(unverification_record)
        performance.is_verified = False
        performance.verified_at = None
        performance.verification_notes = None
        
        db.commit()
        
        logger.info(f"ä¸šç»©è®°å½•å–æ¶ˆéªŒè¯æˆåŠŸ: ID={performance_id}, åŸå› ={unverification_reason}")
        
        return {
            "success": True,
            "message": "ä¸šç»©è®°å½•å·²å–æ¶ˆéªŒè¯ï¼Œå¯ä»¥é‡æ–°éªŒè¯",
            "unverification_record": unverification_record
        }
        
    except Exception as e:
        db.rollback()
        logger.error(f"å–æ¶ˆéªŒè¯ä¸šç»©è®°å½•å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"å–æ¶ˆéªŒè¯å¤±è´¥: {str(e)}")

@router.get("/{performance_id}/verification-history")
async def get_verification_history(
    performance_id: int,
    db: Session = Depends(get_db)
):
    """è·å–ä¸šç»©è®°å½•çš„éªŒè¯å†å²"""
    try:
        performance = db.query(Performance).filter(Performance.id == performance_id).first()
        if not performance:
            raise HTTPException(status_code=404, detail="ä¸šç»©è®°å½•ä¸å­˜åœ¨")
        
        return {
            "success": True,
            "verification_history": performance.verification_history or [],
            "current_status": {
                "is_verified": performance.is_verified,
                "verified_at": performance.verified_at.isoformat() if performance.verified_at else None,
                "verification_notes": performance.verification_notes,
                "ai_confidence_score": performance.confidence_score,
                "ai_analysis_status": performance.ai_analysis_status
            }
        }
        
    except Exception as e:
        logger.error(f"è·å–éªŒè¯å†å²å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–éªŒè¯å†å²å¤±è´¥: {str(e)}")

@router.post("/{performance_id}/reanalyze")
async def reanalyze_performance(
    performance_id: int,
    enable_vision_analysis: bool = Form(True),
    enable_ocr: bool = Form(True),
    update_fields: bool = Form(False),  # æ˜¯å¦ä½¿ç”¨AIç»“æœæ›´æ–°å­—æ®µ
    db: Session = Depends(get_db)
):
    """é‡æ–°åˆ†æä¸šç»©æ–‡ä»¶"""
    try:
        from ai_service import ai_service
        
        performance = db.query(Performance).filter(Performance.id == performance_id).first()
        if not performance:
            raise HTTPException(status_code=404, detail="ä¸šç»©è®°å½•ä¸å­˜åœ¨")
        
        if not performance.source_document or not os.path.exists(performance.source_document):
            raise HTTPException(status_code=400, detail="æºæ–‡æ¡£ä¸å­˜åœ¨ï¼Œæ— æ³•é‡æ–°åˆ†æ")
        
        # æ£€æŸ¥OCRè®¾ç½®
        ocr_enabled = False
        try:
            from database import get_db
            from models import SystemSettings
            
            db_check = next(get_db())
            ocr_setting = db_check.query(SystemSettings).filter(
                SystemSettings.setting_key == "docling_enable_ocr"
            ).first()
            
            if ocr_setting:
                ocr_enabled = ocr_setting.setting_value.lower() == "true"
            db_check.close()
        except Exception as e:
            logger.warning(f"æ£€æŸ¥OCRè®¾ç½®å¤±è´¥: {e}")
        
        # å¦‚æœç”¨æˆ·è¯·æ±‚OCRä½†OCRå·²å…³é—­ï¼Œç»™å‡ºæç¤º
        if enable_ocr and not ocr_enabled:
            logger.info("âš ï¸ ç”¨æˆ·è¯·æ±‚OCRåˆ†æï¼Œä½†OCRåŠŸèƒ½å·²å…³é—­")
            # ç»§ç»­æ‰§è¡Œï¼Œä½†OCRä¼šè¢«AIæœåŠ¡è‡ªåŠ¨ç¦ç”¨
        
        # æ›´æ–°çŠ¶æ€ä¸ºæ­£åœ¨åˆ†æ
        performance.ai_analysis_status = "analyzing"
        db.commit()
        
        # è°ƒç”¨AIæœåŠ¡é‡æ–°åˆ†ææ–‡æ¡£
        logger.info(f"å¼€å§‹é‡æ–°åˆ†æä¸šç»©æ–‡ä»¶: {performance.source_document}")
        
        if not ai_service.enable_ai:
            raise HTTPException(status_code=503, detail="AIæœåŠ¡æœªå¯ç”¨ï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
        
        try:
            ai_result = await ai_service.smart_document_analysis(
                performance.source_document,
                enable_vision=enable_vision_analysis,
                enable_ocr=enable_ocr
            )
            
            if not ai_result:
                raise Exception("AIåˆ†ææœªè¿”å›ç»“æœ")
            
            logger.info(f"AIé‡æ–°åˆ†æå®Œæˆ: {performance.source_document}")
            
            # ä»AIç»“æœä¸­æå–ä¸šç»©ä¿¡æ¯
            extracted_info = await _extract_performance_info(ai_result)
            logger.info(f"é‡æ–°æå–çš„ä¸šç»©ä¿¡æ¯: {extracted_info}")

            # æ›´æ–°AIåˆ†æç»“æœ
            performance.ai_analysis = {
                "reanalysis_result": ai_result,
                "reanalysis_timestamp": datetime.now().isoformat(),
                "extracted_info": extracted_info
            }
            
            # è®¾ç½®ç½®ä¿¡åº¦
            if ai_result.get("final_classification"):
                classification = ai_result["final_classification"]
                performance.confidence_score = classification.get("confidence", 0.0)
            
            # æå–æ–‡æœ¬å†…å®¹
            if ai_result.get('text_extraction_result', {}).get('text'):
                performance.extracted_text = ai_result['text_extraction_result']['text']
            
            # å¦‚æœå¯ç”¨äº†å­—æ®µæ›´æ–°ï¼Œåˆ™ä½¿ç”¨AIç»“æœæ›´æ–°ä¸šç»©å­—æ®µ
            if update_fields:
                if extracted_info.get('project_name'):
                    performance.project_name = extracted_info['project_name']
                if extracted_info.get('client_name'):
                    performance.client_name = extracted_info['client_name']
                if extracted_info.get('business_field'):
                    performance.business_field = extracted_info['business_field']
                if extracted_info.get('contract_amount'):
                    performance.contract_amount = extracted_info['contract_amount']
                if extracted_info.get('year'):
                    performance.year = extracted_info['year']
                if extracted_info.get('description'):
                    performance.description = extracted_info['description']
                
                logger.info(f"å·²ä½¿ç”¨AIç»“æœæ›´æ–°ä¸šç»©å­—æ®µ")
            
            # æ›´æ–°AIåˆ†æçŠ¶æ€ä¸ºå®Œæˆ
            performance.ai_analysis_status = "completed"
            db.commit()
            
            # æ„å»ºå“åº”æ¶ˆæ¯
            response_message = "AIé‡æ–°åˆ†æå®Œæˆ"
            if enable_ocr and not ocr_enabled:
                response_message += "ï¼ˆæ³¨æ„ï¼šOCRåŠŸèƒ½å·²å…³é—­ï¼Œåˆ†æä¸åŒ…å«OCRæ–‡æœ¬è¯†åˆ«ï¼‰"
        
            return {
                "success": True,
                "message": response_message,
                "ai_analysis": {
                    "confidence_score": performance.confidence_score,
                    "extracted_info": extracted_info,
                    "fields_updated": update_fields,
                    "ocr_status": ai_result.get("ocr_status", "unknown")
                }
            }
                    
        except Exception as e:
            logger.error(f"AIé‡æ–°åˆ†æå¤±è´¥: {str(e)}")
            # æ›´æ–°AIåˆ†æçŠ¶æ€ä¸ºå¤±è´¥
            performance.ai_analysis_status = "failed"
            db.commit()
            raise HTTPException(status_code=500, detail=f"AIé‡æ–°åˆ†æå¤±è´¥: {str(e)}")
        
    except Exception as e:
        db.rollback()
        logger.error(f"é‡æ–°åˆ†æä¸šç»©å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"é‡æ–°åˆ†æå¤±è´¥: {str(e)}")

@router.delete("/{performance_id}")
async def delete_performance(
    performance_id: int,
    db: Session = Depends(get_db)
):
    """åˆ é™¤ä¸šç»©è®°å½•"""
    try:
        performance = db.query(Performance).filter(Performance.id == performance_id).first()
        if not performance:
            raise HTTPException(status_code=404, detail="ä¸šç»©è®°å½•ä¸å­˜åœ¨")
        
        # åˆ é™¤å…³è”æ–‡ä»¶
        performance_files = db.query(PerformanceFile).filter(PerformanceFile.performance_id == performance_id).all()
        for perf_file in performance_files:
            if perf_file.file_path and os.path.exists(perf_file.file_path):
                try:
                    os.remove(perf_file.file_path)
                    logger.info(f"å·²åˆ é™¤æ–‡ä»¶: {perf_file.file_path}")
                except Exception as file_error:
                    logger.warning(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {perf_file.file_path}, é”™è¯¯: {file_error}")
        
        # åˆ é™¤æºæ–‡æ¡£
        if performance.source_document and os.path.exists(performance.source_document):
            try:
                os.remove(performance.source_document)
                logger.info(f"å·²åˆ é™¤æºæ–‡æ¡£: {performance.source_document}")
            except Exception as source_error:
                logger.warning(f"åˆ é™¤æºæ–‡æ¡£å¤±è´¥: {performance.source_document}, é”™è¯¯: {source_error}")
        
        # åˆ é™¤æ•°æ®åº“è®°å½•
        db.delete(performance)
        db.commit()
        
        logger.info(f"âœ… ä¸šç»©è®°å½•åˆ é™¤æˆåŠŸ: ID={performance_id}")
        
        return {
            "success": True,
            "message": "ä¸šç»©è®°å½•å·²åˆ é™¤"
        }
        
    except Exception as e:
        db.rollback()
        logger.error(f"åˆ é™¤ä¸šç»©è®°å½•å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ é™¤å¤±è´¥: {str(e)}")

@router.post("/{performance_id}/delete")
async def deletePerformance(
    performance_id: int,
    db: Session = Depends(get_db)
):
    """åˆ é™¤ä¸šç»©è®°å½•ï¼ˆPOSTæ–¹æ³•ï¼Œä¾›å‰ç«¯è°ƒç”¨ï¼‰"""
    return await delete_performance(performance_id, db)

# ==================== é…ç½®ç®¡ç†API ====================

@router.get("/config/business-fields")
async def get_business_fields():
    """è·å–ä¸šåŠ¡é¢†åŸŸåˆ—è¡¨"""
    try:
        fields = config_manager.get_business_fields()
        return {
            "success": True,
            "business_fields": fields
        }
    except Exception as e:
        logger.error(f"è·å–ä¸šåŠ¡é¢†åŸŸå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–ä¸šåŠ¡é¢†åŸŸå¤±è´¥: {str(e)}")

@router.get("/config/performance-types")
async def get_performance_types():
    """è·å–ä¸šç»©ç±»å‹åˆ—è¡¨"""
    try:
        types = config_manager.get_performance_types()
        return {
            "success": True,
            "performance_types": types
        }
    except Exception as e:
        logger.error(f"è·å–ä¸šç»©ç±»å‹å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–ä¸šç»©ç±»å‹å¤±è´¥: {str(e)}")

@router.post("/config/reload")
async def reload_configs():
    """é‡æ–°åŠ è½½é…ç½®æ–‡ä»¶"""
    try:
        config_manager.reload_all_configs()
        return {
            "success": True,
            "message": "é…ç½®æ–‡ä»¶é‡æ–°åŠ è½½æˆåŠŸ"
        }
    except Exception as e:
        logger.error(f"é‡æ–°åŠ è½½é…ç½®å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"é‡æ–°åŠ è½½é…ç½®å¤±è´¥: {str(e)}")

@router.put("/config/business-fields")
async def update_business_fields(business_fields: List[Dict[str, Any]]):
    """æ›´æ–°ä¸šåŠ¡é¢†åŸŸé…ç½®"""
    try:
        config_manager.update_business_fields(business_fields)
        return {
            "success": True,
            "message": "ä¸šåŠ¡é¢†åŸŸé…ç½®æ›´æ–°æˆåŠŸ"
        }
    except Exception as e:
        logger.error(f"æ›´æ–°ä¸šåŠ¡é¢†åŸŸé…ç½®å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ›´æ–°ä¸šåŠ¡é¢†åŸŸé…ç½®å¤±è´¥: {str(e)}")

@router.put("/config/performance-types")
async def update_performance_types(performance_types: List[Dict[str, Any]]):
    """æ›´æ–°ä¸šç»©ç±»å‹é…ç½®"""
    try:
        config_manager.update_performance_types(performance_types)
        return {
            "success": True,
            "message": "ä¸šç»©ç±»å‹é…ç½®æ›´æ–°æˆåŠŸ"
        }
    except Exception as e:
        logger.error(f"æ›´æ–°ä¸šç»©ç±»å‹é…ç½®å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ›´æ–°ä¸šç»©ç±»å‹é…ç½®å¤±è´¥: {str(e)}")

# ==================== AIå­¦ä¹ API ====================

@router.post("/{performance_id}/verify-with-learning")
async def verify_performance_with_learning(
    performance_id: int,
    original_values: Dict[str, Any],
    corrected_values: Dict[str, Any],
    learning_notes: Optional[str] = None,
    db: Session = Depends(get_db)
):
    """éªŒè¯ä¸šç»©è®°å½•å¹¶è®°å½•AIå­¦ä¹ æ•°æ®"""
    try:
        performance = db.query(Performance).filter(Performance.id == performance_id).first()
        if not performance:
            raise HTTPException(status_code=404, detail="ä¸šç»©è®°å½•ä¸å­˜åœ¨")
        
        # è®°å½•ç”¨æˆ·åé¦ˆ
        feedback_data = {
            "type": "performance_correction",
            "performance_id": performance_id,
            "original_values": original_values,
            "corrected_values": corrected_values,
            "learning_notes": learning_notes,
            "source_document": performance.source_document,
            "ai_analysis_status": performance.ai_analysis_status,
            "confidence_score": performance.confidence_score
        }
        
        config_manager.add_user_feedback(feedback_data)
        
        # è®°å½•ä¿®æ­£æ¨¡å¼
        for field, original_value in original_values.items():
            corrected_value = corrected_values.get(field)
            if original_value != corrected_value:
                pattern_data = {
                    "field": field,
                    "original_value": original_value,
                    "corrected_value": corrected_value,
                    "performance_id": performance_id,
                    "context": {
                        "project_name": performance.project_name,
                        "client_name": performance.client_name,
                        "business_field": performance.business_field,
                        "project_type": performance.project_type
                    }
                }
                config_manager.add_correction_pattern(pattern_data)
        
        # æ›´æ–°ä¸šç»©è®°å½•
        for field, value in corrected_values.items():
            if hasattr(performance, field):
                setattr(performance, field, value)
        
        performance.is_verified = True
        performance.updated_at = datetime.now()
        
        db.commit()
        
        logger.info(f"ä¸šç»©è®°å½•éªŒè¯å®Œæˆå¹¶è®°å½•å­¦ä¹ æ•°æ®: ID={performance_id}")
        
        return {
            "success": True,
            "message": "ä¸šç»©è®°å½•éªŒè¯æˆåŠŸï¼ŒAIå­¦ä¹ æ•°æ®å·²è®°å½•",
            "learning_recorded": True
        }
        
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        logger.error(f"éªŒè¯ä¸šç»©è®°å½•å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"éªŒè¯ä¸šç»©è®°å½•å¤±è´¥: {str(e)}")

@router.get("/ai-learning/stats")
async def get_ai_learning_stats():
    """è·å–AIå­¦ä¹ ç»Ÿè®¡ä¿¡æ¯"""
    try:
        learning_data = config_manager.get_ai_learning_data()
        
        feedback_count = len(learning_data.get("user_feedback", []))
        pattern_count = len(learning_data.get("correction_patterns", []))
        
        # ç»Ÿè®¡ä¿®æ­£æ¨¡å¼
        field_corrections = {}
        for pattern in learning_data.get("correction_patterns", []):
            field = pattern.get("field", "unknown")
            field_corrections[field] = field_corrections.get(field, 0) + 1
        
        return {
            "success": True,
            "stats": {
                "total_feedback": feedback_count,
                "total_corrections": pattern_count,
                "field_corrections": field_corrections,
                "last_updated": learning_data.get("last_updated")
            }
        }
    except Exception as e:
        logger.error(f"è·å–AIå­¦ä¹ ç»Ÿè®¡å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–AIå­¦ä¹ ç»Ÿè®¡å¤±è´¥: {str(e)}")

@router.get("/ai-learning/patterns")
async def get_correction_patterns():
    """è·å–ä¿®æ­£æ¨¡å¼"""
    patterns = config_manager.get_correction_patterns()
    return {
        "success": True,
        "patterns": patterns
    }

# AIä»»åŠ¡ç®¡ç†API
@router.get("/tasks/{task_id}")
async def get_ai_task_status(
    task_id: int,
    db: Session = Depends(get_db)
):
    """è·å–AIä»»åŠ¡çŠ¶æ€"""
    from ai_service import get_ai_task_status
    
    task_status = get_ai_task_status(db, task_id)
    
    if "error" in task_status:
        raise HTTPException(status_code=404, detail=task_status["error"])
    
    return {
        "success": True,
        "task": task_status
    }

@router.get("/tasks")
async def list_ai_tasks(
    page: int = 1,
    page_size: int = 20,
    status: Optional[str] = None,
    db: Session = Depends(get_db)
):
    """è·å–AIä»»åŠ¡åˆ—è¡¨"""
    try:
        query = db.query(AITask).filter(AITask.file_type == "performance")
        
        if status:
            query = query.filter(AITask.status == status)
        
        total = query.count()
        offset = (page - 1) * page_size
        tasks = query.order_by(AITask.created_at.desc()).offset(offset).limit(page_size).all()
        
        return {
            "success": True,
            "tasks": [
                {
                    "id": task.id,
                    "file_id": task.file_id,
                    "file_type": task.file_type,
                    "status": task.status,
                    "result_snapshot": task.result_snapshot,
                    "error_message": task.error_message,
                    "created_at": task.created_at.isoformat() if task.created_at else None,
                    "updated_at": task.updated_at.isoformat() if task.updated_at else None
                }
                for task in tasks
            ],
            "pagination": {
                "page": page,
                "page_size": page_size,
                "total": total,
                "pages": (total + page_size - 1) // page_size
            }
        }
        
    except Exception as e:
        logger.error(f"è·å–AIä»»åŠ¡åˆ—è¡¨å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–AIä»»åŠ¡åˆ—è¡¨å¤±è´¥: {str(e)}") 