"""
ä¸šç»©ç®¡ç†API
"""
from fastapi import APIRouter, Depends, HTTPException, File, UploadFile, Form
from sqlalchemy.orm import Session
from sqlalchemy import func, desc
from typing import Optional, List, Dict, Any
import logging
import os
from datetime import datetime

from database import get_db
from models import Performance, PerformanceFile, SystemSettings, AITask
from schemas import PerformanceCreate, PerformanceUpdate, PerformanceResponse
from config_manager import config_manager
from ai_service import create_ai_task, update_ai_task

router = APIRouter()
logger = logging.getLogger(__name__)

@router.get("/stats")
async def get_performance_stats(db: Session = Depends(get_db)):
    """èŽ·å–ä¸šç»©ç»Ÿè®¡æ•°æ®"""
    try:
        total_performances = db.query(Performance).count()
        verified_performances = db.query(Performance).filter(Performance.is_verified == True).count()
        manual_input_performances = db.query(Performance).filter(Performance.is_manual_input == True).count()
        
        # æŒ‰å¹´ä»½ç»Ÿè®¡
        yearly_stats = db.query(
            Performance.year,
            func.count(Performance.id).label('count')
        ).group_by(Performance.year).order_by(desc(Performance.year)).limit(5).all()
        
        # æŒ‰ä¸šåŠ¡é¢†åŸŸç»Ÿè®¡
        field_stats = db.query(
            Performance.business_field,
            func.count(Performance.id).label('count')
        ).group_by(Performance.business_field).order_by(desc(func.count(Performance.id))).limit(5).all()
        
        return {
            "success": True,
            "stats": {
                "total_performances": total_performances,
                "verified_performances": verified_performances,
                "manual_input_performances": manual_input_performances,
                "yearly_distribution": [
                    {"year": stat.year, "count": stat.count} 
                    for stat in yearly_stats
                ],
                "field_distribution": [
                    {"field": stat.business_field, "count": stat.count} 
                    for stat in field_stats if stat.business_field
                ]
            }
        }
        
    except Exception as e:
        logger.error(f"èŽ·å–ä¸šç»©ç»Ÿè®¡å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"èŽ·å–ç»Ÿè®¡æ•°æ®å¤±è´¥: {str(e)}")

@router.get("/list")
async def list_performances(
    page: int = 1,
    page_size: int = 20,
    search: Optional[str] = None,
    business_field: Optional[str] = None,
    year: Optional[int] = None,
    is_verified: Optional[bool] = None,
    db: Session = Depends(get_db)
):
    """èŽ·å–ä¸šç»©åˆ—è¡¨"""
    try:
        query = db.query(Performance)
        
        # æœç´¢è¿‡æ»¤
        if search:
            query = query.filter(
                Performance.project_name.ilike(f'%{search}%') |
                Performance.client_name.ilike(f'%{search}%') |
                Performance.description.ilike(f'%{search}%')
            )
        
        # ä¸šåŠ¡é¢†åŸŸè¿‡æ»¤
        if business_field:
            query = query.filter(Performance.business_field == business_field)
        
        # å¹´ä»½è¿‡æ»¤
        if year:
            query = query.filter(Performance.year == year)
        
        # éªŒè¯çŠ¶æ€è¿‡æ»¤
        if is_verified is not None:
            query = query.filter(Performance.is_verified == is_verified)
        
        # æ€»æ•°
        total = query.count()
        
        # åˆ†é¡µ
        offset = (page - 1) * page_size
        performances = query.order_by(desc(Performance.created_at)).offset(offset).limit(page_size).all()
        
        return {
            "success": True,
            "performances": [
                {
                    "id": perf.id,
                    "project_name": perf.project_name,
                    "client_name": perf.client_name,
                    "project_type": perf.project_type,
                    "business_field": perf.business_field,
                    "year": perf.year,
                    "contract_amount": perf.contract_amount,
                    "currency": perf.currency,
                    "start_date": perf.start_date.isoformat() if perf.start_date else None,
                    "end_date": perf.end_date.isoformat() if perf.end_date else None,
                    "description": perf.description,
                    "confidence_score": perf.confidence_score,
                    "ai_analysis_status": getattr(perf, 'ai_analysis_status', 'pending'),  # å…¼å®¹æ—§æ•°æ®
                    "is_verified": perf.is_verified,
                    "is_manual_input": perf.is_manual_input,
                    "source_document": perf.source_document,
                    "created_at": perf.created_at.isoformat()
                }
                for perf in performances
            ],
            "pagination": {
                "page": page,
                "page_size": page_size,
                "total": total,
                "pages": (total + page_size - 1) // page_size
            }
        }
        
    except Exception as e:
        logger.error(f"èŽ·å–ä¸šç»©åˆ—è¡¨å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"èŽ·å–ä¸šç»©åˆ—è¡¨å¤±è´¥: {str(e)}")

@router.post("/create")
async def create_performance(
    performance_data: dict,
    db: Session = Depends(get_db)
):
    """æ‰‹åŠ¨åˆ›å»ºä¸šç»©è®°å½•"""
    try:
        # åˆ›å»ºä¸šç»©è®°å½•
        performance = Performance(
            project_name=performance_data.get("project_name"),
            client_name=performance_data.get("client_name"),
            project_type=performance_data.get("project_type"),
            business_field=performance_data.get("business_field"),
            year=int(performance_data.get("year")),
            contract_amount=performance_data.get("contract_amount"),
            currency=performance_data.get("currency", "CNY"),
            start_date=datetime.fromisoformat(performance_data["start_date"]) if performance_data.get("start_date") else None,
            end_date=datetime.fromisoformat(performance_data["end_date"]) if performance_data.get("end_date") else None,
            description=performance_data.get("description"),
            is_manual_input=True,
            is_verified=True  # æ‰‹åŠ¨åˆ›å»ºçš„é»˜è®¤ä¸ºå·²éªŒè¯
        )
        
        db.add(performance)
        db.commit()
        db.refresh(performance)
        
        logger.info(f"æ‰‹åŠ¨åˆ›å»ºä¸šç»©è®°å½•æˆåŠŸ: {performance.project_name}")
        
        return {
            "success": True,
            "message": "ä¸šç»©è®°å½•åˆ›å»ºæˆåŠŸ",
            "performance_id": performance.id
        }
        
    except Exception as e:
        db.rollback()
        logger.error(f"åˆ›å»ºä¸šç»©è®°å½•å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºä¸šç»©è®°å½•å¤±è´¥: {str(e)}")

@router.post("/upload")
async def upload_performance_files(
    files: List[UploadFile] = File(...),
    project_name: Optional[str] = Form(None),
    client_name: Optional[str] = Form(None),
    business_field: Optional[str] = Form(None),
    enable_ai_analysis: bool = Form(True),
    enable_vision_analysis: bool = Form(True),
    db: Session = Depends(get_db)
):
    """ä¸Šä¼ ä¸šç»©æ–‡ä»¶ï¼ˆæ”¯æŒå¤šæ–‡ä»¶ï¼‰å¹¶ä¸ºæ¯ä¸ªæ–‡ä»¶åˆ›å»ºç‹¬ç«‹çš„ä¸šç»©è®°å½•"""
    uploaded_performances = []
    uploaded_files = []
    
    try:
        # ä¿å­˜æ–‡ä»¶ç›®å½•
        upload_dir = os.path.join(os.path.dirname(__file__), "uploads", "performances")
        os.makedirs(upload_dir, exist_ok=True)
        
        # ä¸ºæ¯ä¸ªæ–‡ä»¶åˆ›å»ºç‹¬ç«‹çš„ä¸šç»©è®°å½•
        for i, file in enumerate(files):
            # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            file_path = os.path.join(upload_dir, f"{timestamp}_{i}_{file.filename}")
            
            # ä¿å­˜æ–‡ä»¶
            with open(file_path, "wb") as buffer:
                content = await file.read()
                buffer.write(content)
            
            logger.info(f"âœ… ä¸šç»©æ–‡ä»¶ä¿å­˜æˆåŠŸ: {file_path}")
            
            # ä¸ºæ¯ä¸ªæ–‡ä»¶åˆ›å»ºç‹¬ç«‹çš„ä¸šç»©è®°å½•
            # ä½¿ç”¨åŽ»é™¤æ‰©å±•åçš„æ–‡ä»¶åä½œä¸ºä¸´æ—¶é¡¹ç›®åç§°
            temp_project_name = os.path.splitext(file.filename)[0] if not project_name else f"{project_name}_{i+1}"
            
            performance = Performance(
                project_name=temp_project_name,
                client_name=client_name or "å¾…AIåˆ†æž",
                project_type="é‡å¤§ä¸ªæ¡ˆ(éžè¯‰)",  # ä½¿ç”¨æ–°çš„é¡¹ç›®ç±»åž‹åç§°
                business_field=business_field or "å¾…AIåˆ†æž",
                year=datetime.now().year,
                is_manual_input=False,
                is_verified=False,
                confidence_score=0.0,
                description=f"æ–‡ä»¶ '{file.filename}' å·²ä¸Šä¼ ï¼Œç­‰å¾…AIåˆ†æž...",
                source_document=file_path,
                ai_analysis_status="pending"  # æ·»åŠ åˆ†æžçŠ¶æ€æ ‡è®°
            )
            
            db.add(performance)
            db.commit()  # ç«‹å³æäº¤æ¯ä¸ªè®°å½•
            db.refresh(performance)
            
            # åˆ›å»ºæ–‡ä»¶è®°å½•
            performance_file = PerformanceFile(
                performance_id=performance.id,
                file_path=file_path,
                file_type="contract" if "åˆåŒ" in file.filename or "contract" in file.filename.lower() else "supporting_doc",
                file_name=file.filename,
                file_size=len(content)
            )
            
            db.add(performance_file)
            db.commit()
            
            # åˆ›å»ºAIåˆ†æžä»»åŠ¡
            task_id = None
            if enable_ai_analysis:
                task_id = create_ai_task(db, performance.id, "performance")
                logger.info(f"ðŸ“‹ AIä»»åŠ¡å·²åˆ›å»º: ä»»åŠ¡ID={task_id}, ä¸šç»©ID={performance.id}")
            
            uploaded_performances.append({
                "id": performance.id,
                "project_name": performance.project_name,
                "file_name": file.filename,
                "ai_analysis_status": "pending",
                "task_id": task_id  # è¿”å›žä»»åŠ¡ID
            })
            uploaded_files.append(file.filename)
            
            logger.info(f"âœ… ä¸šç»©è®°å½•å·²åˆ›å»º: ID={performance.id}, é¡¹ç›®åç§°={performance.project_name}")
        
    except Exception as e:
        logger.error(f"âŒ æ–‡ä»¶ä¿å­˜å¤±è´¥: {str(e)}")
        db.rollback()
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}")
    
    # å¼‚æ­¥è¿›è¡ŒAIåˆ†æžï¼ˆåœ¨åŽå°ä¸ºæ¯ä¸ªè®°å½•åˆ†åˆ«å¤„ç†ï¼‰
    if enable_ai_analysis:
        # å¯åŠ¨åŽå°ä»»åŠ¡è¿›è¡ŒAIåˆ†æž
        import asyncio
        asyncio.create_task(
            analyze_performances_in_background(
                [perf["id"] for perf in uploaded_performances],
                enable_vision_analysis,
                db
            )
        )
        
        logger.info(f"ðŸ¤– å·²å¯åŠ¨åŽå°AIåˆ†æžä»»åŠ¡ï¼Œå…±{len(uploaded_performances)}ä¸ªæ–‡ä»¶")
    
    # ç«‹å³è¿”å›žç»“æžœï¼Œä¸ç­‰å¾…AIåˆ†æžå®Œæˆ
    return {
        "success": True,
        "message": f"æˆåŠŸä¸Šä¼  {len(files)} ä¸ªæ–‡ä»¶ï¼Œå·²åˆ›å»º {len(uploaded_performances)} æ¡ä¸šç»©è®°å½•",
        "performances": uploaded_performances,
        "uploaded_files": uploaded_files,
        "ai_analysis": {
            "enabled": enable_ai_analysis,
            "status": "background_processing" if enable_ai_analysis else "disabled",
            "total_files": len(files)
        }
    }

async def analyze_performances_in_background(performance_ids: List[int], enable_vision_analysis: bool, db: Session):
    """åŽå°å¼‚æ­¥åˆ†æžä¸šç»©è®°å½•"""
    try:
        from ai_service import ai_service
        
        if not ai_service.enable_ai:
            logger.warning("AIæœåŠ¡æœªå¯ç”¨ï¼Œè·³è¿‡åŽå°åˆ†æž")
            return
        
        logger.info(f"ðŸ¤– å¼€å§‹åŽå°AIåˆ†æžï¼Œå…±{len(performance_ids)}æ¡è®°å½•")
        
        for performance_id in performance_ids:
            task_id = None
            try:
                # é‡æ–°èŽ·å–performanceè®°å½•
                performance = db.query(Performance).filter(Performance.id == performance_id).first()
                
                if not performance or not performance.source_document:
                    logger.warning(f"âš ï¸ ä¸šç»©è®°å½•æˆ–æ–‡ä»¶ä¸å­˜åœ¨: {performance_id}")
                    continue
                
                # æŸ¥æ‰¾å¯¹åº”çš„AIä»»åŠ¡
                task = db.query(AITask).filter(
                    AITask.file_id == performance_id,
                    AITask.file_type == "performance"
                ).first()
                
                if task:
                    task_id = task.id
                    # æ›´æ–°AIä»»åŠ¡çŠ¶æ€ä¸ºprocessing
                    update_ai_task(db, task_id, "processing")
                
                # æ›´æ–°çŠ¶æ€ä¸ºæ­£åœ¨åˆ†æž
                performance.ai_analysis_status = "analyzing"
                performance.description = f"æ­£åœ¨è¿›è¡ŒAIåˆ†æž..."
                db.commit()
                
                logger.info(f"ðŸ” å¼€å§‹åˆ†æžä¸šç»©è®°å½• {performance_id}: {performance.project_name}")
                
                # è¿›è¡ŒAIåˆ†æž
                ai_result = await ai_service.smart_document_analysis(
                    performance.source_document,
                    enable_vision=enable_vision_analysis,
                    enable_ocr=True
                )
                
                if ai_result and ai_result.get("success"):
                    # æå–ä¸šç»©ä¿¡æ¯
                    extracted_info = _extract_performance_info(ai_result)
                    
                    # æ›´æ–°ä¸šç»©è®°å½•ï¼ˆåªåœ¨AIæå–åˆ°æœ‰æ•ˆä¿¡æ¯æ—¶æ›´æ–°ï¼‰
                    updated_fields = []
                    
                    # å¦‚æžœAIæå–åˆ°äº†æœ‰æ„ä¹‰çš„é¡¹ç›®åç§°ï¼Œåˆ™æ›´æ–°
                    if extracted_info.get('project_name') and len(extracted_info['project_name']) > 3:
                        performance.project_name = extracted_info['project_name']
                        updated_fields.append("project_name")
                    
                    if extracted_info.get('client_name') and performance.client_name == "å¾…AIåˆ†æž":
                        performance.client_name = extracted_info['client_name']
                        updated_fields.append("client_name")
                    
                    if extracted_info.get('business_field') and performance.business_field == "å¾…AIåˆ†æž":
                        performance.business_field = extracted_info['business_field']
                        updated_fields.append("business_field")
                    
                    if extracted_info.get('contract_amount'):
                        performance.contract_amount = extracted_info['contract_amount']
                        updated_fields.append("contract_amount")
                    
                    if extracted_info.get('year'):
                        performance.year = extracted_info['year']
                        updated_fields.append("year")
                    
                    # æ›´æ–°åˆ†æžç»“æžœ
                    performance.description = extracted_info.get('description', "AIåˆ†æžå®Œæˆ")
                    performance.confidence_score = ai_result.get("results", {}).get("final_classification", {}).get("confidence", 0.0)
                    performance.ai_analysis_status = "completed"
                    
                    # ä¿å­˜AIåˆ†æžç»“æžœ
                    performance.ai_analysis = {
                        "analysis_result": ai_result,
                        "extracted_info": extracted_info,
                        "updated_fields": updated_fields,
                        "analysis_timestamp": datetime.now().isoformat()
                    }
                    
                    db.commit()
                    
                    # æ›´æ–°AIä»»åŠ¡çŠ¶æ€ä¸ºæˆåŠŸ
                    if task_id:
                        update_ai_task(db, task_id, "success", result={
                            "analysis_result": ai_result,
                            "extracted_info": extracted_info,
                            "updated_fields": updated_fields,
                            "confidence_score": performance.confidence_score
                        })
                    
                    logger.info(f"âœ… AIåˆ†æžå®Œæˆ: è®°å½•{performance_id}, æ›´æ–°å­—æ®µ: {updated_fields}")
                
                else:
                    # AIåˆ†æžå¤±è´¥
                    performance.ai_analysis_status = "failed"
                    performance.description = "AIåˆ†æžå¤±è´¥ï¼Œè¯·æ‰‹åŠ¨ç¼–è¾‘"
                    db.commit()
                    
                    # æ›´æ–°AIä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
                    if task_id:
                        update_ai_task(db, task_id, "failed", error="AIåˆ†æžæœªè¿”å›žæˆåŠŸç»“æžœ")
                    
                    logger.warning(f"âš ï¸ AIåˆ†æžå¤±è´¥: è®°å½•{performance_id}")
                
            except Exception as single_error:
                logger.error(f"âŒ å•ä¸ªè®°å½•åˆ†æžå¤±è´¥ {performance_id}: {str(single_error)}")
                
                # æ ‡è®°åˆ†æžå¤±è´¥
                try:
                    performance = db.query(Performance).filter(Performance.id == performance_id).first()
                    if performance:
                        performance.ai_analysis_status = "failed"
                        performance.description = f"AIåˆ†æžå‡ºé”™: {str(single_error)}"
                        db.commit()
                    
                    # æ›´æ–°AIä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
                    if task_id:
                        update_ai_task(db, task_id, "failed", error=str(single_error))
                        
                except:
                    pass
        
        logger.info(f"ðŸŽ‰ åŽå°AIåˆ†æžä»»åŠ¡å®Œæˆï¼Œå…±å¤„ç†{len(performance_ids)}æ¡è®°å½•")
        
    except Exception as e:
        logger.error(f"âŒ åŽå°AIåˆ†æžä»»åŠ¡å¤±è´¥: {str(e)}")

def _extract_performance_info(ai_result):
    """ä»ŽAIåˆ†æžç»“æžœä¸­æå–ä¸šç»©ä¿¡æ¯å¹¶ç”Ÿæˆè§„èŒƒåŒ–é¡¹ç›®åç§°"""
    extracted_info = {}
    
    if not ai_result:
        return extracted_info
    
    # ä»Žæ–°çš„AIåˆ†æžç»“æžœç»“æž„ä¸­æå–æ–‡æœ¬å†…å®¹
    text_content = ""
    
    # æ–°ç»“æž„ï¼šai_result.results.text_extraction_result.text
    if ai_result.get("results", {}).get("text_extraction_result", {}).get("text"):
        text_content = ai_result["results"]["text_extraction_result"]["text"]
    # å…¼å®¹æ—§ç»“æž„ï¼šai_result.text_extraction_result.text
    elif ai_result.get('text_extraction_result', {}).get('text'):
        text_content = ai_result['text_extraction_result']['text']
    
    # æ·»åŠ è°ƒè¯•æ—¥å¿—
    logger.info(f"æå–çš„æ–‡æœ¬å†…å®¹é•¿åº¦: {len(text_content)} å­—ç¬¦")
    if text_content:
        logger.info(f"æ–‡æœ¬å†…å®¹é¢„è§ˆ: {text_content[:200]}...")
    else:
        logger.warning("æ²¡æœ‰æ‰¾åˆ°æ–‡æœ¬å†…å®¹")
    
    # ä»ŽOCRç»“æžœä¸­å¯»æ‰¾å…³é”®ä¿¡æ¯ï¼ˆå…¼å®¹å¤šç§ç»“æž„ï¼‰
    ocr_text = ""
    if ai_result.get("results", {}).get("ocr_result", {}).get("text"):
        ocr_text = ai_result["results"]["ocr_result"]["text"]
    elif ai_result.get('ocr_result', {}).get('text'):
        ocr_text = ai_result['ocr_result']['text']
    
    # ä»ŽAIæ–‡æœ¬åˆ†æžç»“æžœä¸­æå–ç»“æž„åŒ–ä¿¡æ¯
    ai_text_analysis = ai_result.get("results", {}).get("ai_text_analysis", {})
    if ai_text_analysis and ai_text_analysis.get("key_entities"):
        entities = ai_text_analysis["key_entities"]
        if entities.get("client_name"):
            extracted_info['client_name'] = entities["client_name"]
        if entities.get("amount"):
            try:
                amount_str = str(entities["amount"]).replace(',', '').replace('ï¼Œ', '')
                extracted_info['contract_amount'] = float(amount_str)
            except:
                pass
        if entities.get("date_issued"):
            # å°è¯•ä»Žæ—¥æœŸä¸­æå–å¹´ä»½
            try:
                import re
                year_match = re.search(r'(\d{4})', str(entities["date_issued"]))
                if year_match:
                    extracted_info['year'] = int(year_match.group(1))
            except:
                pass
    
    # ä»Žæœ€ç»ˆåˆ†ç±»ç»“æžœä¸­æå–ä¿¡æ¯
    final_classification = ai_result.get("results", {}).get("final_classification", {})
    if final_classification:
        if final_classification.get("business_field"):
            extracted_info['business_field'] = final_classification["business_field"]
        if final_classification.get("description"):
            extracted_info['description'] = final_classification["description"]
    
    # åˆå¹¶æ‰€æœ‰æ–‡æœ¬å†…å®¹è¿›è¡Œå…³é”®è¯åŒ¹é…
    full_text = f"{text_content}\n{ocr_text}".lower()
    
    if not full_text.strip():
        logger.warning("åˆå¹¶åŽçš„æ–‡æœ¬å†…å®¹ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œå…³é”®è¯åŒ¹é…")
        return extracted_info
    
    logger.info(f"åˆå¹¶åŽçš„æ–‡æœ¬é•¿åº¦: {len(full_text)} å­—ç¬¦")
    
    # ä½¿ç”¨å…³é”®è¯åŒ¹é…æå–ä¿¡æ¯ï¼ˆä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆï¼‰
    import re
    
    # æå–å®¢æˆ·åç§°
    client_name = ""
    if not extracted_info.get('client_name'):
        client_patterns = [
            r'ç”²æ–¹[ï¼š:]\s*([^\n\rï¼Œã€‚ï¼›]+)',
            r'å§”æ‰˜æ–¹[ï¼š:]\s*([^\n\rï¼Œã€‚ï¼›]+)', 
            r'å®¢æˆ·[ï¼š:]\s*([^\n\rï¼Œã€‚ï¼›]+)',
            r'å§”æ‰˜äºº[ï¼š:]\s*([^\n\rï¼Œã€‚ï¼›]+)',
            r'å½“äº‹äºº[ï¼š:]\s*([^\n\rï¼Œã€‚ï¼›]+)',
            r'ç”³è¯·äºº[ï¼š:]\s*([^\n\rï¼Œã€‚ï¼›]+)'
        ]
        for pattern in client_patterns:
            match = re.search(pattern, full_text)
            if match:
                client_name = match.group(1).strip()
                # æ¸…ç†å®¢æˆ·åç§°ï¼Œç§»é™¤å¤šä½™å†…å®¹
                client_name = re.sub(r'ï¼ˆ.*?ï¼‰|\(.*?\)', '', client_name)  # ç§»é™¤æ‹¬å·å†…å®¹
                client_name = re.sub(r'æ³•å®šä»£è¡¨äºº.*|ç»Ÿä¸€ç¤¾ä¼šä¿¡ç”¨ä»£ç .*|åœ°å€.*', '', client_name)  # ç§»é™¤æ— å…³ä¿¡æ¯
                client_name = client_name.strip()
                if len(client_name) > 2 and len(client_name) < 50:  # åˆç†çš„é•¿åº¦èŒƒå›´
                    extracted_info['client_name'] = client_name
                    logger.info(f"æå–åˆ°å®¢æˆ·åç§°: {client_name}")
                    break
    
    # æå–é‡‘é¢
    if not extracted_info.get('contract_amount'):
        amount_patterns = [
            r'åˆåŒé‡‘é¢[ï¼š:]?\s*([0-9,ï¼Œ.]+)\s*ä¸‡?å…ƒ',
            r'æ€»é‡‘é¢[ï¼š:]?\s*([0-9,ï¼Œ.]+)\s*ä¸‡?å…ƒ',
            r'è´¹ç”¨[ï¼š:]?\s*([0-9,ï¼Œ.]+)\s*ä¸‡?å…ƒ',
            r'å¾‹å¸ˆè´¹[ï¼š:]?\s*([0-9,ï¼Œ.]+)\s*ä¸‡?å…ƒ',
            r'æœåŠ¡è´¹[ï¼š:]?\s*([0-9,ï¼Œ.]+)\s*ä¸‡?å…ƒ'
        ]
        for pattern in amount_patterns:
            match = re.search(pattern, full_text)
            if match:
                try:
                    amount_str = match.group(1).replace(',', '').replace('ï¼Œ', '')
                    extracted_info['contract_amount'] = float(amount_str)
                    logger.info(f"æå–åˆ°åˆåŒé‡‘é¢: {amount_str}")
                    break
                except:
                    pass
    
    # æå–å¹´ä»½
    current_year = None
    if not extracted_info.get('year'):
        year_patterns = [
            r'(\d{4})\s*å¹´',
            r'ç­¾è®¢æ—¶é—´[ï¼š:]\s*(\d{4})',
            r'åˆåŒæ—¥æœŸ[ï¼š:]\s*(\d{4})',
            r'(\d{4})[å¹´/-]\d{1,2}[æœˆ/-]\d{1,2}'
        ]
        for pattern in year_patterns:
            match = re.search(pattern, full_text)
            if match:
                try:
                    year = int(match.group(1))
                    if 2000 <= year <= 2030:  # åˆç†çš„å¹´ä»½èŒƒå›´
                        extracted_info['year'] = year
                        current_year = year
                        logger.info(f"æå–åˆ°å¹´ä»½: {year}")
                        break
                except:
                    pass
    else:
        current_year = extracted_info['year']
    
    # æå–ä¸šåŠ¡é¢†åŸŸå’Œé¡¹ç›®ç±»åž‹
    business_field = extracted_info.get('business_field', '')
    project_type = ""
    case_cause = ""  # æ¡ˆç”±ï¼Œç”¨äºŽè¯‰è®¼æ¡ˆä»¶
    project_description = ""  # é¡¹ç›®æè¿°ï¼Œç”¨äºŽéžè¯‰é¡¹ç›®
    
    # åˆ¤æ–­é¡¹ç›®ç±»åž‹å¹¶æå–ç›¸å…³ä¿¡æ¯
    if 'è¯‰è®¼' in full_text or 'ä»²è£' in full_text or 'çº çº·' in full_text or 'äº‰è®®' in full_text:
        project_type = "è¯‰è®¼ä»²è£"
        # æå–æ¡ˆç”±
        case_patterns = [
            r'æ¡ˆç”±[ï¼š:]\s*([^\n\rï¼Œã€‚ï¼›]+)',
            r'çº çº·ç±»åž‹[ï¼š:]\s*([^\n\rï¼Œã€‚ï¼›]+)',
            r'äº‰è®®äº‹é¡¹[ï¼š:]\s*([^\n\rï¼Œã€‚ï¼›]+)',
            r'([^\n\r]*?çº çº·)',
            r'([^\n\r]*?äº‰è®®)',
            r'([^\n\r]*?ä¾µæƒ)',
            r'([^\n\r]*?è¿çº¦)'
        ]
        for pattern in case_patterns:
            match = re.search(pattern, full_text)
            if match:
                case_cause = match.group(1).strip()
                if len(case_cause) > 2 and len(case_cause) < 30:
                    logger.info(f"æå–åˆ°æ¡ˆç”±: {case_cause}")
                    break
        
        if not case_cause:
            # æ ¹æ®ä¸šåŠ¡é¢†åŸŸæŽ¨æ–­æ¡ˆç”±
            if business_field == "çŸ¥è¯†äº§æƒ":
                case_cause = "çŸ¥è¯†äº§æƒçº çº·"
            elif business_field == "åˆåŒçº çº·":
                case_cause = "åˆåŒçº çº·"
            else:
                case_cause = "å•†äº‹çº çº·"
        
        if not business_field:
            business_field = "äº‰è®®è§£å†³"
            extracted_info['business_field'] = business_field
            
    elif 'å¸¸å¹´' in full_text or 'é¡¾é—®' in full_text:
        project_type = "å¸¸å¹´æ³•å¾‹é¡¾é—®"
        if not business_field:
            business_field = "åˆè§„ç›‘ç®¡"
            extracted_info['business_field'] = business_field
    else:
        project_type = "é‡å¤§ä¸ªæ¡ˆ(éžè¯‰)"
        # æå–é¡¹ç›®æè¿°
        project_patterns = [
            r'é¡¹ç›®åç§°[ï¼š:]\s*([^\n\rï¼Œã€‚ï¼›]+)',
            r'æœåŠ¡å†…å®¹[ï¼š:]\s*([^\n\rï¼Œã€‚ï¼›]+)',
            r'å§”æ‰˜äº‹é¡¹[ï¼š:]\s*([^\n\rï¼Œã€‚ï¼›]+)',
            r'ä¸šåŠ¡èŒƒå›´[ï¼š:]\s*([^\n\rï¼Œã€‚ï¼›]+)'
        ]
        for pattern in project_patterns:
            match = re.search(pattern, full_text)
            if match:
                project_description = match.group(1).strip()
                # æ¸…ç†é¡¹ç›®æè¿°ï¼Œç§»é™¤å¤šä½™çš„"é¡¹ç›®"å­—æ ·
                if project_description.endswith('é¡¹ç›®'):
                    project_description = project_description[:-2]
                if len(project_description) > 3 and len(project_description) < 50:
                    logger.info(f"æå–åˆ°é¡¹ç›®æè¿°: {project_description}")
                    break
        
        if not project_description:
            # æ ¹æ®ä¸šåŠ¡é¢†åŸŸç”Ÿæˆé»˜è®¤é¡¹ç›®æè¿°
            if business_field == "å¹¶è´­é‡ç»„":
                project_description = "å¹¶è´­é‡ç»„"
            elif business_field == "èµ„æœ¬å¸‚åœº":
                project_description = "èµ„æœ¬å¸‚åœº"
            elif business_field == "é“¶è¡Œé‡‘èž":
                project_description = "é“¶å›¢è´·æ¬¾"
            elif business_field == "æˆ¿åœ°äº§":
                project_description = "æˆ¿åœ°äº§é¡¹ç›®"
            elif business_field == "çŸ¥è¯†äº§æƒ":
                project_description = "çŸ¥è¯†äº§æƒ"
            else:
                project_description = "æ³•å¾‹æœåŠ¡"
        
        # å¦‚æžœæ²¡æœ‰æå–åˆ°ä¸šåŠ¡é¢†åŸŸï¼Œå°è¯•ä»Žæ–‡æœ¬æŽ¨æ–­
        if not business_field:
            if 'å¹¶è´­' in full_text or 'æ”¶è´­' in full_text or 'M&A' in full_text.upper():
                business_field = 'å¹¶è´­é‡ç»„'
            elif 'IPO' in full_text.upper() or 'ä¸Šå¸‚' in full_text or 'å‘è¡Œ' in full_text:
                business_field = 'èµ„æœ¬å¸‚åœº'
            elif 'ç ´äº§' in full_text or 'é‡æ•´' in full_text:
                business_field = 'ç ´äº§é‡æ•´'
            elif 'é‡‘èž' in full_text or 'é“¶è¡Œ' in full_text or 'è´·æ¬¾' in full_text:
                business_field = 'é“¶è¡Œé‡‘èž'
            elif 'æˆ¿åœ°äº§' in full_text or 'åœŸåœ°' in full_text:
                business_field = 'æˆ¿åœ°äº§'
            elif 'çŸ¥è¯†äº§æƒ' in full_text or 'ä¸“åˆ©' in full_text or 'å•†æ ‡' in full_text:
                business_field = 'çŸ¥è¯†äº§æƒ'
            else:
                business_field = 'åˆè§„ç›‘ç®¡'
            
            extracted_info['business_field'] = business_field
    
    # ç”Ÿæˆè§„èŒƒåŒ–é¡¹ç›®åç§°
    client_name = extracted_info.get('client_name', '').strip()
    
    if client_name and project_type:
        if project_type == "è¯‰è®¼ä»²è£":
            # æ ¼å¼ï¼šä»£è¡¨xxxxçš„xxxx(æ¡ˆç”±)çº çº·
            standardized_name = f"ä»£è¡¨{client_name}çš„{case_cause}"
            if not case_cause.endswith('çº çº·'):
                standardized_name += "çº çº·"
        elif project_type == "å¸¸å¹´æ³•å¾‹é¡¾é—®":
            # æ ¼å¼ï¼šxxxxå¸¸å¹´æ³•å¾‹é¡¾é—®(xxxxå¹´åº¦)
            year_suffix = f"({current_year}å¹´åº¦)" if current_year else ""
            standardized_name = f"{client_name}å¸¸å¹´æ³•å¾‹é¡¾é—®{year_suffix}"
        else:  # é‡å¤§ä¸ªæ¡ˆ(éžè¯‰)
            # æ ¼å¼ï¼šä»£è¡¨xxxxçš„xxxxxxé¡¹ç›®
            standardized_name = f"ä»£è¡¨{client_name}çš„{project_description}é¡¹ç›®"
        
        extracted_info['project_name'] = standardized_name
        extracted_info['project_type'] = project_type
        logger.info(f"ç”Ÿæˆè§„èŒƒåŒ–é¡¹ç›®åç§°: {standardized_name}")
        logger.info(f"ç¡®å®šé¡¹ç›®ç±»åž‹: {project_type}")
    
    # ç”Ÿæˆé¡¹ç›®æè¿°
    if not extracted_info.get('description'):
        if project_type == "è¯‰è®¼ä»²è£":
            description = f"ä»£è¡¨{client_name}å¤„ç†{case_cause}æ¡ˆä»¶ï¼Œæä¾›ä¸“ä¸šçš„è¯‰è®¼ä»£ç†æœåŠ¡"
        elif project_type == "å¸¸å¹´æ³•å¾‹é¡¾é—®":
            description = f"ä¸º{client_name}æä¾›{current_year or ''}å¹´åº¦å¸¸å¹´æ³•å¾‹é¡¾é—®æœåŠ¡ï¼Œæ¶µç›–æ—¥å¸¸æ³•å¾‹äº‹åŠ¡å’¨è¯¢å’Œé£Žé™©é˜²æŽ§"
        else:
            description = f"ä»£è¡¨{client_name}çš„{project_description}é¡¹ç›®ï¼Œåœ¨{business_field}é¢†åŸŸæä¾›ä¸“ä¸šæ³•å¾‹æœåŠ¡"
        
        extracted_info['description'] = description
        logger.info(f"ç”Ÿæˆé¡¹ç›®æè¿°: {description}")
    
    logger.info(f"æœ€ç»ˆæå–çš„ä¸šç»©ä¿¡æ¯: {extracted_info}")
    return extracted_info

@router.patch("/{performance_id}")
async def update_performance(
    performance_id: int,
    performance_data: dict,
    db: Session = Depends(get_db)
):
    """æ›´æ–°ä¸šç»©è®°å½•"""
    try:
        performance = db.query(Performance).filter(Performance.id == performance_id).first()
        if not performance:
            raise HTTPException(status_code=404, detail="ä¸šç»©è®°å½•ä¸å­˜åœ¨")
        
        # æ›´æ–°å­—æ®µ
        updateable_fields = [
            'project_name', 'client_name', 'project_type', 'business_field',
            'year', 'contract_amount', 'currency', 'start_date', 'end_date',
            'description', 'is_manual_input'
        ]
        
        for field in updateable_fields:
            if field in performance_data:
                if field in ['start_date', 'end_date'] and performance_data[field]:
                    # å¤„ç†æ—¥æœŸå­—æ®µ
                    setattr(performance, field, datetime.fromisoformat(performance_data[field]))
                elif field == 'year' and performance_data[field]:
                    # å¤„ç†å¹´ä»½å­—æ®µ
                    setattr(performance, field, int(performance_data[field]))
                else:
                    setattr(performance, field, performance_data[field])
        
        performance.updated_at = datetime.now()
        db.commit()
        
        logger.info(f"ä¸šç»©è®°å½•æ›´æ–°æˆåŠŸ: ID={performance_id}")
        
        return {
            "success": True,
            "message": "ä¸šç»©è®°å½•æ›´æ–°æˆåŠŸ"
        }
        
    except Exception as e:
        db.rollback()
        logger.error(f"æ›´æ–°ä¸šç»©è®°å½•å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ›´æ–°ä¸šç»©è®°å½•å¤±è´¥: {str(e)}")

@router.patch("/{performance_id}/verify")
async def verify_performance(
    performance_id: int,
    verification_notes: Optional[str] = None,
    db: Session = Depends(get_db)
):
    """ç”¨æˆ·ä¸»åŠ¨éªŒè¯ä¸šç»©è®°å½•"""
    try:
        performance = db.query(Performance).filter(Performance.id == performance_id).first()
        if not performance:
            raise HTTPException(status_code=404, detail="ä¸šç»©è®°å½•ä¸å­˜åœ¨")
        
        # è®°å½•éªŒè¯åŽ†å²
        if not performance.verification_history:
            performance.verification_history = []
        
        verification_record = {
            "timestamp": datetime.now().isoformat(),
            "verified_by": "user",  # å¯ä»¥æ‰©å±•ä¸ºå…·ä½“ç”¨æˆ·ID
            "verification_notes": verification_notes,
            "previous_status": performance.is_verified,
            "ai_confidence_score": performance.confidence_score,
            "ai_analysis_status": performance.ai_analysis_status
        }
        
        performance.verification_history.append(verification_record)
        performance.is_verified = True
        performance.verified_at = datetime.now()
        performance.verification_notes = verification_notes
        
        db.commit()
        
        logger.info(f"ä¸šç»©è®°å½•éªŒè¯æˆåŠŸ: ID={performance_id}, éªŒè¯å¤‡æ³¨={verification_notes}")
        
        return {
            "success": True,
            "message": "ä¸šç»©è®°å½•éªŒè¯æˆåŠŸ",
            "verification_record": verification_record
        }
        
    except Exception as e:
        db.rollback()
        logger.error(f"éªŒè¯ä¸šç»©è®°å½•å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"éªŒè¯å¤±è´¥: {str(e)}")

@router.patch("/{performance_id}/unverify")
async def unverify_performance(
    performance_id: int,
    unverification_reason: Optional[str] = None,
    db: Session = Depends(get_db)
):
    """å–æ¶ˆéªŒè¯ä¸šç»©è®°å½•ï¼ˆé‡æ–°éªŒè¯ï¼‰"""
    try:
        performance = db.query(Performance).filter(Performance.id == performance_id).first()
        if not performance:
            raise HTTPException(status_code=404, detail="ä¸šç»©è®°å½•ä¸å­˜åœ¨")
        
        # è®°å½•å–æ¶ˆéªŒè¯åŽ†å²
        if not performance.verification_history:
            performance.verification_history = []
        
        unverification_record = {
            "timestamp": datetime.now().isoformat(),
            "action": "unverify",
            "unverified_by": "user",
            "unverification_reason": unverification_reason,
            "previous_status": performance.is_verified,
            "ai_confidence_score": performance.confidence_score,
            "ai_analysis_status": performance.ai_analysis_status
        }
        
        performance.verification_history.append(unverification_record)
        performance.is_verified = False
        performance.verified_at = None
        performance.verification_notes = None
        
        db.commit()
        
        logger.info(f"ä¸šç»©è®°å½•å–æ¶ˆéªŒè¯æˆåŠŸ: ID={performance_id}, åŽŸå› ={unverification_reason}")
        
        return {
            "success": True,
            "message": "ä¸šç»©è®°å½•å·²å–æ¶ˆéªŒè¯ï¼Œå¯ä»¥é‡æ–°éªŒè¯",
            "unverification_record": unverification_record
        }
        
    except Exception as e:
        db.rollback()
        logger.error(f"å–æ¶ˆéªŒè¯ä¸šç»©è®°å½•å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"å–æ¶ˆéªŒè¯å¤±è´¥: {str(e)}")

@router.get("/{performance_id}/verification-history")
async def get_verification_history(
    performance_id: int,
    db: Session = Depends(get_db)
):
    """èŽ·å–ä¸šç»©è®°å½•çš„éªŒè¯åŽ†å²"""
    try:
        performance = db.query(Performance).filter(Performance.id == performance_id).first()
        if not performance:
            raise HTTPException(status_code=404, detail="ä¸šç»©è®°å½•ä¸å­˜åœ¨")
        
        return {
            "success": True,
            "verification_history": performance.verification_history or [],
            "current_status": {
                "is_verified": performance.is_verified,
                "verified_at": performance.verified_at.isoformat() if performance.verified_at else None,
                "verification_notes": performance.verification_notes,
                "ai_confidence_score": performance.confidence_score,
                "ai_analysis_status": performance.ai_analysis_status
            }
        }
        
    except Exception as e:
        logger.error(f"èŽ·å–éªŒè¯åŽ†å²å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"èŽ·å–éªŒè¯åŽ†å²å¤±è´¥: {str(e)}")

@router.post("/{performance_id}/reanalyze")
async def reanalyze_performance(
    performance_id: int,
    enable_vision_analysis: bool = Form(True),
    enable_ocr: bool = Form(True),
    update_fields: bool = Form(False),  # æ˜¯å¦ä½¿ç”¨AIç»“æžœæ›´æ–°å­—æ®µ
    db: Session = Depends(get_db)
):
    """é‡æ–°åˆ†æžä¸šç»©æ–‡ä»¶"""
    try:
        from ai_service import ai_service
        
        performance = db.query(Performance).filter(Performance.id == performance_id).first()
        if not performance:
            raise HTTPException(status_code=404, detail="ä¸šç»©è®°å½•ä¸å­˜åœ¨")
        
        if not performance.source_document or not os.path.exists(performance.source_document):
            raise HTTPException(status_code=400, detail="æºæ–‡æ¡£ä¸å­˜åœ¨ï¼Œæ— æ³•é‡æ–°åˆ†æž")
        
        # æ£€æŸ¥OCRè®¾ç½®
        ocr_enabled = False
        try:
            from database import get_db
            from models import SystemSettings
            
            db_check = next(get_db())
            ocr_setting = db_check.query(SystemSettings).filter(
                SystemSettings.setting_key == "docling_enable_ocr"
            ).first()
            
            if ocr_setting:
                ocr_enabled = ocr_setting.setting_value.lower() == "true"
            db_check.close()
        except Exception as e:
            logger.warning(f"æ£€æŸ¥OCRè®¾ç½®å¤±è´¥: {e}")
        
        # å¦‚æžœç”¨æˆ·è¯·æ±‚OCRä½†OCRå·²å…³é—­ï¼Œç»™å‡ºæç¤º
        if enable_ocr and not ocr_enabled:
            logger.info("âš ï¸ ç”¨æˆ·è¯·æ±‚OCRåˆ†æžï¼Œä½†OCRåŠŸèƒ½å·²å…³é—­")
            # ç»§ç»­æ‰§è¡Œï¼Œä½†OCRä¼šè¢«AIæœåŠ¡è‡ªåŠ¨ç¦ç”¨
        
        # æ›´æ–°çŠ¶æ€ä¸ºæ­£åœ¨åˆ†æž
        performance.ai_analysis_status = "analyzing"
        db.commit()
        
        # è°ƒç”¨AIæœåŠ¡é‡æ–°åˆ†æžæ–‡æ¡£
        logger.info(f"å¼€å§‹é‡æ–°åˆ†æžä¸šç»©æ–‡ä»¶: {performance.source_document}")
        
        if not ai_service.enable_ai:
            raise HTTPException(status_code=503, detail="AIæœåŠ¡æœªå¯ç”¨ï¼Œæ— æ³•è¿›è¡Œåˆ†æž")
        
        try:
            ai_result = await ai_service.smart_document_analysis(
                performance.source_document,
                enable_vision=enable_vision_analysis,
                enable_ocr=enable_ocr
            )
            
            if not ai_result:
                raise Exception("AIåˆ†æžæœªè¿”å›žç»“æžœ")
            
            logger.info(f"AIé‡æ–°åˆ†æžå®Œæˆ: {performance.source_document}")
            
            # ä»ŽAIç»“æžœä¸­æå–ä¸šç»©ä¿¡æ¯
            extracted_info = _extract_performance_info(ai_result)
            logger.info(f"é‡æ–°æå–çš„ä¸šç»©ä¿¡æ¯: {extracted_info}")

            # æ›´æ–°AIåˆ†æžç»“æžœ
            performance.ai_analysis = {
                "reanalysis_result": ai_result,
                "reanalysis_timestamp": datetime.now().isoformat(),
                "extracted_info": extracted_info
            }
            
            # è®¾ç½®ç½®ä¿¡åº¦
            if ai_result.get("final_classification"):
                classification = ai_result["final_classification"]
                performance.confidence_score = classification.get("confidence", 0.0)
            
            # æå–æ–‡æœ¬å†…å®¹
            if ai_result.get('text_extraction_result', {}).get('text'):
                performance.extracted_text = ai_result['text_extraction_result']['text']
            
            # å¦‚æžœå¯ç”¨äº†å­—æ®µæ›´æ–°ï¼Œåˆ™ä½¿ç”¨AIç»“æžœæ›´æ–°ä¸šç»©å­—æ®µ
            if update_fields:
                if extracted_info.get('project_name'):
                    performance.project_name = extracted_info['project_name']
                if extracted_info.get('client_name'):
                    performance.client_name = extracted_info['client_name']
                if extracted_info.get('business_field'):
                    performance.business_field = extracted_info['business_field']
                if extracted_info.get('contract_amount'):
                    performance.contract_amount = extracted_info['contract_amount']
                if extracted_info.get('year'):
                    performance.year = extracted_info['year']
                if extracted_info.get('description'):
                    performance.description = extracted_info['description']
                
                logger.info(f"å·²ä½¿ç”¨AIç»“æžœæ›´æ–°ä¸šç»©å­—æ®µ")
            
            # æ›´æ–°AIåˆ†æžçŠ¶æ€ä¸ºå®Œæˆ
            performance.ai_analysis_status = "completed"
            db.commit()
            
            # æž„å»ºå“åº”æ¶ˆæ¯
            response_message = "AIé‡æ–°åˆ†æžå®Œæˆ"
            if enable_ocr and not ocr_enabled:
                response_message += "ï¼ˆæ³¨æ„ï¼šOCRåŠŸèƒ½å·²å…³é—­ï¼Œåˆ†æžä¸åŒ…å«OCRæ–‡æœ¬è¯†åˆ«ï¼‰"
        
            return {
                "success": True,
                "message": response_message,
                "ai_analysis": {
                    "confidence_score": performance.confidence_score,
                    "extracted_info": extracted_info,
                    "fields_updated": update_fields,
                    "ocr_status": ai_result.get("ocr_status", "unknown")
                }
            }
                    
        except Exception as e:
            logger.error(f"AIé‡æ–°åˆ†æžå¤±è´¥: {str(e)}")
            # æ›´æ–°AIåˆ†æžçŠ¶æ€ä¸ºå¤±è´¥
            performance.ai_analysis_status = "failed"
            db.commit()
            raise HTTPException(status_code=500, detail=f"AIé‡æ–°åˆ†æžå¤±è´¥: {str(e)}")
        
    except Exception as e:
        db.rollback()
        logger.error(f"é‡æ–°åˆ†æžä¸šç»©å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"é‡æ–°åˆ†æžå¤±è´¥: {str(e)}")

@router.delete("/{performance_id}")
async def delete_performance(
    performance_id: int,
    db: Session = Depends(get_db)
):
    """åˆ é™¤ä¸šç»©è®°å½•"""
    try:
        performance = db.query(Performance).filter(Performance.id == performance_id).first()
        if not performance:
            raise HTTPException(status_code=404, detail="ä¸šç»©è®°å½•ä¸å­˜åœ¨")
        
        # åˆ é™¤å…³è”æ–‡ä»¶
        performance_files = db.query(PerformanceFile).filter(PerformanceFile.performance_id == performance_id).all()
        for perf_file in performance_files:
            if perf_file.file_path and os.path.exists(perf_file.file_path):
                try:
                    os.remove(perf_file.file_path)
                    logger.info(f"å·²åˆ é™¤æ–‡ä»¶: {perf_file.file_path}")
                except Exception as file_error:
                    logger.warning(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {perf_file.file_path}, é”™è¯¯: {file_error}")
        
        # åˆ é™¤æºæ–‡æ¡£
        if performance.source_document and os.path.exists(performance.source_document):
            try:
                os.remove(performance.source_document)
                logger.info(f"å·²åˆ é™¤æºæ–‡æ¡£: {performance.source_document}")
            except Exception as source_error:
                logger.warning(f"åˆ é™¤æºæ–‡æ¡£å¤±è´¥: {performance.source_document}, é”™è¯¯: {source_error}")
        
        # åˆ é™¤æ•°æ®åº“è®°å½•
        db.delete(performance)
        db.commit()
        
        logger.info(f"âœ… ä¸šç»©è®°å½•åˆ é™¤æˆåŠŸ: ID={performance_id}")
        
        return {
            "success": True,
            "message": "ä¸šç»©è®°å½•å·²åˆ é™¤"
        }
        
    except Exception as e:
        db.rollback()
        logger.error(f"åˆ é™¤ä¸šç»©è®°å½•å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ é™¤å¤±è´¥: {str(e)}")

@router.post("/{performance_id}/delete")
async def deletePerformance(
    performance_id: int,
    db: Session = Depends(get_db)
):
    """åˆ é™¤ä¸šç»©è®°å½•ï¼ˆPOSTæ–¹æ³•ï¼Œä¾›å‰ç«¯è°ƒç”¨ï¼‰"""
    return await delete_performance(performance_id, db)

# ==================== é…ç½®ç®¡ç†API ====================

@router.get("/config/business-fields")
async def get_business_fields():
    """èŽ·å–ä¸šåŠ¡é¢†åŸŸåˆ—è¡¨"""
    try:
        fields = config_manager.get_business_fields()
        return {
            "success": True,
            "business_fields": fields
        }
    except Exception as e:
        logger.error(f"èŽ·å–ä¸šåŠ¡é¢†åŸŸå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"èŽ·å–ä¸šåŠ¡é¢†åŸŸå¤±è´¥: {str(e)}")

@router.get("/config/performance-types")
async def get_performance_types():
    """èŽ·å–ä¸šç»©ç±»åž‹åˆ—è¡¨"""
    try:
        types = config_manager.get_performance_types()
        return {
            "success": True,
            "performance_types": types
        }
    except Exception as e:
        logger.error(f"èŽ·å–ä¸šç»©ç±»åž‹å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"èŽ·å–ä¸šç»©ç±»åž‹å¤±è´¥: {str(e)}")

@router.post("/config/reload")
async def reload_configs():
    """é‡æ–°åŠ è½½é…ç½®æ–‡ä»¶"""
    try:
        config_manager.reload_all_configs()
        return {
            "success": True,
            "message": "é…ç½®æ–‡ä»¶é‡æ–°åŠ è½½æˆåŠŸ"
        }
    except Exception as e:
        logger.error(f"é‡æ–°åŠ è½½é…ç½®å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"é‡æ–°åŠ è½½é…ç½®å¤±è´¥: {str(e)}")

@router.put("/config/business-fields")
async def update_business_fields(business_fields: List[Dict[str, Any]]):
    """æ›´æ–°ä¸šåŠ¡é¢†åŸŸé…ç½®"""
    try:
        config_manager.update_business_fields(business_fields)
        return {
            "success": True,
            "message": "ä¸šåŠ¡é¢†åŸŸé…ç½®æ›´æ–°æˆåŠŸ"
        }
    except Exception as e:
        logger.error(f"æ›´æ–°ä¸šåŠ¡é¢†åŸŸé…ç½®å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ›´æ–°ä¸šåŠ¡é¢†åŸŸé…ç½®å¤±è´¥: {str(e)}")

@router.put("/config/performance-types")
async def update_performance_types(performance_types: List[Dict[str, Any]]):
    """æ›´æ–°ä¸šç»©ç±»åž‹é…ç½®"""
    try:
        config_manager.update_performance_types(performance_types)
        return {
            "success": True,
            "message": "ä¸šç»©ç±»åž‹é…ç½®æ›´æ–°æˆåŠŸ"
        }
    except Exception as e:
        logger.error(f"æ›´æ–°ä¸šç»©ç±»åž‹é…ç½®å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ›´æ–°ä¸šç»©ç±»åž‹é…ç½®å¤±è´¥: {str(e)}")

# ==================== AIå­¦ä¹ API ====================

@router.post("/{performance_id}/verify-with-learning")
async def verify_performance_with_learning(
    performance_id: int,
    original_values: Dict[str, Any],
    corrected_values: Dict[str, Any],
    learning_notes: Optional[str] = None,
    db: Session = Depends(get_db)
):
    """éªŒè¯ä¸šç»©è®°å½•å¹¶è®°å½•AIå­¦ä¹ æ•°æ®"""
    try:
        performance = db.query(Performance).filter(Performance.id == performance_id).first()
        if not performance:
            raise HTTPException(status_code=404, detail="ä¸šç»©è®°å½•ä¸å­˜åœ¨")
        
        # è®°å½•ç”¨æˆ·åé¦ˆ
        feedback_data = {
            "type": "performance_correction",
            "performance_id": performance_id,
            "original_values": original_values,
            "corrected_values": corrected_values,
            "learning_notes": learning_notes,
            "source_document": performance.source_document,
            "ai_analysis_status": performance.ai_analysis_status,
            "confidence_score": performance.confidence_score
        }
        
        config_manager.add_user_feedback(feedback_data)
        
        # è®°å½•ä¿®æ­£æ¨¡å¼
        for field, original_value in original_values.items():
            corrected_value = corrected_values.get(field)
            if original_value != corrected_value:
                pattern_data = {
                    "field": field,
                    "original_value": original_value,
                    "corrected_value": corrected_value,
                    "performance_id": performance_id,
                    "context": {
                        "project_name": performance.project_name,
                        "client_name": performance.client_name,
                        "business_field": performance.business_field,
                        "project_type": performance.project_type
                    }
                }
                config_manager.add_correction_pattern(pattern_data)
        
        # æ›´æ–°ä¸šç»©è®°å½•
        for field, value in corrected_values.items():
            if hasattr(performance, field):
                setattr(performance, field, value)
        
        performance.is_verified = True
        performance.updated_at = datetime.now()
        
        db.commit()
        
        logger.info(f"ä¸šç»©è®°å½•éªŒè¯å®Œæˆå¹¶è®°å½•å­¦ä¹ æ•°æ®: ID={performance_id}")
        
        return {
            "success": True,
            "message": "ä¸šç»©è®°å½•éªŒè¯æˆåŠŸï¼ŒAIå­¦ä¹ æ•°æ®å·²è®°å½•",
            "learning_recorded": True
        }
        
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        logger.error(f"éªŒè¯ä¸šç»©è®°å½•å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"éªŒè¯ä¸šç»©è®°å½•å¤±è´¥: {str(e)}")

@router.get("/ai-learning/stats")
async def get_ai_learning_stats():
    """èŽ·å–AIå­¦ä¹ ç»Ÿè®¡ä¿¡æ¯"""
    try:
        learning_data = config_manager.get_ai_learning_data()
        
        feedback_count = len(learning_data.get("user_feedback", []))
        pattern_count = len(learning_data.get("correction_patterns", []))
        
        # ç»Ÿè®¡ä¿®æ­£æ¨¡å¼
        field_corrections = {}
        for pattern in learning_data.get("correction_patterns", []):
            field = pattern.get("field", "unknown")
            field_corrections[field] = field_corrections.get(field, 0) + 1
        
        return {
            "success": True,
            "stats": {
                "total_feedback": feedback_count,
                "total_corrections": pattern_count,
                "field_corrections": field_corrections,
                "last_updated": learning_data.get("last_updated")
            }
        }
    except Exception as e:
        logger.error(f"èŽ·å–AIå­¦ä¹ ç»Ÿè®¡å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"èŽ·å–AIå­¦ä¹ ç»Ÿè®¡å¤±è´¥: {str(e)}")

@router.get("/ai-learning/patterns")
async def get_correction_patterns():
    """èŽ·å–ä¿®æ­£æ¨¡å¼"""
    patterns = config_manager.get_correction_patterns()
    return {
        "success": True,
        "patterns": patterns
    }

# AIä»»åŠ¡ç®¡ç†API
@router.get("/tasks/{task_id}")
async def get_ai_task_status(
    task_id: int,
    db: Session = Depends(get_db)
):
    """èŽ·å–AIä»»åŠ¡çŠ¶æ€"""
    from ai_service import get_ai_task_status
    
    task_status = get_ai_task_status(db, task_id)
    
    if "error" in task_status:
        raise HTTPException(status_code=404, detail=task_status["error"])
    
    return {
        "success": True,
        "task": task_status
    }

@router.get("/tasks")
async def list_ai_tasks(
    page: int = 1,
    page_size: int = 20,
    status: Optional[str] = None,
    db: Session = Depends(get_db)
):
    """èŽ·å–AIä»»åŠ¡åˆ—è¡¨"""
    try:
        query = db.query(AITask).filter(AITask.file_type == "performance")
        
        if status:
            query = query.filter(AITask.status == status)
        
        total = query.count()
        offset = (page - 1) * page_size
        tasks = query.order_by(AITask.created_at.desc()).offset(offset).limit(page_size).all()
        
        return {
            "success": True,
            "tasks": [
                {
                    "id": task.id,
                    "file_id": task.file_id,
                    "file_type": task.file_type,
                    "status": task.status,
                    "result_snapshot": task.result_snapshot,
                    "error_message": task.error_message,
                    "created_at": task.created_at.isoformat() if task.created_at else None,
                    "updated_at": task.updated_at.isoformat() if task.updated_at else None
                }
                for task in tasks
            ],
            "pagination": {
                "page": page,
                "page_size": page_size,
                "total": total,
                "pages": (total + page_size - 1) // page_size
            }
        }
        
    except Exception as e:
        logger.error(f"èŽ·å–AIä»»åŠ¡åˆ—è¡¨å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"èŽ·å–AIä»»åŠ¡åˆ—è¡¨å¤±è´¥: {str(e)}") 