from fastapi import FastAPI, File, UploadFile, Depends, HTTPException, Form, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from fastapi.staticfiles import StaticFiles
from sqlalchemy.orm import Session
from typing import List, Optional, Dict, Any, Union
import os
import sys
import shutil
import asyncio
import logging
from datetime import datetime, timezone, timedelta
import json
from docx import Document
from docx.shared import Inches, Pt, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH
from PIL import Image
from sqlalchemy import desc
import pytz
from sqlalchemy.sql import func

# æ·»åŠ å½“å‰ç›®å½•åˆ°sys.pathä»¥ç¡®ä¿æ¨¡å—å¯ä»¥è¢«å¯¼å…¥
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from database import get_db, init_db, SessionLocal, engine
from models import *
from ai_service import ai_service
from screenshot_service import screenshot_service
from document_generator import document_generator
import schemas
from schemas import (
    Award as AwardSchema, AwardCreate, AwardResponse,
    Performance as PerformanceSchema, PerformanceCreate, PerformanceResponse,
    Project as ProjectSchema, ProjectCreate, ProjectResponse,
    SystemSettingsResponse, BrandResponse, BusinessFieldResponse, BaseResponse
)

# å¯¼å…¥æ–°çš„APIæ¨¡å—ï¼ˆç¡®ä¿å®ƒä»¬å­˜åœ¨äºåŒä¸€ç›®å½•ä¸‹ï¼‰
try:
    import project_api
    import template_api
    import section_api
    import search_api
    import ai_tools_api
    import file_management_api
    from document_processor import docling_processor
    IMPORT_SUCCESS = True
except ImportError as e:
    logging.error(f"å¯¼å…¥APIæ¨¡å—å¤±è´¥: {str(e)}")
    IMPORT_SUCCESS = False

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åˆ›å»ºFastAPIåº”ç”¨
APP_NAME = os.getenv("APP_NAME", "æŠ•æ ‡è‹¦")
app = FastAPI(
    title=APP_NAME,
    description="æ³•å¾‹è¡Œä¸šæŠ•æ ‡èµ„æ–™ç®¡ç†å’Œç”Ÿæˆç³»ç»Ÿï¼ŒåŒ…æ‹¬æŠ•æ ‡æ–‡ä»¶è‡ªåŠ¨ç»„è£…åŠŸèƒ½",
    version="1.0.0"
)

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://frontend:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æŒ‚è½½é™æ€æ–‡ä»¶
app.mount("/screenshots", StaticFiles(directory="/app/screenshots"), name="screenshots")
app.mount("/uploads", StaticFiles(directory="/app/uploads"), name="uploads")
app.mount("/generated", StaticFiles(directory="/app/generated_docs"), name="generated")

# åˆ›å»ºå¿…è¦çš„ç›®å½•
os.makedirs("/app/uploads", exist_ok=True)
os.makedirs("/app/screenshots", exist_ok=True)
os.makedirs("/app/generated_docs", exist_ok=True)

MAX_UPLOAD_SIZE_MB = int(os.getenv('MAX_UPLOAD_SIZE_MB', '50'))
MAX_UPLOAD_SIZE = MAX_UPLOAD_SIZE_MB * 1024 * 1024
HISTORY_FILE = "/app/generated_docs/convert_history.json"

def format_heading(doc, text, level=1, center=False):
    """
    åˆ›å»ºæ ¼å¼åŒ–çš„æ ‡é¢˜
    
    å‚æ•°:
    - doc: Documentå¯¹è±¡
    - text: æ ‡é¢˜æ–‡å­—
    - level: æ ‡é¢˜çº§åˆ« (0=ä¸»æ ‡é¢˜, 1=ä¸€çº§æ ‡é¢˜, 2=äºŒçº§æ ‡é¢˜)
    - center: æ˜¯å¦å±…ä¸­
    
    è¿”å›:
    - æ ¼å¼åŒ–çš„æ ‡é¢˜æ®µè½
    """
    from docx.oxml.shared import qn
    from docx.oxml.ns import nsdecls, qn
    from docx.oxml import parse_xml
    
    # åˆ›å»ºæ ‡é¢˜
    heading = doc.add_heading(text, level)
    
    # è®¾ç½®å¯¹é½æ–¹å¼
    if center or level == 0:
        heading.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
            # æ¸…é™¤æ®µè½çš„åˆ—è¡¨æ ·å¼å’Œé¡¹ç›®ç¬¦å·ï¼Œé˜²æ­¢å‡ºç°å°é»‘ç‚¹
        try:
            # æ¸…é™¤æ®µè½çš„ç¼–å·å’Œé¡¹ç›®ç¬¦å·
            pPr = heading._element.get_or_add_pPr()
            
            # ç§»é™¤ç¼–å·å±æ€§
            numPr = pPr.find(qn('w:numPr'))
            if numPr is not None:
                pPr.remove(numPr)
            
            # è®¾ç½®æ®µè½æ ¼å¼ï¼Œæ˜ç¡®ç¦ç”¨åˆ—è¡¨æ ·å¼å’Œåˆ†é¡µæ§åˆ¶
            if hasattr(heading, 'paragraph_format'):
                heading.paragraph_format.left_indent = None
                heading.paragraph_format.first_line_indent = None
                
                # ç¦ç”¨åˆ†é¡µæ§åˆ¶é€‰é¡¹ï¼Œå¯¹åº”Wordä¸­çš„"åˆ†é¡µ"è®¾ç½®
                heading.paragraph_format.widow_control = False      # å­¤è¡Œæ§åˆ¶
                heading.paragraph_format.keep_with_next = False     # ä¸ä¸‹æ®µåŒé¡µ
                heading.paragraph_format.keep_together = False      # æ®µä¸­ä¸åˆ†é¡µ
                heading.paragraph_format.page_break_before = False  # æ®µå‰åˆ†é¡µ
                
        except Exception as style_error:
            logger.warning(f"æ¸…é™¤æ ‡é¢˜æ ·å¼æ—¶å‡ºé”™: {style_error}")
    
    # è®¾ç½®å­—ä½“
    if heading.runs:
        run = heading.runs[0]
        
        # æ ¹æ®çº§åˆ«è®¾ç½®å­—ä½“å¤§å°
        if level == 0:
            run.font.size = Pt(18)
            run.font.name = 'æ¥·ä½“'
        elif level == 1:
            run.font.size = Pt(16)
            run.font.name = 'æ¥·ä½“'
        elif level == 2:
            run.font.size = Pt(14)
            run.font.name = 'æ¥·ä½“'
        else:
            run.font.size = Pt(12)
            run.font.name = 'æ¥·ä½“'
        
        run.bold = True
        
        # ç¡®ä¿å­—ä½“é¢œè‰²æ˜¯é»‘è‰²
        run.font.color.rgb = RGBColor(0, 0, 0)  # è®¾ç½®ä¸ºé»‘è‰²
        
        # è®¾ç½®ä¸­è‹±æ–‡å­—ä½“
        run._element.rPr.rFonts.set(qn('w:ascii'), 'Times New Roman')
        run._element.rPr.rFonts.set(qn('w:eastAsia'), 'æ¥·ä½“')
        run._element.rPr.rFonts.set(qn('w:hAnsi'), 'Times New Roman')
    
    return heading

@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–æ•°æ®åº“"""
    try:
        init_db()
        logger.info("æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
        
        # åˆå§‹åŒ–åŸºç¡€æ•°æ®
        await init_base_data()
        
        # åˆå§‹åŒ–é«˜çº§åŠŸèƒ½æ•°æ®
        await init_advanced_data()
        
        # å¯åŠ¨æ—¶ä¸‹è½½Doclingæ¨¡å‹
        await download_docling_models_on_startup()
        
        # æ³¨å†ŒAPIè·¯ç”±
        if IMPORT_SUCCESS:
            # æ³¨å†Œé¡¹ç›®APIè·¯ç”±
            project_api.setup_router(app)
            logger.info("é¡¹ç›®APIè·¯ç”±æ³¨å†ŒæˆåŠŸ")
            
            # æ³¨å†Œæ¨¡æ¿APIè·¯ç”±
            template_api.setup_router(app)
            logger.info("æ¨¡æ¿APIè·¯ç”±æ³¨å†ŒæˆåŠŸ")
            
            # æ³¨å†Œæ™ºèƒ½ç« èŠ‚ç®¡ç†APIè·¯ç”±
            app.include_router(section_api.router)
            logger.info("æ™ºèƒ½ç« èŠ‚ç®¡ç†APIè·¯ç”±æ³¨å†ŒæˆåŠŸ")
            
            # æ³¨å†ŒAIè‡ªåŠ¨æ£€ç´¢APIè·¯ç”±
            app.include_router(search_api.router)
            logger.info("AIè‡ªåŠ¨æ£€ç´¢APIè·¯ç”±æ³¨å†ŒæˆåŠŸ")
            
            # æ³¨å†ŒAIå·¥å…·APIè·¯ç”±
            app.include_router(ai_tools_api.router)
            logger.info("AIå·¥å…·APIè·¯ç”±æ³¨å†ŒæˆåŠŸ")
            
            # æ³¨å†Œæ–‡ä»¶ç®¡ç†APIè·¯ç”±
            file_management_api.setup_router(app)
            logger.info("æ–‡ä»¶ç®¡ç†APIè·¯ç”±æ³¨å†ŒæˆåŠŸ")
            
            # æ³¨å†Œå¾‹å¸ˆè¯ç®¡ç†APIè·¯ç”±
            try:
                import lawyer_certificate_api
                lawyer_certificate_api.setup_router(app)
                logger.info("å¾‹å¸ˆè¯ç®¡ç†APIè·¯ç”±æ³¨å†ŒæˆåŠŸ")
            except ImportError as e:
                logger.warning(f"å¾‹å¸ˆè¯ç®¡ç†APIè·¯ç”±æ³¨å†Œå¤±è´¥: {str(e)}")
            
            # æ³¨å†Œä¸šç»©ç®¡ç†APIè·¯ç”±
            try:
                import performance_api
                app.include_router(performance_api.router, prefix="/api/performances", tags=["performances"])
                logger.info("ä¸šç»©ç®¡ç†APIè·¯ç”±æ³¨å†ŒæˆåŠŸ")
            except ImportError as e:
                logger.warning(f"ä¸šç»©ç®¡ç†APIè·¯ç”±æ³¨å†Œå¤±è´¥: {str(e)}")
            
            # æ³¨å†Œå¥–é¡¹ç®¡ç†APIè·¯ç”±
            try:
                import award_api
                app.include_router(award_api.router, prefix="/api/awards", tags=["awards"])
                logger.info("å¥–é¡¹ç®¡ç†APIè·¯ç”±æ³¨å†ŒæˆåŠŸ")
            except ImportError as e:
                logger.warning(f"å¥–é¡¹ç®¡ç†APIè·¯ç”±æ³¨å†Œå¤±è´¥: {str(e)}")
        else:
            logger.error("APIè·¯ç”±æ³¨å†Œå¤±è´¥ï¼Œæ¨¡å—å¯¼å…¥é”™è¯¯")
        
    except Exception as e:
        logger.error(f"åº”ç”¨å¯åŠ¨å¤±è´¥: {str(e)}")

async def init_base_data():
    """åˆå§‹åŒ–åŸºç¡€æ•°æ®ï¼Œå¦‚å‚ç‰Œã€ä¸šåŠ¡é¢†åŸŸç­‰"""
    db = SessionLocal()
    try:
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰å‚ç‰Œæ•°æ®
        if db.query(Brand).count() == 0:
            default_brands = [
                Brand(name="å“ç‰ŒA", full_name="å“ç‰ŒAå…¨ç§°", website="http://example.com/a"),
                Brand(name="å“ç‰ŒB", full_name="å“ç‰ŒBå…¨ç§°", website="http://example.com/b"),
                Brand(name="å“ç‰ŒC", full_name="å“ç‰ŒCå…¨ç§°", website="http://example.com/c", is_active=False),
                Brand(name="é€šç”¨", full_name="é€šç”¨/å…¶ä»–", website=""),
            ]
            db.add_all(default_brands)
            logger.info(f"å·²åˆå§‹åŒ– {len(default_brands)} ä¸ªé»˜è®¤å‚ç‰Œ")

        # æ£€æŸ¥æ˜¯å¦å·²æœ‰ä¸šåŠ¡é¢†åŸŸæ•°æ®
        if db.query(BusinessField).count() == 0:
            default_fields = [
                BusinessField(name="é¢†åŸŸX", description="ä¸šåŠ¡é¢†åŸŸXçš„æè¿°"),
                BusinessField(name="é¢†åŸŸY", description="ä¸šåŠ¡é¢†åŸŸYçš„æè¿°"),
                BusinessField(name="é¢†åŸŸZ", description="ä¸šåŠ¡é¢†åŸŸZçš„æè¿°"),
            ]
            db.add_all(default_fields)
            logger.info(f"å·²åˆå§‹åŒ– {len(default_fields)} ä¸ªé»˜è®¤ä¸šåŠ¡é¢†åŸŸ")

        # æ£€æŸ¥æ˜¯å¦å·²æœ‰å¥–é¡¹æ•°æ®
        if db.query(Award).count() == 0:
            default_awards = [
                Award(title="å¹´åº¦æœ€ä½³æ³•å¾‹é¡¾é—®å¥–", brand="å“ç‰ŒA", year=2023, business_type="é¢†åŸŸX", description="åœ¨å…¬å¸å¹¶è´­é¢†åŸŸè¡¨ç°å“è¶Šã€‚", is_verified=True),
                Award(title="é‡‘èç§‘æŠ€åˆ›æ–°å¥–", brand="å“ç‰ŒB", year=2024, business_type="é¢†åŸŸY", description="ä¸ºé‡‘èç§‘æŠ€è¡Œä¸šå¸¦æ¥é©å‘½æ€§åˆ›æ–°ã€‚", is_verified=False),
            ]
            db.add_all(default_awards)
            logger.info(f"å·²åˆå§‹åŒ– {len(default_awards)} ä¸ªé»˜è®¤å¥–é¡¹")

        # æ£€æŸ¥æ˜¯å¦å·²æœ‰ä¸šç»©æ•°æ® - ç§»é™¤é»˜è®¤æ•°æ®åˆ›å»º
        # if db.query(Performance).count() == 0:
        #     default_performances = [
        #         Performance(client_name="å¤§å‹ç§‘æŠ€å…¬å¸", project_name="Alphaé¡¹ç›®-è·¨å›½å¹¶è´­æ¡ˆ", project_type="å¹¶è´­é‡ç»„", business_field="é¢†åŸŸX", year=2023, contract_amount=1000000, description="ä¸€ä¸ªå¤æ‚çš„è·¨å›½å¹¶è´­é¡¹ç›®ã€‚"),
        #         Performance(client_name="åˆåˆ›é‡‘èå…¬å¸", project_name="Betaé¡¹ç›®-æ”¯ä»˜ç³»ç»Ÿåˆè§„å®¡æŸ¥", project_type="åˆè§„ä¸ç›‘ç®¡", business_field="é¢†åŸŸY", year=2024, contract_amount=500000, description="ç¡®ä¿å…¶æ”¯ä»˜ç³»ç»Ÿç¬¦åˆæ‰€æœ‰ç›¸å…³æ³•è§„ã€‚"),
        #     ]
        #     db.add_all(default_performances)
        #     logger.info(f"å·²åˆå§‹åŒ– {len(default_performances)} ä¸ªé»˜è®¤ä¸šç»©")
        # ä¸šç»©æ•°æ®ç°åœ¨ä»ç©ºå¼€å§‹ï¼Œç”¨æˆ·æ‰‹åŠ¨æ·»åŠ 
        logger.info("ä¸šç»©ç®¡ç†å·²å‡†å¤‡å°±ç»ªï¼Œæ•°æ®ä»ç©ºå¼€å§‹")
            
        db.commit()
    except Exception as e:
        logger.error(f"åŸºç¡€æ•°æ®åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        db.rollback()
    finally:
        db.close()

async def init_advanced_data():
    """åˆå§‹åŒ–é«˜çº§åŠŸèƒ½æ•°æ®"""
    try:
        # å¯¼å…¥é«˜çº§åŠŸèƒ½åˆå§‹åŒ–æ¨¡å—
        from init_advanced_data import (
            init_section_types, init_data_sources, init_recommendation_rules,
            init_advanced_brands, init_advanced_business_fields
        )
        
        # æ‰§è¡Œé«˜çº§åŠŸèƒ½åˆå§‹åŒ–
        init_section_types()
        init_data_sources()
        init_recommendation_rules()
        init_advanced_brands()
        init_advanced_business_fields()
        
        logger.info("é«˜çº§åŠŸèƒ½æ•°æ®åˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        logger.error(f"é«˜çº§åŠŸèƒ½æ•°æ®åˆå§‹åŒ–å¤±è´¥: {str(e)}")

async def download_docling_models_on_startup():
    """å¯åŠ¨æ—¶ä¸‹è½½Doclingæ¨¡å‹"""
    try:
        logger.info("ğŸš€ å¼€å§‹å¯åŠ¨æ—¶ä¸‹è½½Doclingæ¨¡å‹...")
        
        # æ£€æŸ¥DoclingæœåŠ¡æ˜¯å¦å¯ç”¨
        try:
            from docling_service import docling_service
            if docling_service and docling_service.is_initialized:
                logger.info("âœ… DoclingæœåŠ¡å·²åˆå§‹åŒ–ï¼Œæ¨¡å‹å·²å°±ç»ª")
                return
        except ImportError:
            logger.warning("âš ï¸ DoclingæœåŠ¡ä¸å¯ç”¨ï¼Œè·³è¿‡æ¨¡å‹ä¸‹è½½")
            return
        
        # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ä¼˜åŒ–ä¸‹è½½
        import os
        os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
        os.environ["HUGGINGFACE_HUB_URL"] = "https://hf-mirror.com"
        os.environ["HF_HOME"] = "/root/.cache/huggingface"
        os.environ["TRANSFORMERS_CACHE"] = "/root/.cache/huggingface/transformers"
        os.environ["HF_HUB_CACHE"] = "/root/.cache/huggingface/hub"
        
        # ç¦ç”¨hf_transferä»¥é¿å…é”™è¯¯
        os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "0"
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        import subprocess
        cache_dirs = [
            "/root/.cache/huggingface",
            "/root/.cache/huggingface/transformers", 
            "/root/.cache/huggingface/hub",
            "/root/.cache/docling/models"
        ]
        
        for cache_dir in cache_dirs:
            os.makedirs(cache_dir, exist_ok=True)
        
        logger.info("ğŸ“ ç¼“å­˜ç›®å½•å·²åˆ›å»º")
        
        # ä½¿ç”¨docling-toolsä¸‹è½½æ¨¡å‹
        try:
            logger.info("â¬‡ï¸ å¼€å§‹ä¸‹è½½Doclingæ¨¡å‹...")
            
            # ä¸‹è½½æ ¸å¿ƒæ¨¡å‹
            cmd = [
                "docling-tools", "models", "download",
                "--output-dir", "/root/.cache/docling/models",
                "--models", "layout,tableformer",
                "--force"
            ]
            
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
            )
            
            if result.returncode == 0:
                logger.info("âœ… Doclingæ¨¡å‹ä¸‹è½½æˆåŠŸ")
                logger.info(f"ğŸ“ ä¸‹è½½è¾“å‡º: {result.stdout}")
            else:
                logger.warning(f"âš ï¸ Doclingæ¨¡å‹ä¸‹è½½å¤±è´¥: {result.stderr}")
                logger.info("ğŸ”„ å°è¯•å¤‡ç”¨ä¸‹è½½æ–¹æ³•...")
                
                # å¤‡ç”¨æ–¹æ³•ï¼šç›´æ¥åˆå§‹åŒ–DoclingService
                try:
                    from docling_service import docling_service
                    if docling_service:
                        docling_service._download_models()
                        logger.info("âœ… å¤‡ç”¨ä¸‹è½½æ–¹æ³•æˆåŠŸ")
                except Exception as backup_error:
                    logger.error(f"âŒ å¤‡ç”¨ä¸‹è½½æ–¹æ³•ä¹Ÿå¤±è´¥: {backup_error}")
                    
        except subprocess.TimeoutExpired:
            logger.warning("â° æ¨¡å‹ä¸‹è½½è¶…æ—¶ï¼Œå°†åœ¨åå°ç»§ç»­")
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹ä¸‹è½½è¿‡ç¨‹å‡ºé”™: {e}")
        
        # è®¾ç½®OCRä¸ºå¼€å¯çŠ¶æ€
        try:
            from database import get_db
            from models import SystemSettings
            
            db = next(get_db())
            
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰OCRè®¾ç½®
            ocr_setting = db.query(SystemSettings).filter(
                SystemSettings.setting_key == "docling_enable_ocr"
            ).first()
            
            if ocr_setting:
                ocr_setting.setting_value = "true"
                logger.info("ğŸ”§ å·²æ›´æ–°OCRè®¾ç½®ä¸ºå¼€å¯çŠ¶æ€")
            else:
                # åˆ›å»ºæ–°çš„OCRè®¾ç½®
                new_setting = SystemSettings(
                    setting_key="docling_enable_ocr",
                    setting_value="true",
                    setting_type="boolean",
                    category="ocr",
                    description="æ˜¯å¦å¯ç”¨Docling OCRåŠŸèƒ½"
                )
                db.add(new_setting)
                logger.info("ğŸ”§ å·²åˆ›å»ºOCRè®¾ç½®ä¸ºå¼€å¯çŠ¶æ€")
            
            db.commit()
            logger.info("âœ… OCRåŠŸèƒ½å·²è®¾ç½®ä¸ºå¼€å¯çŠ¶æ€")
            
        except Exception as e:
            logger.error(f"âŒ è®¾ç½®OCRçŠ¶æ€å¤±è´¥: {e}")
        finally:
            try:
                if 'db' in locals():
                    db.close()
            except:
                pass
        
        logger.info("ğŸ‰ Doclingæ¨¡å‹å¯åŠ¨ä¸‹è½½å®Œæˆ")
        
    except Exception as e:
        logger.error(f"âŒ Doclingæ¨¡å‹å¯åŠ¨ä¸‹è½½å¤±è´¥: {e}")

# ==================== æ–‡ä»¶ä¸Šä¼ å’Œå¤„ç† ====================

@app.post("/api/upload/document")
async def upload_document(
    file: UploadFile = File(...),
    document_type: str = Form(...),  # "award" or "performance"
    db: Session = Depends(get_db)
):
    """ä¸Šä¼ æ–‡æ¡£å¹¶è¿›è¡ŒAIåˆ†æ"""
    try:
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{timestamp}_{file.filename}"
        filepath = os.path.join("/app/uploads", filename)
        
        with open(filepath, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹è¿›è¡Œå¤„ç†
        if file.filename.lower().endswith('.pdf'):
            extracted_data = await ai_service.extract_text_from_pdf(filepath)
        elif file.filename.lower().endswith('.docx'):
            extracted_data = await ai_service.extract_text_from_docx(filepath)
        elif file.filename.lower().endswith(('.png', '.jpg', '.jpeg')):
            extracted_data = await ai_service.extract_text_from_image(filepath)
        else:
            raise HTTPException(status_code=400, detail="ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼")
        
        if extracted_data.get("error"):
            raise HTTPException(status_code=500, detail=f"æ–‡ä»¶å¤„ç†å¤±è´¥: {extracted_data['error']}")
        
        # AIåˆ†ææ–‡æ¡£å†…å®¹
        if document_type == "award":
            text_content = extracted_data.get("full_text") or "\n".join([item.get("text", "") for item in extracted_data.get("content", [])])
            analysis_result = await ai_service.analyze_award_document(text_content)
        else:
            text_content = extracted_data.get("full_text") or "\n".join([item.get("text", "") for item in extracted_data.get("content", [])])
            analysis_result = await ai_service.analyze_performance_document(text_content)
        
        return {
            "success": True,
            "file_path": filepath,
            "extracted_data": extracted_data,
            "analysis_result": analysis_result,
            "document_type": document_type
        }
        
    except Exception as e:
        logger.error(f"æ–‡æ¡£ä¸Šä¼ å¤„ç†å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/process/screenshot")
async def process_screenshot(
    urls: List[str],
    award_id: Optional[int] = None
):
    """å¤„ç†ç½‘é¡µæˆªå›¾"""
    try:
        if not urls:
            raise HTTPException(status_code=400, detail="URLåˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        # æ‰¹é‡æˆªå›¾
        results = await screenshot_service.capture_award_pages(urls, award_id)
        
        return {
            "success": True,
            "results": results
        }
        
    except Exception as e:
        logger.error(f"ç½‘é¡µæˆªå›¾å¤„ç†å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# ==================== è·å¥–ä¿¡æ¯ç®¡ç† ====================

@app.post("/api/awards")
async def create_award(award_data: Dict[str, Any], db: Session = Depends(get_db)):
    """åˆ›å»ºè·å¥–è®°å½•"""
    try:
        award = Award(
            title=award_data.get("title"),
            brand=award_data.get("brand"),
            year=award_data.get("year"),
            business_type=award_data.get("business_type"),
            description=award_data.get("description"),
            source_document=award_data.get("source_document"),
            source_url=award_data.get("source_url"),
            screenshot_path=award_data.get("screenshot_path"),
            screenshot_pages=award_data.get("screenshot_pages"),
            ai_analysis=award_data.get("ai_analysis"),
            confidence_score=award_data.get("confidence_score", 0.0),
            is_manual_input=award_data.get("is_manual_input", False)
        )
        
        db.add(award)
        db.commit()
        db.refresh(award)
        
        return {"success": True, "award_id": award.id}
        
    except Exception as e:
        logger.error(f"åˆ›å»ºè·å¥–è®°å½•å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/awards")
async def get_awards(
    brand: Optional[str] = Query(None),
    business_type: Optional[str] = Query(None),
    year: Optional[int] = Query(None),
    verified_only: bool = Query(False),
    skip: int = Query(0),
    limit: int = Query(100),
    db: Session = Depends(get_db)
):
    """è·å–è·å¥–è®°å½•åˆ—è¡¨"""
    try:
        query = db.query(Award)
        
        if brand:
            query = query.filter(Award.brand == brand)
        if business_type:
            query = query.filter(Award.business_type == business_type)
        if year:
            query = query.filter(Award.year == year)
        if verified_only:
            query = query.filter(Award.is_verified == True)
        
        total = query.count()
        awards = query.offset(skip).limit(limit).all()
        
        return {
            "success": True,
            "total": total,
            "awards": [
                {
                    "id": award.id,
                    "title": award.title,
                    "brand": award.brand,
                    "year": award.year,
                    "business_type": award.business_type,
                    "description": award.description,
                    "is_verified": award.is_verified,
                    "confidence_score": award.confidence_score,
                    "screenshot_pages": award.screenshot_pages,
                    "created_at": award.created_at.isoformat()
                }
                for award in awards
            ]
        }
        
    except Exception as e:
        logger.error(f"è·å–è·å¥–è®°å½•å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.put("/api/awards/{award_id}")
async def update_award(
    award_id: int,
    award_data: Dict[str, Any],
    db: Session = Depends(get_db)
):
    """æ›´æ–°è·å¥–è®°å½•"""
    try:
        award = db.query(Award).filter(Award.id == award_id).first()
        if not award:
            raise HTTPException(status_code=404, detail="è·å¥–è®°å½•ä¸å­˜åœ¨")
        
        # æ›´æ–°å­—æ®µ
        for key, value in award_data.items():
            if hasattr(award, key):
                setattr(award, key, value)
        
        award.updated_at = datetime.utcnow()
        
        db.commit()
        db.refresh(award)
        
        return {"success": True, "message": "è·å¥–è®°å½•æ›´æ–°æˆåŠŸ"}
        
    except Exception as e:
        logger.error(f"æ›´æ–°è·å¥–è®°å½•å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/api/awards/{award_id}")
async def delete_award(award_id: int, db: Session = Depends(get_db)):
    """åˆ é™¤è·å¥–è®°å½•"""
    try:
        award = db.query(Award).filter(Award.id == award_id).first()
        if not award:
            raise HTTPException(status_code=404, detail="è·å¥–è®°å½•ä¸å­˜åœ¨")
        
        db.delete(award)
        db.commit()
        
        return {"success": True, "message": "è·å¥–è®°å½•åˆ é™¤æˆåŠŸ"}
        
    except Exception as e:
        logger.error(f"åˆ é™¤è·å¥–è®°å½•å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# ==================== ä¸šç»©ä¿¡æ¯ç®¡ç† ====================

@app.post("/api/performances")
async def create_performance(performance_data: Dict[str, Any], db: Session = Depends(get_db)):
    """åˆ›å»ºä¸šç»©è®°å½•"""
    try:
        performance = Performance(
            client_name=performance_data.get("client_name"),
            project_name=performance_data.get("project_name"),
            project_type=performance_data.get("project_type"),
            business_field=performance_data.get("business_field"),
            start_date=datetime.fromisoformat(performance_data["start_date"]) if performance_data.get("start_date") else None,
            end_date=datetime.fromisoformat(performance_data["end_date"]) if performance_data.get("end_date") else None,
            year=performance_data.get("year"),
            contract_amount=performance_data.get("contract_amount"),
            currency=performance_data.get("currency", "CNY"),
            description=performance_data.get("description"),
            source_document=performance_data.get("source_document"),
            contract_file=performance_data.get("contract_file"),
            ai_analysis=performance_data.get("ai_analysis"),
            confidence_score=performance_data.get("confidence_score", 0.0),
            is_manual_input=performance_data.get("is_manual_input", False)
        )
        
        db.add(performance)
        db.commit()
        db.refresh(performance)
        
        return {"success": True, "performance_id": performance.id}
        
    except Exception as e:
        logger.error(f"åˆ›å»ºä¸šç»©è®°å½•å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/performances", response_model=PerformanceResponse)
def get_performances(
    business_field: Optional[str] = Query(None),
    project_type: Optional[str] = Query(None),
    year: Optional[int] = Query(None),
    verified_only: bool = Query(False),
    skip: int = Query(0),
    limit: int = Query(100),
    db: Session = Depends(get_db)
):
    """è·å–ä¸šç»©æ•°æ®åˆ—è¡¨"""
    try:
        query = db.query(Performance)

        if business_field:
            query = query.filter(Performance.business_field == business_field)

        if project_type:
            query = query.filter(Performance.project_type == project_type)

        if year:
            query = query.filter(Performance.year == year)

        if verified_only:
            query = query.filter(Performance.is_verified == True)

        # æŒ‰æ›´æ–°æ—¶é—´é™åºæ’åˆ—
        query = query.order_by(desc(Performance.updated_at))

        performances = query.offset(skip).limit(limit).all()
        
        # å°†SQLAlchemyæ¨¡å‹è½¬æ¢ä¸ºPydanticæ¨¡å‹
        performance_schemas = [PerformanceSchema.from_orm(p) for p in performances]

        return {
            "success": True,
            "message": "ä¸šç»©æ•°æ®è·å–æˆåŠŸ",
            "performances": performance_schemas
        }

    except Exception as e:
        logger.error(f"è·å–ä¸šç»©æ•°æ®å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–ä¸šç»©æ•°æ®å¤±è´¥: {str(e)}")

# ==================== æ–‡æ¡£ç”Ÿæˆ ====================

@app.post("/api/generate/document")
async def generate_document(
    request_data: Dict[str, Any],
    db: Session = Depends(get_db)
):
    """ç”ŸæˆWordæ–‡æ¡£"""
    try:
        doc_type = request_data.get("type", "combined")  # "award", "performance", "combined"
        filters = request_data.get("filters", {})
        
        # æ„å»ºæŸ¥è¯¢
        award_query = db.query(Award)
        performance_query = db.query(Performance)
        
        # åº”ç”¨ç­›é€‰æ¡ä»¶
        if filters.get("brands"):
            award_query = award_query.filter(Award.brand.in_(filters["brands"]))
        
        if filters.get("business_fields"):
            award_query = award_query.filter(Award.business_type.in_(filters["business_fields"]))
            performance_query = performance_query.filter(Performance.business_field.in_(filters["business_fields"]))
        
        if filters.get("years"):
            award_query = award_query.filter(Award.year.in_(filters["years"]))
            performance_query = performance_query.filter(Performance.year.in_(filters["years"]))
        
        # è·å–æ•°æ®
        awards = []
        performances = []
        
        if doc_type in ["award", "combined"]:
            awards = award_query.all()
            awards = [
                {
                    "id": award.id,
                    "title": award.title,
                    "brand": award.brand,
                    "year": award.year,
                    "business_type": award.business_type,
                    "description": award.description,
                    "screenshot_pages": award.screenshot_pages or []
                }
                for award in awards
            ]
        
        if doc_type in ["performance", "combined"]:
            performances = performance_query.all()
            performances = [
                {
                    "id": perf.id,
                    "client_name": perf.client_name,
                    "project_name": perf.project_name,
                    "project_type": perf.project_type,
                    "business_field": perf.business_field,
                    "year": perf.year,
                    "contract_amount": perf.contract_amount,
                    "currency": perf.currency,
                    "description": perf.description,
                    "start_date": perf.start_date.strftime("%Y-%m-%d") if perf.start_date else None,
                    "end_date": perf.end_date.strftime("%Y-%m-%d") if perf.end_date else None,
                    "files": []  # TODO: æ·»åŠ æ–‡ä»¶å…³è”
                }
                for perf in performances
            ]
        
        # ç”Ÿæˆæ–‡æ¡£
        if doc_type == "award":
            filepath = document_generator.generate_award_document(awards, filters)
        elif doc_type == "performance":
            filepath = document_generator.generate_performance_document(performances, filters)
        else:
            filepath = document_generator.generate_combined_document(awards, performances, filters)
        
        # è¿”å›æ–‡ä»¶ä¸‹è½½é“¾æ¥
        filename = os.path.basename(filepath)
        return {
            "success": True,
            "download_url": f"/api/download/{filename}",
            "filename": filename
        }
        
    except Exception as e:
        logger.error(f"æ–‡æ¡£ç”Ÿæˆå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/download/{filename}")
async def download_file(filename: str):
    """ä¸‹è½½ç”Ÿæˆçš„æ–‡æ¡£"""
    # å…ˆåœ¨ generated_docs ç›®å½•æŸ¥æ‰¾ï¼ˆç”Ÿæˆçš„æ–‡æ¡£ï¼‰
    generated_filepath = os.path.join("/app/generated_docs", filename)
    if os.path.exists(generated_filepath):
        return FileResponse(
            path=generated_filepath,
            filename=filename,
            media_type="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
        )
    
    # å†åœ¨ uploads ç›®å½•æŸ¥æ‰¾ï¼ˆå†å²é—ç•™æ–‡ä»¶ï¼‰
    uploads_filepath = os.path.join("/app/uploads", filename)
    if os.path.exists(uploads_filepath):
        return FileResponse(
            path=uploads_filepath,
            filename=filename,
            media_type="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
        )
    
    raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")

# ==================== é…ç½®ç®¡ç† ====================

@app.get("/api/config/brands")
async def get_brands(db: Session = Depends(get_db)):
    """è·å–å‚ç‰Œåˆ—è¡¨"""
    brands = db.query(Brand).filter(Brand.is_active == True).all()
    return {
        "success": True,
        "brands": [
            {
                "id": brand.id,
                "name": brand.name,
                "full_name": brand.full_name,
                "website": brand.website
            }
            for brand in brands
        ]
    }

@app.get("/api/config/business-fields")
async def get_business_fields(db: Session = Depends(get_db)):
    """è·å–ä¸šåŠ¡é¢†åŸŸåˆ—è¡¨"""
    fields = db.query(BusinessField).filter(BusinessField.is_active == True).all()
    return {
        "success": True,
        "business_fields": [
            {
                "id": field.id,
                "name": field.name,
                "description": field.description,
                "parent_id": field.parent_id
            }
            for field in fields
        ]
    }

# ==================== å¥åº·æ£€æŸ¥ ====================

@app.get("/api/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "version": "1.0.0"
    }

@app.get("/api/app-info")
async def get_app_info():
    """è·å–åº”ç”¨ä¿¡æ¯"""
    return {
        "app_name": APP_NAME,
        "version": "1.0.0",
        "description": "æ³•å¾‹è¡Œä¸šæŠ•æ ‡èµ„æ–™ç®¡ç†å’Œç”Ÿæˆç³»ç»Ÿ",
        "max_upload_size_mb": MAX_UPLOAD_SIZE_MB
    }

# ==================== ç³»ç»Ÿè®¾ç½®ç®¡ç† ====================

@app.get("/api/settings")
async def get_settings(category: Optional[str] = None, db: Session = Depends(get_db)):
    """è·å–ç³»ç»Ÿè®¾ç½®"""
    try:
        query = db.query(SystemSettings)
        if category:
            query = query.filter(SystemSettings.category == category)
        
        settings = query.all()
        
        # å°†è®¾ç½®è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        settings_dict = {}
        for setting in settings:
            # æ•æ„Ÿä¿¡æ¯è¿›è¡Œè„±æ•å¤„ç†
            value = setting.setting_value
            if setting.is_sensitive and value:
                value = "******" if len(value) > 6 else "*" * len(value)
            
            settings_dict[setting.setting_key] = {
                "value": value,
                "type": setting.setting_type,
                "category": setting.category,
                "description": setting.description,
                "is_sensitive": setting.is_sensitive
            }
        
        return {
            "success": True,
            "settings": settings_dict
        }
        
    except Exception as e:
        logger.error(f"è·å–è®¾ç½®å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/settings")
async def update_settings(
    settings_data: Dict[str, Any],
    db: Session = Depends(get_db)
):
    """æ›´æ–°ç³»ç»Ÿè®¾ç½®"""
    try:
        updated_settings = []
        
        for key, value in settings_data.items():
            # æŸ¥æ‰¾ç°æœ‰è®¾ç½®
            existing_setting = db.query(SystemSettings).filter(
                SystemSettings.setting_key == key
            ).first()
            
            if existing_setting:
                # å¯¹äºæ•æ„Ÿä¿¡æ¯ï¼Œå¦‚æœå€¼ä¸ºè„±æ•ç¬¦å·åˆ™è·³è¿‡æ›´æ–°
                if existing_setting.is_sensitive and str(value) == "******":
                    logger.info(f"è·³è¿‡æ•æ„Ÿä¿¡æ¯ {key} çš„è„±æ•å€¼æ›´æ–°")
                    continue
                # æ›´æ–°ç°æœ‰è®¾ç½®
                existing_setting.setting_value = str(value)
                existing_setting.updated_at = datetime.utcnow()
            else:
                # åˆ›å»ºæ–°è®¾ç½®ï¼ˆæ ¹æ®é”®åæ¨æ–­åˆ†ç±»ï¼‰
                category = "general"
                if key.startswith("ai_") or "model" in key.lower():
                    category = "ai"
                elif key.startswith("upload_"):
                    category = "upload"
                elif key.startswith("screenshot_"):
                    category = "screenshot"
                elif key.startswith("easyocr_") or key.startswith("docling_") or key == "enable_docling_ocr":
                    category = "ocr"
                
                new_setting = SystemSettings(
                    setting_key=key,
                    setting_value=str(value),
                    category=category,
                    is_sensitive="api_key" in key.lower() or "secret" in key.lower()
                )
                db.add(new_setting)
            
            updated_settings.append(key)
        
        db.commit()
        
        # é‡æ–°åˆå§‹åŒ–AIæœåŠ¡ä»¥åº”ç”¨æ–°çš„æ¨¡å‹é…ç½®
        if any("model" in key.lower() or "ai_" in key for key in updated_settings):
            ai_service.reload_config()
        
        return {
            "success": True,
            "message": f"å·²æ›´æ–° {len(updated_settings)} ä¸ªè®¾ç½®",
            "updated_settings": updated_settings
        }
        
    except Exception as e:
        logger.error(f"æ›´æ–°è®¾ç½®å¤±è´¥: {str(e)}")
        db.rollback()
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/settings/init-defaults")
async def init_default_settings(db: Session = Depends(get_db)):
    """åˆå§‹åŒ–é»˜è®¤è®¾ç½®"""
    try:
        default_settings = [
            # AIæ¨¡å‹è®¾ç½®
            {
                "key": "ai_provider",
                "value": "openai",
                "category": "ai",
                "description": "AIæœåŠ¡æä¾›å•† (openai/azure/custom)"
            },
            {
                "key": "ai_text_model",
                "value": "gpt-4",
                "category": "ai",
                "description": "æ–‡æœ¬åˆ†ææ¨¡å‹"
            },
            {
                "key": "ai_vision_model",
                "value": "gpt-4-vision-preview",
                "category": "ai",
                "description": "å›¾åƒåˆ†ææ¨¡å‹"
            },
            {
                "key": "ai_api_key",
                "value": "",
                "category": "ai",
                "description": "AI APIå¯†é’¥",
                "is_sensitive": True
            },
            {
                "key": "ai_base_url",
                "value": "https://api.openai.com/v1",
                "category": "ai",
                "description": "AI APIåŸºç¡€URL"
            },
            # è§†è§‰æ¨¡å‹è®¾ç½®
            {
                "key": "vision_provider",
                "value": "openai",
                "category": "vision",
                "description": "è§†è§‰æ¨¡å‹æä¾›å•† (openai/ollama/azure/custom)"
            },
            {
                "key": "vision_base_url",
                "value": "",
                "category": "vision",
                "description": "è§†è§‰æ¨¡å‹APIåŸºç¡€URLï¼ˆç•™ç©ºåˆ™ä½¿ç”¨ai_base_urlï¼‰"
            },
            {
                "key": "vision_api_key",
                "value": "",
                "category": "vision",
                "description": "è§†è§‰æ¨¡å‹APIå¯†é’¥ï¼ˆç•™ç©ºåˆ™ä½¿ç”¨ai_api_keyï¼‰",
                "is_sensitive": True
            },
            {
                "key": "ollama_vision_base_url",
                "value": "http://localhost:11434/v1",
                "category": "vision",
                "description": "Ollamaè§†è§‰æ¨¡å‹æœåŠ¡åœ°å€"
            },

            # ä¸Šä¼ è®¾ç½®
            {
                "key": "upload_max_file_size",
                "value": "524288000",
                "category": "upload",
                "description": "æœ€å¤§æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰"
            },
            {
                "key": "upload_allowed_types",
                "value": "pdf,docx,doc,png,jpg,jpeg",
                "category": "upload",
                "description": "å…è®¸çš„æ–‡ä»¶ç±»å‹"
            },
            # æˆªå›¾è®¾ç½®
            {
                "key": "screenshot_timeout",
                "value": "30",
                "category": "screenshot",
                "description": "æˆªå›¾è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰"
            },
            {
                "key": "screenshot_max_pages",
                "value": "20",
                "category": "screenshot",
                "description": "æœ€å¤§æˆªå›¾é¡µæ•°"
            },
            # OCRè®¾ç½® - é»˜è®¤å¯ç”¨EasyOCR
            {
                "key": "enable_docling_ocr",
                "value": "true",
                "category": "ocr",
                "description": "å¯ç”¨Docling OCR"
            },
            {
                "key": "docling_ocr_languages",
                "value": '["ch_sim", "en"]',
                "category": "ocr", 
                "description": "Docling OCRæ”¯æŒçš„è¯­è¨€"
            },
            {
                "key": "easyocr_enable",
                "value": "true",
                "category": "ocr",
                "description": "å¯ç”¨EasyOCR (é»˜è®¤å¼€å¯)"
            },
            {
                "key": "easyocr_model_path",
                "value": "/easyocr_models",
                "category": "ocr",
                "description": "EasyOCRæ¨¡å‹å­˜å‚¨è·¯å¾„"
            },
            {
                "key": "easyocr_languages",
                "value": '["ch_sim", "en"]',
                "category": "ocr",
                "description": "EasyOCRæ”¯æŒçš„è¯­è¨€"
            },
            {
                "key": "easyocr_use_gpu",
                "value": "false",
                "category": "ocr",
                "description": "EasyOCRæ˜¯å¦ä½¿ç”¨GPU"
            },
            {
                "key": "easyocr_download_proxy",
                "value": "",
                "category": "ocr",
                "description": "EasyOCRæ¨¡å‹ä¸‹è½½ä»£ç†"
            },
            # AIåˆ†æé«˜çº§è®¾ç½®
            {
                "key": "ai_analysis_enable_table_structure",
                "value": "false",
                "category": "ai_analysis",
                "description": "AIåˆ†ææ—¶æ˜¯å¦å¯ç”¨è¡¨æ ¼ç»“æ„åˆ†æ"
            },
            {
                "key": "ai_analysis_enable_picture_classification",
                "value": "false",
                "category": "ai_analysis",
                "description": "AIåˆ†ææ—¶æ˜¯å¦å¯ç”¨å›¾ç‰‡åˆ†ç±»"
            },
            {
                "key": "ai_analysis_enable_picture_description",
                "value": "false",
                "category": "ai_analysis",
                "description": "AIåˆ†ææ—¶æ˜¯å¦å¯ç”¨å›¾ç‰‡æè¿°"
            },
            {
                "key": "ai_analysis_generate_page_images",
                "value": "false",
                "category": "ai_analysis",
                "description": "AIåˆ†ææ—¶æ˜¯å¦ç”Ÿæˆé¡µé¢å›¾ç‰‡"
            },
            {
                "key": "ai_analysis_generate_picture_images",
                "value": "false",
                "category": "ai_analysis",
                "description": "AIåˆ†ææ—¶æ˜¯å¦ç”Ÿæˆå›¾ç‰‡æ–‡ä»¶"
            },
            # OCRç²¾åº¦ä¼˜åŒ–è®¾ç½®
            {
                "key": "docling_confidence_threshold",
                "value": "0.5",
                "category": "ocr",
                "description": "OCRè¯†åˆ«ç½®ä¿¡åº¦é˜ˆå€¼ (0.1-1.0)"
            },
            {
                "key": "docling_bitmap_area_threshold",
                "value": "0.05",
                "category": "ocr",
                "description": "OCRä½å›¾åŒºåŸŸé˜ˆå€¼ (0.01-0.1)"
            },
            {
                "key": "docling_force_full_page_ocr",
                "value": "false",
                "category": "ocr",
                "description": "æ˜¯å¦å¼ºåˆ¶å…¨é¡µOCR (å¯èƒ½æé«˜æ¼è¡Œæ£€æµ‹)"
            },
            {
                "key": "docling_recog_network",
                "value": "standard",
                "category": "ocr",
                "description": "OCRè¯†åˆ«ç½‘ç»œç±»å‹ (standard/fast)"
            },
            {
                "key": "docling_use_gpu",
                "value": "false",
                "category": "ocr",
                "description": "OCRæ˜¯å¦ä½¿ç”¨GPUåŠ é€Ÿ"
            },
            {
                "key": "docling_images_scale",
                "value": "2.0",
                "category": "ocr",
                "description": "OCRå›¾åƒç¼©æ”¾æ¯”ä¾‹ (1.0-3.0)"
            },
            # å›¾ç‰‡æè¿°é…ç½®
            {
                "key": "picture_description_prompt",
                "value": "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬æ–‡å­—ã€å›¾è¡¨ã€ç»“æ„ç­‰ä¿¡æ¯ã€‚",
                "category": "ai_analysis",
                "description": "Doclingå›¾ç‰‡æè¿°çš„æç¤ºè¯"
            }
        ]
        
        # æ£€æŸ¥SystemSettingsè¡¨æ˜¯å¦å­˜åœ¨
        try:
            # å°è¯•åˆ›å»ºè¡¨
            SystemSettings.__table__.create(bind=db.get_bind(), checkfirst=True)
            logger.info("åˆ›å»ºSystemSettingsè¡¨æˆåŠŸ")
        except Exception as table_err:
            logger.warning(f"åˆ›å»ºè¡¨å¤±è´¥æˆ–è¡¨å·²å­˜åœ¨: {str(table_err)}")
        
        # åˆ›å»ºè®¾ç½®
        created_count = 0
        for setting_info in default_settings:
            # æ£€æŸ¥è®¾ç½®æ˜¯å¦å·²å­˜åœ¨
            existing = db.query(SystemSettings).filter(
                SystemSettings.setting_key == setting_info["key"]
            ).first()
            
            if not existing:
                new_setting = SystemSettings(
                    setting_key=setting_info["key"],
                    setting_value=setting_info["value"],
                    category=setting_info["category"],
                    description=setting_info["description"],
                    is_sensitive=setting_info.get("is_sensitive", False)
                )
                db.add(new_setting)
                created_count += 1
        
        if created_count > 0:
            db.commit()
            logger.info(f"å·²åˆå§‹åŒ– {created_count} ä¸ªé»˜è®¤è®¾ç½®")
        
    except Exception as e:
        logger.error(f"åˆå§‹åŒ–é»˜è®¤è®¾ç½®å¤±è´¥: {str(e)}")
        db.rollback()

@app.post("/api/convert-to-word")
async def convert_files_to_word(
    files: List[UploadFile] = File(default=[]),
    document_title: str = Form(default="è½¬æ¢æ–‡æ¡£"),
    show_main_title: bool = Form(default=True),  # æ–°å¢ï¼šæ˜¯å¦æ˜¾ç¤ºä¸»æ ‡é¢˜
    show_file_titles: bool = Form(default=True),  # æ–°å¢ï¼šæ˜¯å¦æ˜¾ç¤ºæ¯ä¸ªæ–‡æ¡£çš„æ ‡é¢˜
    main_title_level: int = Form(default=1),  # æ–°å¢ï¼šä¸»æ ‡é¢˜å¤§çº²å±‚çº§
    file_title_level: int = Form(default=2),  # æ–°å¢ï¼šæ–‡ä»¶æ ‡é¢˜å¤§çº²å±‚çº§
    enable_watermark: bool = Form(default=False),
    file_watermark_settings: str = Form(default="[]"),  # æ–°å¢ï¼šæ¯ä¸ªæ–‡ä»¶çš„æ°´å°è®¾ç½®JSON
    file_page_break_settings: str = Form(default="[]"),  # æ–°å¢ï¼šæ¯ä¸ªæ–‡ä»¶çš„åˆ†é¡µç¬¦è®¾ç½®JSON
    file_page_number_settings: str = Form(default="[]"),  # æ–°å¢ï¼šæ¯ä¸ªæ–‡ä»¶çš„é¡µç è®¾ç½®JSON
    permanent_file_ids: str = Form(default="[]"),  # æ–°å¢ï¼šé€‰æ‹©çš„å¸¸é©»æ–‡ä»¶IDåˆ—è¡¨
    watermark_text: str = Form(default=""),
    watermark_font_size: int = Form(default=24),
    watermark_angle: int = Form(default=-45),
    watermark_opacity: int = Form(default=30),
    watermark_color: str = Form(default="#808080"),
    watermark_position: str = Form(default="center"),
    db: Session = Depends(get_db)
):
    """
    å°†ä¸Šä¼ çš„PDFå’Œå›¾ç‰‡æ–‡ä»¶è½¬æ¢ä¸ºWordæ–‡æ¡£
    """
    try:
        logger.info(f"æ”¶åˆ°è½¬æ¢è¯·æ±‚ - æ–‡æ¡£æ ‡é¢˜: {document_title}, æ–‡ä»¶æ•°é‡: {len(files)}")
        logger.info(f"æ ‡é¢˜é…ç½® - æ˜¾ç¤ºä¸»æ ‡é¢˜: {show_main_title}(å±‚çº§{main_title_level}), æ˜¾ç¤ºæ–‡ä»¶æ ‡é¢˜: {show_file_titles}(å±‚çº§{file_title_level})")
        logger.info(f"åˆ†é¡µè®¾ç½® - æ–‡ä»¶é—´åˆ†é¡µç¬¦: {file_page_break_settings}, é¡µç : {file_page_number_settings}")
        if enable_watermark and watermark_text:
            logger.info(f"æ°´å°é…ç½® - æ–‡å­—: {watermark_text}, å­—ä½“å¤§å°: {watermark_font_size}, è§’åº¦: {watermark_angle}Â°, é€æ˜åº¦: {watermark_opacity}%, é¢œè‰²: {watermark_color}, ä½ç½®: {watermark_position}")
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        temp_dir = "temp_conversions"
        os.makedirs(temp_dir, exist_ok=True)
        
        # å¤„ç†ä¸Šä¼ çš„ä¸´æ—¶æ–‡ä»¶
        file_paths = []
        file_names = []
        
        for file in files:
            file_path = os.path.join(temp_dir, file.filename)
            content = await file.read()
            if len(content) > MAX_UPLOAD_SIZE:
                return {"success": False, "message": f"å•ä¸ªæ–‡ä»¶ä¸èƒ½è¶…è¿‡{MAX_UPLOAD_SIZE_MB}MB"}
            with open(file_path, "wb") as buffer:
                buffer.write(content)
            file_paths.append(file_path)
            file_names.append(file.filename)
            
            # å°†ä¸´æ—¶æ–‡ä»¶ä¿å­˜åˆ°æ•°æ®åº“
            try:
                from models import ManagedFile
                import hashlib
                import mimetypes
                
                file_hash = hashlib.md5(content).hexdigest()
                mime_type, _ = mimetypes.guess_type(file.filename)
                if not mime_type:
                    mime_type = "application/octet-stream"
                
                temp_db_file = ManagedFile(
                    original_filename=file.filename,
                    display_name=file.filename,
                    storage_path=file_path,
                    file_type='pdf' if file.filename.lower().endswith('.pdf') else 'image',
                    mime_type=mime_type,
                    file_size=len(content),
                    file_hash=file_hash,
                    file_category="temporary_upload",  # ä¸Šä¼ çš„ä¸´æ—¶æ–‡ä»¶
                    category="document_conversion",
                    expires_at=datetime.now() + timedelta(days=30),  # 30å¤©è¿‡æœŸ
                    access_count=1,
                    last_accessed=datetime.now()
                )
                db.add(temp_db_file)
                
            except Exception as db_err:
                logger.warning(f"ä¿å­˜ä¸´æ—¶æ–‡ä»¶åˆ°æ•°æ®åº“å¤±è´¥: {db_err}")
        
        # å¤„ç†é€‰æ‹©çš„å¸¸é©»æ–‡ä»¶
        try:
            permanent_ids = json.loads(permanent_file_ids)
            if permanent_ids:
                from models import ManagedFile
                
                permanent_files = db.query(ManagedFile).filter(
                    ManagedFile.id.in_(permanent_ids),
                    ManagedFile.file_category == "permanent",
                    ManagedFile.is_archived == False
                ).all()
                
                for perm_file in permanent_files:
                    if os.path.exists(perm_file.storage_path):
                        # å¤åˆ¶å¸¸é©»æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•ï¼Œç”¨äºå¤„ç†
                        temp_copy_filename = f"copy_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{perm_file.original_filename}"
                        temp_copy_path = os.path.join(temp_dir, temp_copy_filename)
                        shutil.copy2(perm_file.storage_path, temp_copy_path)
                        file_paths.append(temp_copy_path)
                        file_names.append(perm_file.original_filename)
                        
                        # å°†å¤åˆ¶çš„æ–‡ä»¶ä¿å­˜ä¸ºä¸´æ—¶ç”Ÿæˆæ–‡ä»¶ï¼ˆ180å¤©åæ¸…ç†ï¼‰
                        try:
                            import hashlib
                            with open(temp_copy_path, 'rb') as f:
                                file_hash = hashlib.md5(f.read()).hexdigest()
                            
                            copy_db_file = ManagedFile(
                                original_filename=perm_file.original_filename,
                                display_name=f"å¤åˆ¶_{perm_file.display_name}",
                                storage_path=temp_copy_path,
                                file_type=perm_file.file_type,
                                mime_type=perm_file.mime_type,
                                file_size=perm_file.file_size,
                                file_hash=file_hash,
                                file_category="temporary_generated",  # ç”Ÿæˆçš„ä¸´æ—¶æ–‡ä»¶
                                category="permanent_file_copy",
                                description=f"å¸¸é©»æ–‡ä»¶çš„å¤„ç†å‰¯æœ¬: {perm_file.display_name}",
                                expires_at=datetime.now() + timedelta(days=180),  # 180å¤©è¿‡æœŸ
                                access_count=1,
                                last_accessed=datetime.now()
                            )
                            db.add(copy_db_file)
                        except Exception as copy_db_err:
                            logger.warning(f"ä¿å­˜å¸¸é©»æ–‡ä»¶å‰¯æœ¬åˆ°æ•°æ®åº“å¤±è´¥: {copy_db_err}")
                        
                        # æ›´æ–°åŸå§‹å¸¸é©»æ–‡ä»¶çš„è®¿é—®ç»Ÿè®¡
                        perm_file.access_count += 1
                        perm_file.last_accessed = datetime.now()
                        
                        logger.info(f"å¤åˆ¶å¸¸é©»æ–‡ä»¶ç”¨äºå¤„ç†: {perm_file.display_name} -> {temp_copy_filename}")
                    else:
                        logger.warning(f"å¸¸é©»æ–‡ä»¶ä¸å­˜åœ¨: {perm_file.storage_path}")
                
        except Exception as perm_err:
            logger.warning(f"å¤„ç†å¸¸é©»æ–‡ä»¶å¤±è´¥: {perm_err}")
        
        # æäº¤æ•°æ®åº“æ›´æ”¹
        try:
            db.commit()
        except Exception as commit_err:
            logger.warning(f"æäº¤æ•°æ®åº“æ›´æ”¹å¤±è´¥: {commit_err}")
            db.rollback()
        
        # åˆ›å»ºWordæ–‡æ¡£ï¼ˆç›´æ¥åœ¨è¿™é‡Œåˆ›å»ºï¼Œä¸ä½¿ç”¨convert_files_to_wordæ–¹æ³•ï¼‰
        doc = Document()
        
        # è®¾ç½®é¡µé¢ä¸ºA4å¤§å°
        from docx.shared import Inches
        from docx.enum.section import WD_ORIENTATION
        section = doc.sections[0]
        section.page_width = Inches(8.27)  # A4å®½åº¦ (210mm)
        section.page_height = Inches(11.69)  # A4é«˜åº¦ (297mm)
        section.orientation = WD_ORIENTATION.PORTRAIT  # çºµå‘
        section.left_margin = Inches(1.0)  # 1è‹±å¯¸é¡µè¾¹è·
        section.right_margin = Inches(1.0)
        section.top_margin = Inches(1.0)
        section.bottom_margin = Inches(1.0)
        
        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦åˆ›å»ºä¸»æ ‡é¢˜
        if show_main_title:
            format_heading(doc, document_title, level=main_title_level, center=True)
        
        # è§£ææ¯ä¸ªæ–‡ä»¶çš„æ°´å°è®¾ç½®
        try:
            file_watermark_list = json.loads(file_watermark_settings)
        except:
            file_watermark_list = []
        
        # è§£ææ¯ä¸ªæ–‡ä»¶çš„åˆ†é¡µç¬¦è®¾ç½®
        try:
            file_page_break_list = json.loads(file_page_break_settings)
        except:
            file_page_break_list = []
        
        # è§£ææ¯ä¸ªæ–‡ä»¶çš„é¡µç è®¾ç½®
        try:
            file_page_number_list = json.loads(file_page_number_settings)
        except:
            file_page_number_list = []
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•æ–‡ä»¶éœ€è¦é¡µç 
        has_any_page_numbers = any(file_page_number_list) if file_page_number_list else False
        
        # å¦‚æœæœ‰æ–‡ä»¶éœ€è¦é¡µç ï¼Œè®¾ç½®é¡µç æ ·å¼
        if has_any_page_numbers:
            # æ·»åŠ é¡µè„šå’Œé¡µç 
            from docx.oxml.shared import qn
            from docx.oxml import OxmlElement
            
            section = doc.sections[0]
            footer = section.footer
            footer_para = footer.paragraphs[0]
            footer_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
            
            # æ·»åŠ é¡µç åŸŸ
            fldChar1 = OxmlElement('w:fldChar')
            fldChar1.set(qn('w:fldCharType'), 'begin')
            
            instrText = OxmlElement('w:instrText')
            instrText.text = 'PAGE'
            
            fldChar2 = OxmlElement('w:fldChar')
            fldChar2.set(qn('w:fldCharType'), 'end')
            
            run = footer_para.runs[0] if footer_para.runs else footer_para.add_run()
            run._element.append(fldChar1)
            run._element.append(instrText)
            run._element.append(fldChar2)
            
            logger.info("é¡µç è®¾ç½®å®Œæˆ")
        
        processed_files = []
        results = []
        total_files = len(file_paths)
        
        if total_files == 0:
            return {"success": False, "message": "æ²¡æœ‰å¯å¤„ç†çš„æ–‡ä»¶"}
        
        # é€ä¸ªå¤„ç†æ–‡ä»¶
        for i, file_path in enumerate(file_paths):
            filename = os.path.basename(file_path)
            file_ext = os.path.splitext(filename)[1].lower()
            is_last_file = (i == total_files - 1)
            
            # æ£€æŸ¥å½“å‰æ–‡ä»¶æ˜¯å¦éœ€è¦æ°´å°
            file_needs_watermark = False
            if i < len(file_watermark_list):
                file_needs_watermark = file_watermark_list[i]
            else:
                file_needs_watermark = enable_watermark  # é»˜è®¤è·Ÿéšå…¨å±€è®¾ç½®
            
            # æ£€æŸ¥å½“å‰æ–‡ä»¶æ˜¯å¦éœ€è¦åˆ†é¡µç¬¦
            file_needs_page_break = True  # é»˜è®¤éœ€è¦åˆ†é¡µç¬¦
            if i < len(file_page_break_list):
                file_needs_page_break = file_page_break_list[i]
            
            # æ£€æŸ¥å½“å‰æ–‡ä»¶æ˜¯å¦éœ€è¦é¡µç 
            file_needs_page_numbers = False  # é»˜è®¤ä¸éœ€è¦é¡µç 
            if i < len(file_page_number_list):
                file_needs_page_numbers = file_page_number_list[i]
            
            try:
                if file_ext == '.pdf':
                    # å¦‚æœæ­¤æ–‡ä»¶éœ€è¦æ°´å°ï¼Œå…ˆç»™PDFæ·»åŠ æ°´å°
                    pdf_to_process = file_path
                    if file_needs_watermark and watermark_text:
                        pdf_to_process = add_watermark_to_pdf(file_path, watermark_text, watermark_font_size,
                                                            watermark_angle, watermark_opacity, 
                                                            watermark_color, watermark_position)
                        logger.info(f"ä¸ºæ–‡ä»¶ {filename} æ·»åŠ æ°´å°: {watermark_text}")
                    else:
                        logger.info(f"æ–‡ä»¶ {filename} è·³è¿‡æ°´å°")
                    
                    # æ ¹æ®åˆ†é¡µç¬¦è®¾ç½®å†³å®šæ˜¯å¦ä¸ºæœ€åä¸€ä¸ªæ–‡ä»¶
                    effective_is_last_file = is_last_file if file_needs_page_break else True  # å¦‚æœä¸æ·»åŠ åˆ†é¡µç¬¦ï¼Œæ¯ä¸ªæ–‡ä»¶éƒ½å½“ä½œæœ€åä¸€ä¸ªæ–‡ä»¶å¤„ç†
                    
                    result = await docling_processor.process_pdf_with_docling(
                        pdf_to_process, doc, filename, None, show_file_titles, file_title_level, effective_is_last_file
                    )
                    processed_files.append(f"PDF: {filename}{' (å«æ°´å°)' if file_needs_watermark and watermark_text else ''}")
                    results.append(result)
                elif file_ext in ['.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff']:
                    # æ ¹æ®åˆ†é¡µç¬¦è®¾ç½®å†³å®šæ˜¯å¦ä¸ºæœ€åä¸€ä¸ªæ–‡ä»¶
                    effective_is_last_file = is_last_file if file_needs_page_break else True  # å¦‚æœä¸æ·»åŠ åˆ†é¡µç¬¦ï¼Œæ¯ä¸ªæ–‡ä»¶éƒ½å½“ä½œæœ€åä¸€ä¸ªæ–‡ä»¶å¤„ç†
                    
                    # å›¾ç‰‡æ–‡ä»¶çš„æ°´å°å¤„ç†ï¼ˆç›®å‰æš‚ä¸æ”¯æŒï¼Œä»…è®°å½•çŠ¶æ€ï¼‰
                    result = await docling_processor.process_image(
                        file_path, doc, filename, None, show_file_titles, file_title_level, effective_is_last_file
                    )
                    processed_files.append(f"å›¾ç‰‡: {filename}{' (å›¾ç‰‡æš‚ä¸æ”¯æŒæ°´å°)' if file_needs_watermark else ''}")
                    results.append(result)
                else:
                    processed_files.append(f"ä¸æ”¯æŒçš„æ ¼å¼: {filename}")
                    results.append({
                        "success": False,
                        "message": f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}"
                    })
            except Exception as file_error:
                logger.error(f"å¤„ç†æ–‡ä»¶ {filename} å¤±è´¥: {file_error}")
                processed_files.append(f"å¤±è´¥: {filename}")
                results.append({
                    "success": False,
                    "message": f"å¤„ç†å¤±è´¥: {str(file_error)}"
                })
        
        # æ–‡æ¡£å†…å®¹ç”Ÿæˆå®Œæˆï¼Œæ°´å°å·²åœ¨PDFé˜¶æ®µå¤„ç†
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶åå’Œè·¯å¾„
        sanitized_title = "".join(c for c in document_title if c.isalnum() or c in " -_").strip()
        if not sanitized_title:
            sanitized_title = f"è½¬æ¢æ–‡æ¡£_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        output_filename = f"{sanitized_title}.docx"
        output_path = os.path.join("/app/generated_docs", output_filename)
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs("/app/generated_docs", exist_ok=True)
        
        # ä¿å­˜æ–‡æ¡£
        doc.save(output_path)
        logger.info(f"æ–‡æ¡£ä¿å­˜æˆåŠŸ: {output_path}")
        
        # å°†ç”Ÿæˆçš„æ–‡æ¡£ä¿å­˜åˆ°æ•°æ®åº“ç®¡ç†
        try:
            import hashlib
            with open(output_path, 'rb') as f:
                file_hash = hashlib.md5(f.read()).hexdigest()
            
            file_size = os.path.getsize(output_path)
            
            generated_doc = ManagedFile(
                original_filename=output_filename,
                display_name=f"è½¬æ¢æ–‡æ¡£_{document_title}",
                storage_path=output_path,
                file_type='document',
                mime_type='application/vnd.openxmlformats-officedocument.wordprocessingml.document',
                file_size=file_size,
                file_hash=file_hash,
                file_category="temporary_generated",  # ç”Ÿæˆçš„ä¸´æ—¶æ–‡ä»¶
                category="conversion_result",
                description=f"ç”±{len(file_paths)}ä¸ªæ–‡ä»¶è½¬æ¢ç”Ÿæˆ",
                expires_at=datetime.now() + timedelta(days=180),  # 180å¤©è¿‡æœŸ
                access_count=0,
                last_accessed=datetime.now()
            )
            db.add(generated_doc)
            db.commit()
            logger.info(f"ç”Ÿæˆæ–‡æ¡£å·²ä¿å­˜åˆ°æ•°æ®åº“ç®¡ç†: {output_filename}")
        except Exception as db_save_err:
            logger.warning(f"ä¿å­˜ç”Ÿæˆæ–‡æ¡£åˆ°æ•°æ®åº“å¤±è´¥: {db_save_err}")
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        import shutil
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
        
        # ç»Ÿè®¡å¤„ç†ç»“æœ
        failed_count = sum(1 for r in results if not r.get("success", True))
        success_count = len(results) - failed_count
        
        # è®°å½•å†å²
        china_tz = pytz.timezone('Asia/Shanghai')
        china_time = datetime.now(china_tz)
        
        history_item = {
            "document_title": document_title,
            "output_file": output_filename,
            "file_count": len(file_paths),
            "created_at": china_time.strftime('%Y-%m-%d %H:%M:%S'),
            "status": "success" if failed_count == 0 else "partial",
            "processed_files": processed_files,
            "success_count": success_count,
            "failed_count": failed_count
        }
        
        try:
            if os.path.exists(HISTORY_FILE):
                with open(HISTORY_FILE, "r", encoding="utf-8") as f:
                    history = json.load(f)
            else:
                history = []
            history.insert(0, history_item)
            with open(HISTORY_FILE, "w", encoding="utf-8") as f:
                json.dump(history[:100], f, ensure_ascii=False, indent=2)
        except Exception as hist_err:
            logger.error(f"å†™å…¥è½¬æ¢å†å²å¤±è´¥: {hist_err}")
        
        return {
            "success": failed_count == 0,
            "message": f"æ–‡ä»¶è½¬æ¢å®Œæˆ - æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count}",
            "output_file": output_filename,
            "processed_files": processed_files,
            "download_url": f"/api/download/{output_filename}",
            "success_count": success_count,
            "failed_count": failed_count
        }
        
    except Exception as e:
        logger.error(f"æ–‡ä»¶è½¬æ¢å¤±è´¥: {str(e)}")
        
        # è®°å½•å¼‚å¸¸å¤±è´¥å†å²
        try:
            china_tz = pytz.timezone('Asia/Shanghai')
            china_time = datetime.now(china_tz)
            
            history_item = {
                "document_title": document_title,
                "output_file": "",
                "file_count": len(files),
                "created_at": china_time.strftime('%Y-%m-%d %H:%M:%S'),
                "status": "error",
                "error_message": str(e),
                "processed_files": [f"é”™è¯¯: {file.filename}" for file in files]
            }
            
            if os.path.exists(HISTORY_FILE):
                with open(HISTORY_FILE, "r", encoding="utf-8") as f:
                    history = json.load(f)
            else:
                history = []
            history.insert(0, history_item)
            with open(HISTORY_FILE, "w", encoding="utf-8") as f:
                json.dump(history[:100], f, ensure_ascii=False, indent=2)
        except Exception as hist_err:
            logger.error(f"å†™å…¥å¼‚å¸¸å†å²å¤±è´¥: {hist_err}")
        
        return {"success": False, "message": f"è½¬æ¢å¤±è´¥: {str(e)}"}

@app.get("/api/convert-history")
async def get_convert_history():
    try:
        if os.path.exists(HISTORY_FILE):
            with open(HISTORY_FILE, "r", encoding="utf-8") as f:
                history = json.load(f)
        else:
            history = []
        return {"success": True, "history": history}
    except Exception as e:
        return {"success": False, "message": f"è¯»å–å†å²å¤±è´¥: {str(e)}"}

@app.delete("/api/convert-history")
async def clear_convert_history(db: Session = Depends(get_db)):
    """æ¸…ç©ºè½¬æ¢å†å²å¹¶å°†ç”Ÿæˆçš„æ–‡ä»¶åŠ å…¥æ¸…ç†åˆ—è¡¨"""
    try:
        # å°†ç°æœ‰ç”Ÿæˆçš„æ–‡æ¡£æ–‡ä»¶åŠ å…¥åˆ°æ•°æ®åº“ç®¡ç†
        generated_docs_dir = "/app/generated_docs"
        if os.path.exists(generated_docs_dir):
            # è·å–æ‰€æœ‰.docxæ–‡ä»¶
            docx_files = [f for f in os.listdir(generated_docs_dir) if f.endswith('.docx')]
            
            for filename in docx_files:
                file_path = os.path.join(generated_docs_dir, filename)
                if os.path.exists(file_path):
                    try:
                        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²åœ¨æ•°æ®åº“ä¸­
                        import hashlib
                        with open(file_path, 'rb') as f:
                            file_hash = hashlib.md5(f.read()).hexdigest()
                        
                        existing_file = db.query(ManagedFile).filter(
                            ManagedFile.file_hash == file_hash
                        ).first()
                        
                        if not existing_file:
                            # æ·»åŠ åˆ°æ•°æ®åº“ç®¡ç†ä¸­ï¼Œæ ‡è®°ä¸ºç”Ÿæˆçš„ä¸´æ—¶æ–‡ä»¶
                            import mimetypes
                            file_size = os.path.getsize(file_path)
                            mime_type, _ = mimetypes.guess_type(filename)
                            if not mime_type:
                                mime_type = "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
                            
                            generated_file = ManagedFile(
                                original_filename=filename,
                                display_name=filename,
                                storage_path=file_path,
                                file_type='document',
                                mime_type=mime_type,
                                file_size=file_size,
                                file_hash=file_hash,
                                file_category="temporary_generated",  # ç”Ÿæˆçš„ä¸´æ—¶æ–‡ä»¶
                                category="generated_document",
                                description="ç³»ç»Ÿç”Ÿæˆçš„è½¬æ¢æ–‡æ¡£",
                                expires_at=datetime.now() + timedelta(days=180),  # 180å¤©è¿‡æœŸ
                                access_count=0,
                                last_accessed=datetime.now()
                            )
                            db.add(generated_file)
                            logger.info(f"å°†ç”Ÿæˆæ–‡æ¡£åŠ å…¥ç®¡ç†: {filename}")
                        
                    except Exception as db_err:
                        logger.warning(f"å¤„ç†ç”Ÿæˆæ–‡æ¡£å¤±è´¥ {filename}: {db_err}")
        
        # æäº¤æ•°æ®åº“æ›´æ”¹
        try:
            db.commit()
        except Exception as commit_err:
            logger.warning(f"æäº¤æ•°æ®åº“æ›´æ”¹å¤±è´¥: {commit_err}")
            db.rollback()
        
        # æ¸…ç©ºå†å²æ–‡ä»¶
        if os.path.exists(HISTORY_FILE):
            with open(HISTORY_FILE, "w", encoding="utf-8") as f:
                json.dump([], f, ensure_ascii=False, indent=2)
        
        return {"success": True, "message": "è½¬æ¢å†å²å·²æ¸…ç©ºï¼Œç”Ÿæˆçš„æ–‡æ¡£å·²åŠ å…¥180å¤©æ¸…ç†åˆ—è¡¨"}
    except Exception as e:
        logger.error(f"æ¸…ç©ºå†å²å¤±è´¥: {str(e)}")
        return {"success": False, "message": f"æ¸…ç©ºå†å²å¤±è´¥: {str(e)}"}

# ==================
# Project API
# ==================
@app.post("/api/projects", response_model=BaseResponse)
def create_project(project: ProjectCreate, db: Session = Depends(get_db)):
    try:
        db_project = Project(**project.model_dump())
        db.add(db_project)
        db.commit()
        db.refresh(db_project)
        return {"success": True, "message": "é¡¹ç›®åˆ›å»ºæˆåŠŸ"}
    except Exception as e:
        db.rollback()
        logger.error(f"åˆ›å»ºé¡¹ç›®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail="åˆ›å»ºé¡¹ç›®å¤±è´¥")

@app.get("/api/projects", response_model=ProjectResponse)
def get_projects(db: Session = Depends(get_db)):
    projects = db.query(Project).order_by(Project.created_at.desc()).all()
    return {
        "success": True,
        "message": "è·å–é¡¹ç›®åˆ—è¡¨æˆåŠŸ",
        "projects": projects
    }

def create_tables():
    try:
        models.Base.metadata.create_all(bind=engine)
        db = SessionLocal()
        
        # æ£€æŸ¥å¹¶åˆå§‹åŒ–SystemSettingsè¡¨
        if db.query(models.SystemSettings).count() == 0:
            default_settings = models.SystemSettings(
                company_name="é»˜è®¤å…¬å¸åç§°",
                company_address="é»˜è®¤å…¬å¸åœ°å€",
                contact_person="é»˜è®¤è”ç³»äºº",
                contact_phone="1234567890",
                contact_email="default@example.com"
            )
            db.add(default_settings)
            db.commit()
            logger.info("åˆ›å»ºSystemSettingsè¡¨æˆåŠŸï¼Œå¹¶æ’å…¥äº†é»˜è®¤è®¾ç½®")

        # æ£€æŸ¥å¹¶åˆå§‹åŒ–åŸºç¡€æ•°æ®è¡¨
        if db.query(models.Brand).count() == 0 and \
           db.query(models.BusinessField).count() == 0 and \
           db.query(models.Award).count() == 0 and \
           db.query(models.Performance).count() == 0:
            init_base_data(db)
            logger.info("åˆå§‹åŒ–åŸºç¡€æ•°æ®è¡¨æˆåŠŸ")
            
        if db.query(models.Project).count() == 0:
            logger.info("æ£€æµ‹åˆ°Projectè¡¨ä¸ºç©ºï¼Œè·³è¿‡åˆå§‹åŒ–æ•°æ®ã€‚")

    except Exception as e:
        logger.error(f"åˆ›å»ºè¡¨æˆ–åˆå§‹åŒ–æ•°æ®å¤±è´¥: {e}")
    finally:
        db.close()

def add_watermark_to_pdf(pdf_path: str, text: str, font_size: int, angle: int, opacity: int, color: str, position: str) -> str:
    """åœ¨PDFä¸­æ·»åŠ æ°´å°ï¼Œè¿”å›å¤„ç†åçš„PDFè·¯å¾„"""
    try:
        import fitz  # PyMuPDF
        import math
        
        # è§’åº¦å–åï¼Œä¿è¯ä¸å‰ç«¯é¢„è§ˆä¸€è‡´
        angle = -angle
        
        # æ‰“å¼€PDF
        doc = fitz.open(pdf_path)
        
        # è§£æé¢œè‰²
        color_hex = color.lstrip('#')
        if len(color_hex) == 6:
            r, g, b = tuple(int(color_hex[i:i+2], 16) for i in (0, 2, 4))
        else:
            r, g, b = 128, 128, 128
        
        # é¢œè‰²å€¼è½¬æ¢ä¸º0-1èŒƒå›´
        color_r = r / 255.0
        color_g = g / 255.0  
        color_b = b / 255.0
        
        # é€æ˜åº¦ï¼ˆ0-1èŒƒå›´ï¼‰
        opacity_ratio = opacity / 100.0
        
        logger.info(f"æ°´å°é¢œè‰²å¤„ç†: æ¥æ”¶åˆ°é¢œè‰² '{color}' -> è§£æhex '{color_hex}' -> åŸå§‹RGB({r},{g},{b}) -> é€æ˜åº¦{opacity}% -> æœ€ç»ˆRGB({color_r:.2f},{color_g:.2f},{color_b:.2f})")
        
        # éå†æ¯ä¸€é¡µ
        for page_num in range(len(doc)):
            page = doc[page_num]
            
            # è·å–é¡µé¢å°ºå¯¸
            page_rect = page.rect
            page_width = page_rect.width
            page_height = page_rect.height
            
            # è®¡ç®—æ–‡æœ¬å°ºå¯¸ä»¥æ­£ç¡®å±…ä¸­
            try:
                # è·å–æ–‡æœ¬å®½åº¦ï¼ˆPyMuPDFè¿”å›åƒç´ å®½åº¦ï¼‰
                text_width = fitz.get_text_length(text, fontname="china-ss", fontsize=font_size)
                # æ›´ç²¾ç¡®çš„æ–‡æœ¬é«˜åº¦è®¡ç®—
                text_height = font_size * 1.2  # è€ƒè™‘å­—ä½“çš„ä¸Šå‡éƒ¨å’Œä¸‹é™éƒ¨
            except:
                # å¦‚æœæµ‹é‡å¤±è´¥ï¼Œä½¿ç”¨ä¼°ç®—å€¼
                text_width = len(text) * font_size * 0.6  # ä¼°ç®—ï¼šæ¯ä¸ªå­—ç¬¦çº¦0.6å€å­—ä½“å¤§å°
                text_height = font_size * 1.2
            
            # è®¡ç®—çœŸæ­£çš„é¡µé¢ä¸­å¿ƒç‚¹
            page_center_x = page_width / 2
            page_center_y = page_height / 2
            
            # æ ¹æ®ä½ç½®è®¡ç®—æ°´å°ä½ç½®ï¼ˆç¡®ä¿çœŸæ­£å±…ä¸­ï¼‰
            if position == "center":
                # ç›´æ¥ä½¿ç”¨é¡µé¢ä¸­å¿ƒä½œä¸ºæ–‡æœ¬çš„ä¸­å¿ƒç‚¹
                # è¿™æ ·æ— è®ºå­—ä½“å¤§å°å¦‚ä½•å˜åŒ–ï¼Œæ°´å°éƒ½ä¼šçœŸæ­£å±…ä¸­
                x = page_center_x  # æ–‡æœ¬ä¸­å¿ƒç‚¹çš„xåæ ‡
                y = page_center_y  # æ–‡æœ¬ä¸­å¿ƒç‚¹çš„yåæ ‡
            elif position == "top-left":
                x = 100 + text_width / 2  # è®©æ–‡æœ¬ä¸­å¿ƒè·ç¦»è¾¹ç¼˜100åƒç´ 
                y = 100 + text_height / 2
            elif position == "top-right":
                x = page_width - 100 - text_width / 2
                y = 100 + text_height / 2
            elif position == "bottom-left":
                x = 100 + text_width / 2
                y = page_height - 100 - text_height / 2
            elif position == "bottom-right":
                x = page_width - 100 - text_width / 2
                y = page_height - 100 - text_height / 2
            else:  # é»˜è®¤å±…ä¸­
                x = page_center_x
                y = page_center_y
            
            logger.info(f"æ°´å°ä½ç½®è®¡ç®—: é¡µé¢({page_width:.1f}x{page_height:.1f}), æ–‡æœ¬å°ºå¯¸({text_width:.1f}x{text_height:.1f}), ä¸­å¿ƒç‚¹({x:.1f},{y:.1f})")
            
            try:
                if angle != 0:
                    # å°è¯•ä½¿ç”¨PyMuPDFçš„æ—‹è½¬åŠŸèƒ½
                    angle_rad = math.radians(angle)
                    
                    try:
                        # æ–¹æ³•1: ä½¿ç”¨æ–‡æœ¬æ—‹è½¬çŸ©é˜µ
                        # åˆ›å»ºæ—‹è½¬å˜æ¢çŸ©é˜µ
                        cos_a = math.cos(angle_rad)
                        sin_a = math.sin(angle_rad)
                        rotation_matrix = fitz.Matrix(cos_a, sin_a, -sin_a, cos_a, 0, 0)
                        
                        # ä½¿ç”¨é¢„è®¾çš„ä¸­å¿ƒç‚¹ä½œä¸ºæ—‹è½¬ä¸­å¿ƒ
                        # x, y å·²ç»æ˜¯æ–‡æœ¬ä¸­å¿ƒç‚¹ï¼Œç›´æ¥ä½¿ç”¨
                        transform_point = fitz.Point(x, y)
                        
                        # ä½¿ç”¨å˜æ¢çŸ©é˜µæ’å…¥æ–‡æœ¬ï¼Œä½¿ç”¨RGBé¢œè‰²+fill_opacityå‚æ•°
                        page.insert_text(
                            transform_point,
                            text,
                            fontsize=font_size,
                            color=(color_r, color_g, color_b),  # RGBé¢œè‰²
                            fill_opacity=opacity_ratio,  # é€æ˜åº¦å‚æ•°
                            fontname="china-ss",  # ä¿®å¤ï¼šä½¿ç”¨æ€æºå®‹ä½“æ”¯æŒä¸­æ–‡
                            morph=(transform_point, rotation_matrix)  # ä½¿ç”¨morphå‚æ•°è¿›è¡Œæ—‹è½¬
                        )
                        logger.info(f"é¡µé¢ {page_num + 1} è§’åº¦æ°´å°æ·»åŠ æˆåŠŸï¼ˆè§’åº¦: {angle}Â°, é¢œè‰²: {color}ï¼‰")
                        
                    except Exception as rotate_error:
                        logger.warning(f"æ—‹è½¬çŸ©é˜µå¤±è´¥ï¼Œå°è¯•å­—ç¬¦åˆ†å¸ƒ: {rotate_error}")
                        
                        # æ–¹æ³•2: å­—ç¬¦åˆ†å¸ƒæ¨¡æ‹Ÿè§’åº¦
                        cos_a = math.cos(angle_rad)
                        sin_a = math.sin(angle_rad)
                        
                        # è®¡ç®—å­—ç¬¦åˆ†å¸ƒçš„ç²¾ç¡®é—´è·
                        char_spacing = text_width / len(text) if len(text) > 0 else font_size * 0.6
                        
                        # è®¡ç®—èµ·å§‹ä½ç½®ï¼Œä½¿æ•´ä¸ªæ–‡æœ¬ä»¥(x,y)ä¸ºä¸­å¿ƒ
                        start_offset_x = -text_width / 2
                        start_offset_y = 0  # å‚ç›´å±…ä¸­ï¼ŒåŸºçº¿åç§»ä¸º0
                        
                        # åˆ†åˆ«æ”¾ç½®æ¯ä¸ªå­—ç¬¦
                        for i, char in enumerate(text):
                            # è®¡ç®—å­—ç¬¦åœ¨æ—‹è½¬å‰çš„ç›¸å¯¹ä½ç½®
                            char_offset_x = start_offset_x + (i + 0.5) * char_spacing
                            char_offset_y = start_offset_y
                            
                            # åº”ç”¨æ—‹è½¬å˜æ¢
                            rotated_x = char_offset_x * cos_a - char_offset_y * sin_a
                            rotated_y = char_offset_x * sin_a + char_offset_y * cos_a
                            
                            # è®¡ç®—æœ€ç»ˆä½ç½®ï¼ˆä»¥é¢„è®¾çš„ä¸­å¿ƒç‚¹ä¸ºæ—‹è½¬ä¸­å¿ƒï¼‰
                            char_x = x + rotated_x
                            char_y = y + rotated_y
                            
                            try:
                                page.insert_text(
                                    fitz.Point(char_x, char_y),
                                    char,
                                    fontsize=font_size,
                                    color=(color_r, color_g, color_b),  # RGBé¢œè‰²
                                    fill_opacity=opacity_ratio,  # é€æ˜åº¦å‚æ•°
                                    fontname="china-ss"  # ä¿®å¤ï¼šä½¿ç”¨æ€æºå®‹ä½“æ”¯æŒä¸­æ–‡
                                )
                            except Exception as char_error:
                                logger.warning(f"å­—ç¬¦ '{char}' æ·»åŠ å¤±è´¥: {char_error}")
                                continue
                        
                        logger.info(f"é¡µé¢ {page_num + 1} ä½¿ç”¨å­—ç¬¦åˆ†å¸ƒæ¨¡æ‹Ÿè§’åº¦ï¼ˆè§’åº¦: {angle}Â°, é¢œè‰²: {color}ï¼‰")
                        
                else:
                    # æ— æ—‹è½¬ï¼Œéœ€è¦è®¡ç®—æ–‡æœ¬èµ·å§‹ä½ç½®ï¼ˆå·¦ä¸‹è§’ï¼‰
                    # x, yæ˜¯æ–‡æœ¬ä¸­å¿ƒç‚¹ï¼Œéœ€è¦è½¬æ¢ä¸ºæ–‡æœ¬èµ·å§‹ç‚¹
                    text_start_x = x - text_width / 2
                    text_start_y = y + font_size / 3  # è°ƒæ•´åŸºçº¿ä½ç½®
                    
                    # ç›´æ¥æ·»åŠ æ–‡æœ¬ï¼Œä½¿ç”¨RGBé¢œè‰²+fill_opacityå‚æ•°
                    page.insert_text(
                        fitz.Point(text_start_x, text_start_y),
                        text,
                        fontsize=font_size,
                        color=(color_r, color_g, color_b),  # RGBé¢œè‰²
                        fill_opacity=opacity_ratio,  # é€æ˜åº¦å‚æ•°
                        fontname="china-ss"  # ä¿®å¤ï¼šä½¿ç”¨æ€æºå®‹ä½“æ”¯æŒä¸­æ–‡
                    )
                    logger.info(f"é¡µé¢ {page_num + 1} æ°´å°æ·»åŠ æˆåŠŸï¼ˆæ— æ—‹è½¬, é¢œè‰²: {color}ï¼‰")
                    
            except Exception as text_error:
                logger.error(f"é¡µé¢ {page_num + 1} æ°´å°æ·»åŠ å¤±è´¥: {text_error}")
                # é™çº§å¤„ç†ï¼šä½¿ç”¨æœ€ç®€å•çš„æ–¹æ³•
                try:
                    # é™çº§å¤„ç†ï¼šä½¿ç”¨ç®€å•çš„å±…ä¸­è®¡ç®—
                    fallback_x = x - text_width / 2
                    fallback_y = y + font_size / 3
                    
                    # é™çº§å¤„ç†ä¹Ÿä½¿ç”¨RGBé¢œè‰²+fill_opacityå‚æ•°
                    page.insert_text(
                        fitz.Point(fallback_x, fallback_y),
                        text,
                        fontsize=font_size,
                        color=(color_r, color_g, color_b),  # RGBé¢œè‰²
                        fill_opacity=opacity_ratio,  # é€æ˜åº¦å‚æ•°
                        fontname="china-ss"  # ä¿®å¤ï¼šä½¿ç”¨æ€æºå®‹ä½“æ”¯æŒä¸­æ–‡
                    )
                    logger.info(f"é¡µé¢ {page_num + 1} ä½¿ç”¨é™çº§æ–¹æ³•æ·»åŠ æ°´å°")
                except Exception as fallback_error:
                    logger.error(f"é¡µé¢ {page_num + 1} æ‰€æœ‰æ°´å°æ–¹æ³•éƒ½å¤±è´¥: {fallback_error}")
        
        # ä¿å­˜å¤„ç†åçš„PDF
        watermarked_pdf_path = pdf_path.replace('.pdf', '_watermarked.pdf')
        doc.save(watermarked_pdf_path)
        doc.close()
        
        logger.info(f"PDFæ°´å°æ·»åŠ æˆåŠŸ: {watermarked_pdf_path}")
        return watermarked_pdf_path
        
    except Exception as e:
        logger.error(f"PDFæ°´å°æ·»åŠ å¤±è´¥: {e}")
        return pdf_path  # å¦‚æœå¤±è´¥ï¼Œè¿”å›åŸPDFè·¯å¾„

def add_watermark_to_existing_document(doc, text, font_size, angle, opacity, color, position):
    """åºŸå¼ƒ - ç°åœ¨åœ¨PDFé˜¶æ®µå¤„ç†æ°´å°"""
    pass

# æ·»åŠ åˆ†ç±»æç¤ºè¯è®¾ç½®ç®¡ç†API
@app.get("/api/settings/classification-prompts")
async def get_classification_prompts(db: Session = Depends(get_db)):
    """è·å–åˆ†ç±»æç¤ºè¯è®¾ç½®"""
    try:
        prompts = db.query(SystemSettings).filter(
            SystemSettings.category == "ai_classification"
        ).all()
        
        result = {}
        for prompt in prompts:
            result[prompt.setting_key] = {
                "value": prompt.setting_value,
                "description": prompt.description,
                "is_editable": prompt.is_editable,
                "requires_restart": prompt.requires_restart,
                "updated_at": prompt.updated_at.isoformat() if prompt.updated_at else None
            }
        
        return {"success": True, "prompts": result}
    except Exception as e:
        logger.error(f"è·å–åˆ†ç±»æç¤ºè¯å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.put("/api/settings/classification-prompts/{prompt_key}")
async def update_classification_prompt(
    prompt_key: str,
    prompt_data: dict,
    db: Session = Depends(get_db)
):
    """æ›´æ–°åˆ†ç±»æç¤ºè¯"""
    try:
        # éªŒè¯prompt_key
        allowed_keys = ["classification_vision_prompt", "classification_text_prompt"]
        if prompt_key not in allowed_keys:
            raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„æç¤ºè¯ç±»å‹: {prompt_key}")
        
        # è·å–è®¾ç½®
        setting = db.query(SystemSettings).filter(
            SystemSettings.setting_key == prompt_key
        ).first()
        
        if not setting:
            raise HTTPException(status_code=404, detail="æç¤ºè¯è®¾ç½®ä¸å­˜åœ¨")
        
        if not setting.is_editable:
            raise HTTPException(status_code=400, detail="è¯¥è®¾ç½®ä¸å¯ç¼–è¾‘")
        
        # æ›´æ–°å€¼
        new_value = prompt_data.get("value", "")
        if not new_value.strip():
            raise HTTPException(status_code=400, detail="æç¤ºè¯å†…å®¹ä¸èƒ½ä¸ºç©º")
        
        setting.setting_value = new_value
        setting.updated_at = func.now()
        
        db.commit()
        
        # çƒ­é‡è½½AIæœåŠ¡é…ç½®
        ai_service.reload_config()
        
        return {
            "success": True,
            "message": f"æç¤ºè¯ {prompt_key} æ›´æ–°æˆåŠŸ",
            "updated_at": setting.updated_at.isoformat(),
            "reloaded": True
        }
        
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        logger.error(f"æ›´æ–°åˆ†ç±»æç¤ºè¯å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/settings/classification-prompts/reset/{prompt_key}")
async def reset_classification_prompt(
    prompt_key: str,
    db: Session = Depends(get_db)
):
    """é‡ç½®åˆ†ç±»æç¤ºè¯ä¸ºé»˜è®¤å€¼"""
    try:
        # éªŒè¯prompt_key
        allowed_keys = ["classification_vision_prompt", "classification_text_prompt"]
        if prompt_key not in allowed_keys:
            raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„æç¤ºè¯ç±»å‹: {prompt_key}")
        
        # è·å–è®¾ç½®
        setting = db.query(SystemSettings).filter(
            SystemSettings.setting_key == prompt_key
        ).first()
        
        if not setting:
            raise HTTPException(status_code=404, detail="æç¤ºè¯è®¾ç½®ä¸å­˜åœ¨")
        
        # è·å–é»˜è®¤æç¤ºè¯
        if prompt_key == "classification_vision_prompt":
            default_prompt = ai_service._get_default_vision_prompt()
        else:
            default_prompt = ai_service._get_default_text_prompt()
        
        # æ›´æ–°ä¸ºé»˜è®¤å€¼
        setting.setting_value = default_prompt
        setting.updated_at = func.now()
        
        db.commit()
        
        # çƒ­é‡è½½AIæœåŠ¡é…ç½®
        ai_service.reload_config()
        
        return {
            "success": True,
            "message": f"æç¤ºè¯ {prompt_key} é‡ç½®ä¸ºé»˜è®¤å€¼",
            "updated_at": setting.updated_at.isoformat(),
            "reloaded": True
        }
        
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        logger.error(f"é‡ç½®åˆ†ç±»æç¤ºè¯å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# æ£€æŸ¥Doclingæ˜¯å¦å¯ç”¨
try:
    import docling
    DOCLING_AVAILABLE = True
except ImportError:
    DOCLING_AVAILABLE = False

# Docling OCRç®¡ç†API
@app.get("/api/ocr/docling/status")
async def get_docling_ocr_status():
    """è·å–Docling OCRçŠ¶æ€"""
    try:
        # ä½¿ç”¨DoclingServiceè·å–çŠ¶æ€
        from docling_service import docling_service as ds
        status = ds.get_status()
        
        return {
            "success": True,
            "docling_available": status.get("docling_available", False),
            "ocr_enabled": status.get("config", {}).get("enable_ocr", False),
            "converter_initialized": status.get("initialized", False),
            "languages": status.get("config", {}).get("ocr_languages", [])
        }
    except Exception as e:
        logger.error(f"è·å–Docling OCRçŠ¶æ€å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/ocr/docling/reload")
async def reload_docling_ocr():
    """é‡æ–°åŠ è½½Docling OCRé…ç½®"""
    try:
        ai_service.reload_config()
        
        return {
            "success": True,
            "message": "Docling OCRé…ç½®é‡æ–°åŠ è½½æˆåŠŸ",
            "ocr_enabled": ai_service.enable_docling_ocr,
            "converter_initialized": ai_service.docling_converter is not None
        }
    except Exception as e:
        logger.error(f"é‡æ–°åŠ è½½Docling OCRé…ç½®å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/ocr/easyocr/download-models")
async def download_easyocr_models():
    """ä¸‹è½½EasyOCRæ¨¡å‹"""
    try:
        # è°ƒç”¨AIæœåŠ¡çš„æ¨¡å‹ä¸‹è½½æ–¹æ³•
        result = await ai_service.download_easyocr_models()
        
        if result.get("success"):
            return {
                "success": True,
                "message": result.get("message", "EasyOCRæ¨¡å‹ä¸‹è½½æˆåŠŸ"),
                "model_path": result.get("model_path")
            }
        else:
            return {
                "success": False,
                "message": result.get("error", "æ¨¡å‹ä¸‹è½½å¤±è´¥")
            }
    except Exception as e:
        logger.error(f"ä¸‹è½½EasyOCRæ¨¡å‹å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/ocr/easyocr/status")
async def get_easyocr_status():
    """è·å–EasyOCRæ¨¡å‹çŠ¶æ€"""
    try:
        import os
        # ä½¿ç”¨DoclingServiceè·å–æ¨¡å‹è·¯å¾„
        from docling_service import docling_service as ds
        status = ds.get_status()
        model_path = status.get("config", {}).get("easyocr_models_path", "")
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        model_files = ['craft_mlt_25k.pth', 'zh_sim_g2.pth', 'english_g2.pth']
        existing_models = []
        
        if model_path and os.path.exists(model_path):
            for model_file in model_files:
                file_path = os.path.join(model_path, model_file)
                if os.path.exists(file_path):
                    existing_models.append(model_file)
        
        is_initialized = len(existing_models) >= 2  # è‡³å°‘éœ€è¦2ä¸ªä¸»è¦æ¨¡å‹æ–‡ä»¶
        
        return {
            "success": True,
            "initialized": is_initialized,
            "model_path": model_path,
            "existing_models": existing_models,
            "total_models": len(model_files),
            "languages": status.get("config", {}).get("ocr_languages", [])
        }
    except Exception as e:
        logger.error(f"è·å–EasyOCRçŠ¶æ€å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/ai-models/status")
async def get_ai_models_status():
    """è·å–AIæ¨¡å‹çŠ¶æ€"""
    try:
        from ai_service import ai_service
        status = ai_service.get_ai_models_status()
        return {"success": True, **status}
    except Exception as e:
        logger.error(f"è·å–AIæ¨¡å‹çŠ¶æ€å¤±è´¥: {e}")
        return {"success": False, "error": str(e)}

@app.post("/api/ai-models/download")
async def trigger_ai_models_download():
    """è§¦å‘AIæ¨¡å‹ä¸‹è½½"""
    try:
        from ai_service import ai_service
        ai_service._start_ai_models_download()
        return {"success": True, "message": "AIæ¨¡å‹ä¸‹è½½å·²å¯åŠ¨"}
    except Exception as e:
        logger.error(f"å¯åŠ¨AIæ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
        return {"success": False, "error": str(e)}



if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)